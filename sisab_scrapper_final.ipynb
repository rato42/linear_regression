{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8bef2f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "CID_CODES = [\n",
    "    \"F00\", \"F000\", \"F001\", \"F002\", \"F009\", \"F01\",\n",
    "    \"F010\", \"F011\", \"F012\", \"F013\", \"F018\",\n",
    "    \"F019\", \"F02\", \"F020\", \"F021\", \"F022\", \"F023\",\n",
    "    \"F024\", \"F028\", \"F03\", \"F04\", \"F05\", \"F050\",\n",
    "    \"F051\", \"F058\", \"F059\", \"F06\", \"F060\", \"F061\",\n",
    "    \"F062\", \"F063\", \"F064\", \"F065\", \"F066\",\n",
    "    \"F067\", \"F068\", \"F069\", \"F07\", \"F070\", \"F071\",\n",
    "    \"F072\", \"F078\", \"F079\", \"F09\", \"F20\", \"F200\",\n",
    "    \"F201\", \"F202\", \"F203\", \"F204\", \"F205\",\n",
    "    \"F206\", \"F208\", \"F209\", \"F21\", \"F22\", \"F220\",\n",
    "    \"F228\", \"F229\", \"F23\", \"F230\", \"F231\", \"F232\",\n",
    "    \"F233\", \"F238\", \"F239\", \"F24\", \"F25\", \"F250\",\n",
    "    \"F251\", \"F252\", \"F258\", \"F259\", \"F28\", \"F29\",\n",
    "    \"F30\", \"F300\", \"F301\", \"F302\", \"F308\", \"F309\",\n",
    "    \"F31\", \"F310\", \"F311\", \"F312\", \"F313\", \"F314\",\n",
    "    \"F315\", \"F316\", \"F317\", \"F318\", \"F319\", \"F32\",\n",
    "    \"F320\", \"F321\", \"F322\", \"F323\", \"F328\",\n",
    "    \"F329\", \"F33\", \"F330\", \"F331\", \"F332\", \"F333\",\n",
    "    \"F334\", \"F338\", \"F339\", \"F34\", \"F340\", \"F341\",\n",
    "    \"F348\", \"F349\", \"F38\", \"F380\", \"F381\", \"F388\",\n",
    "    \"F39\", \"F40\", \"F400\", \"F401\", \"F402\", \"F408\",\n",
    "    \"F409\", \"F41\", \"F410\", \"F411\", \"F412\", \"F413\",\n",
    "    \"F418\", \"F419\", \"F42\", \"F420\", \"F421\", \"F422\",\n",
    "    \"F428\", \"F429\", \"F43\", \"F430\", \"F431\", \"F432\",\n",
    "    \"F438\", \"F439\", \"F44\", \"F440\", \"F441\", \"F442\",\n",
    "    \"F443\", \"F444\", \"F445\", \"F446\", \"F447\",\n",
    "    \"F448\", \"F449\", \"F45\", \"F450\", \"F451\", \"F452\",\n",
    "    \"F453\", \"F454\", \"F458\", \"F459\", \"F48\", \"F480\",\n",
    "    \"F481\", \"F488\", \"F489\", \"F50\", \"F500\", \"F501\",\n",
    "    \"F502\", \"F503\", \"F504\", \"F505\", \"F508\",\n",
    "    \"F509\", \"F51\", \"F510\", \"F511\", \"F512\", \"F513\",\n",
    "    \"F514\", \"F515\", \"F518\", \"F519\", \"F52\", \"F520\",\n",
    "    \"F521\", \"F522\", \"F523\", \"F524\", \"F525\",\n",
    "    \"F526\", \"F527\", \"F528\", \"F529\", \"F53\", \"F530\",\n",
    "    \"F531\", \"F538\", \"F539\", \"F54\", \"F55\", \"F59\",\n",
    "    \"F60\", \"F600\", \"F601\", \"F602\", \"F603\", \"F604\",\n",
    "    \"F605\", \"F606\", \"F607\", \"F608\", \"F609\", \"F61\",\n",
    "    \"F62\", \"F620\", \"F621\", \"F628\", \"F629\", \"F63\",\n",
    "    \"F630\", \"F631\", \"F632\", \"F633\", \"F638\",\n",
    "    \"F639\", \"F64\", \"F640\", \"F641\", \"F642\", \"F648\",\n",
    "    \"F649\", \"F65\", \"F650\", \"F651\", \"F652\", \"F653\",\n",
    "    \"F654\", \"F655\", \"F656\", \"F658\", \"F659\", \"F66\",\n",
    "    \"F660\", \"F661\", \"F662\", \"F668\", \"F669\", \"F68\",\n",
    "    \"F680\", \"F681\", \"F688\", \"F69\", \"F70\", \"F700\",\n",
    "    \"F701\", \"F708\", \"F709\", \"F71\", \"F710\", \"F711\",\n",
    "    \"F718\", \"F719\", \"F72\", \"F720\", \"F721\", \"F728\",\n",
    "    \"F729\", \"F73\", \"F730\", \"F731\", \"F738\", \"F739\",\n",
    "    \"F78\", \"F780\", \"F781\", \"F788\", \"F789\", \"F79\",\n",
    "    \"F790\", \"F791\", \"F798\", \"F799\", \"F80\", \"F800\",\n",
    "    \"F801\", \"F802\", \"F803\", \"F808\", \"F809\", \"F81\",\n",
    "    \"F810\", \"F811\", \"F812\", \"F813\", \"F818\",\n",
    "    \"F819\", \"F82\", \"F83\", \"F84\", \"F840\", \"F841\",\n",
    "    \"F842\", \"F843\", \"F844\", \"F845\", \"F848\",\n",
    "    \"F849\", \"F88\", \"F89\", \"F90\", \"F900\", \"F901\",\n",
    "    \"F908\", \"F909\", \"F91\", \"F910\", \"F911\", \"F912\",\n",
    "    \"F913\", \"F918\", \"F919\", \"F92\", \"F920\", \"F928\",\n",
    "    \"F929\", \"F93\", \"F930\", \"F931\", \"F932\", \"F933\",\n",
    "    \"F938\", \"F939\", \"F94\", \"F940\", \"F941\", \"F942\",\n",
    "    \"F948\", \"F949\", \"F95\", \"F950\", \"F951\", \"F952\",\n",
    "    \"F958\", \"F959\", \"F98\", \"F980\", \"F981\", \"F982\",\n",
    "    \"F983\", \"F984\", \"F985\", \"F986\", \"F988\",\n",
    "    \"F989\", \"F99\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24f0865",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bc3070d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03885df6",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e0f29c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cid_string(cid, suffixo = \"-1\"):\n",
    "    \"\"\"\n",
    "    Cria uma string CID formatada para ser usada no payload do POST.\n",
    "    \"\"\"\n",
    "    cid_string = cid + suffixo\n",
    "    return cid_string\n",
    "\n",
    "def populate_cid_list(codes):\n",
    "    \"\"\"\n",
    "    Preenche a lista CID_LIST com os códigos CID formatados.\n",
    "    \"\"\"\n",
    "    cid_list = []\n",
    "    for code in codes:\n",
    "        cid_list.append(create_cid_string(code))\n",
    "    return cid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "48d8c475",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Helper Functions\n",
    "\n",
    "def select_option_by_value(driver, select_xpath, value, timeout=50):\n",
    "    try:\n",
    "        select_element = WebDriverWait(driver, timeout).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, select_xpath))\n",
    "        )\n",
    "        select = Select(select_element)\n",
    "        select.select_by_value(value)\n",
    "        #print(f\"Selecionado valor '{value}' em '{select_xpath}'\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao selecionar valor '{value}': {e}\")\n",
    "        return False\n",
    "    \n",
    "def criar_lista_competencias_por_ano(ano):\n",
    "    competencias = []\n",
    "    for mes in range(1, 13):\n",
    "        competencia = f\"{ano}{mes:02}\"\n",
    "        competencias.append(competencia)\n",
    "    return competencias\n",
    "\n",
    "def create_script_string(script, arg=None, arg_as_string = True):\n",
    "    \"\"\"\n",
    "    Cria uma string de script JavaScript para execução.\n",
    "    \"\"\"\n",
    "    script_str = None\n",
    "    if arg:\n",
    "        if arg_as_string:\n",
    "            script_str = f\"{script}('{arg}')\"\n",
    "        else:\n",
    "            script_str = f\"{script}({arg})\"\n",
    "    else:\n",
    "        script_str = f\"{script}()\"\n",
    "    #print(\"script_str = \", script_str)\n",
    "    return script_str\n",
    "\n",
    "def exec_script(driver, script, arg=None, arg_as_string = True):\n",
    "    \"\"\"\n",
    "    Executa um script JavaScript no contexto da página atual.\n",
    "    \"\"\"\n",
    "    script_string = create_script_string(script, arg, arg_as_string)\n",
    "        \n",
    "    return driver.execute_script(script_string)\n",
    "\n",
    "def ExecuteCIDfilters(driver, cid_list):\n",
    "    for cid in cid_list:\n",
    "        exec_script(driver, 'addCid', cid)\n",
    "        #time.sleep(0.01)\n",
    "        #print(f\"Adicionando CID: {cid}\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def ativar_checkbox_por_valor(driver, container_xpath, valor):\n",
    "    \"\"\"\n",
    "    Ativa o checkbox com o valor especificado dentro de um contêiner identificado por XPath.\n",
    "\n",
    "    Parâmetros:\n",
    "    - driver: instância do WebDriver\n",
    "    - container_xpath: XPath do contêiner que contém os checkboxes\n",
    "    - valor: valor do atributo 'value' do checkbox a ser ativado\n",
    "    \"\"\"\n",
    "    try:\n",
    "        container = driver.find_element(By.XPATH, container_xpath)\n",
    "        checkboxes = container.find_elements(By.XPATH, \".//input[@type='checkbox']\")\n",
    "\n",
    "        for checkbox in checkboxes:\n",
    "            if checkbox.get_attribute(\"value\") == valor:\n",
    "                if not checkbox.is_selected():\n",
    "                    checkbox.click()\n",
    "                    #print(f\"Checkbox '{valor}' ativado.\")\n",
    "                #else:\n",
    "                    #print(f\"Checkbox '{valor}' já estava ativado.\")\n",
    "                return True\n",
    "\n",
    "        print(f\"Checkbox com valor '{valor}' não encontrado.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ativar checkbox '{valor}': {e}\")\n",
    "        return False\n",
    "      \n",
    "def AtivarListaDeCheckBoxes(driver, lista_checkboxes, dropdown_xpath, container_xpath):\n",
    "    drop_button = WebDriverWait(driver, 50).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, dropdown_xpath))\n",
    "    )\n",
    "    actions = ActionChains(driver)\n",
    "    actions.move_to_element(drop_button).click().perform()\n",
    "    WebDriverWait(driver, 50).until(\n",
    "        EC.visibility_of_element_located((By.XPATH, container_xpath))\n",
    "    )\n",
    "    for checkbox in lista_checkboxes:\n",
    "        ativar_checkbox_por_valor(driver, container_xpath, checkbox)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d83003",
   "metadata": {},
   "source": [
    "# Args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7b07e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regioes \"CENTRO-OESTE\", \"NORTE\", \"NORDESTE\", \"SUDESTE\", \"SUL\"\n",
    "\n",
    "PREFFIX = 'scrapped_relatorio_'\n",
    "\n",
    "\n",
    "MAX_TRIES_DOWNLOAD = 5\n",
    "\n",
    "base_download_dir = \"D:\\\\CodeStuff\\\\Stats\\\\colab_linear_regression\\\\linear_regression\\\\sisab_scrap_download\"\n",
    "\n",
    "total_atd_name = \"total_atendimentos\"\n",
    "cidf_name = \"cid_f\"\n",
    "\n",
    "base_cidf_args = {\n",
    "        'ano_range': [2016, 2024],\n",
    "        #\"ano\": 2024,\n",
    "        \"cid_list\": populate_cid_list(CID_CODES),\n",
    "        'linha': \"CID\",\n",
    "        'coluna': \"NU_COMPETENCIA\",\n",
    "        'ug': 'regiao',#\"brasil\",#\"estado\",\n",
    "        'uf': \"43\",\n",
    "        'tpProducao' : \"4\",\n",
    "        #'municipios': [\"431490\"],\n",
    "\n",
    "        #'estados': [\"RS\"],\n",
    "        'headless_browser': False,\n",
    "\n",
    "\n",
    "        ### join args\n",
    "        'joinby_col': 'CIAP/CID',\n",
    "        \"perform_join\": True,          # Join multiple DataFrames into one\n",
    "        \"extract_code\": True,          # Extract codes (e.g., CID codes) from a specified column\n",
    "        \"group_and_sum\": True,         # Group the data by CID groups and sum the values\n",
    "        \"convert_labels\": True,        # Convert column labels (e.g., month names) into datetime objects\n",
    "        \"perform_grand_total\": True,   # Add a row with the sum of all numeric columns\n",
    "        \"save_to_excel\": True          # Save the final processed DataFrame to an Excel file\n",
    "}\n",
    "\n",
    "\n",
    "CID_F_ARGS = {\n",
    "    'sul': {\n",
    "        'download_dir': base_download_dir+\"\\\\sul\"+\"\\\\\"+cidf_name,\n",
    "        'regioes': [\"SUL\"],\n",
    "    },\n",
    "        'norte': {\n",
    "        'download_dir': base_download_dir+\"\\\\norte\"+\"\\\\\"+cidf_name,\n",
    "        'regioes': [\"NORTE\"],\n",
    "    },\n",
    "        'centro_oeste': {\n",
    "        'download_dir': base_download_dir+\"\\\\centro_oeste\"+\"\\\\\"+cidf_name,\n",
    "        'regioes': [\"CENTRO-OESTE\"],\n",
    "    },\n",
    "        'sudeste': {\n",
    "        'download_dir': base_download_dir+\"\\\\sudeste\"+\"\\\\\"+cidf_name,\n",
    "        'regioes': [\"SUDESTE\"],\n",
    "    },\n",
    "        'nordeste': {\n",
    "        'download_dir': base_download_dir+\"\\\\nordeste\"+\"\\\\\"+cidf_name,\n",
    "        'regioes': [\"NORDESTE\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "base_total_args =  {\n",
    "        'ano_range': [2016, 2024],\n",
    "        #\"ano\": 2024,\n",
    "        \"cid_list\": [],#populate_cid_list(CID_CODES),\n",
    "        'linha': \"BRASIL\",\n",
    "        'coluna': \"NU_COMPETENCIA\",\n",
    "        'ug': \"regiao\",\n",
    "        'uf': \"43\",\n",
    "        'tpProducao' : \"4\",\n",
    "        #'municipios': [\"431490\"],\n",
    "        #'estados': [\"RS\"],\n",
    "        'headless_browser': False,\n",
    "        \n",
    "        ### join args\n",
    "        'joinby_col': \"Brasil\",\n",
    "        \"perform_join\": True,         \n",
    "        \"extract_code\": False,        \n",
    "        \"group_and_sum\": False,       \n",
    "        \"convert_labels\": True,       \n",
    "        \"perform_grand_total\": False, \n",
    "        \"save_to_excel\": True         \n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "TOTAL_ATD_ARGS = {\n",
    "    'sul_total': {\n",
    "        'download_dir': base_download_dir+\"\\\\sul\"+\"\\\\\" +total_atd_name,\n",
    "        'regioes': [\"SUL\"],\n",
    "    },\n",
    "        'norte_total': {\n",
    "        'download_dir': base_download_dir+\"\\\\norte\"+\"\\\\\" +total_atd_name,\n",
    "        'regioes': [\"NORTE\"],\n",
    "    },\n",
    "        'centro_oeste_total': {\n",
    "        'download_dir': base_download_dir+\"\\\\centro_oeste\"+\"\\\\\" +total_atd_name,\n",
    "        'regioes': [\"CENTRO-OESTE\"],\n",
    "    },\n",
    "        'sudeste_total': {\n",
    "        'download_dir': base_download_dir+\"\\\\sudeste\"+ \"\\\\\" +total_atd_name,\n",
    "        'regioes': [\"SUDESTE\"],\n",
    "    },\n",
    "        'nordeste_total': {\n",
    "        'download_dir': base_download_dir+\"\\\\nordeste\"+ \"\\\\\" +total_atd_name,\n",
    "        'regioes': [\"NORDESTE\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "VARIABLES_TABLE ={}\n",
    "\n",
    "\n",
    "for key in CID_F_ARGS.keys():\n",
    "    VARIABLES_TABLE[key] = copy.deepcopy(base_cidf_args)\n",
    "    for key2, value in CID_F_ARGS[key].items():\n",
    "        VARIABLES_TABLE[key][key2] = value\n",
    "\n",
    "for key in TOTAL_ATD_ARGS.keys():\n",
    "    VARIABLES_TABLE[key] = copy.deepcopy(base_total_args)\n",
    "    for key2, value in TOTAL_ATD_ARGS[key].items():\n",
    "        VARIABLES_TABLE[key][key2] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8518f6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sul': {'ano_range': [2016, 2024],\n",
       "  'cid_list': ['F00-1',\n",
       "   'F000-1',\n",
       "   'F001-1',\n",
       "   'F002-1',\n",
       "   'F009-1',\n",
       "   'F01-1',\n",
       "   'F010-1',\n",
       "   'F011-1',\n",
       "   'F012-1',\n",
       "   'F013-1',\n",
       "   'F018-1',\n",
       "   'F019-1',\n",
       "   'F02-1',\n",
       "   'F020-1',\n",
       "   'F021-1',\n",
       "   'F022-1',\n",
       "   'F023-1',\n",
       "   'F024-1',\n",
       "   'F028-1',\n",
       "   'F03-1',\n",
       "   'F04-1',\n",
       "   'F05-1',\n",
       "   'F050-1',\n",
       "   'F051-1',\n",
       "   'F058-1',\n",
       "   'F059-1',\n",
       "   'F06-1',\n",
       "   'F060-1',\n",
       "   'F061-1',\n",
       "   'F062-1',\n",
       "   'F063-1',\n",
       "   'F064-1',\n",
       "   'F065-1',\n",
       "   'F066-1',\n",
       "   'F067-1',\n",
       "   'F068-1',\n",
       "   'F069-1',\n",
       "   'F07-1',\n",
       "   'F070-1',\n",
       "   'F071-1',\n",
       "   'F072-1',\n",
       "   'F078-1',\n",
       "   'F079-1',\n",
       "   'F09-1',\n",
       "   'F20-1',\n",
       "   'F200-1',\n",
       "   'F201-1',\n",
       "   'F202-1',\n",
       "   'F203-1',\n",
       "   'F204-1',\n",
       "   'F205-1',\n",
       "   'F206-1',\n",
       "   'F208-1',\n",
       "   'F209-1',\n",
       "   'F21-1',\n",
       "   'F22-1',\n",
       "   'F220-1',\n",
       "   'F228-1',\n",
       "   'F229-1',\n",
       "   'F23-1',\n",
       "   'F230-1',\n",
       "   'F231-1',\n",
       "   'F232-1',\n",
       "   'F233-1',\n",
       "   'F238-1',\n",
       "   'F239-1',\n",
       "   'F24-1',\n",
       "   'F25-1',\n",
       "   'F250-1',\n",
       "   'F251-1',\n",
       "   'F252-1',\n",
       "   'F258-1',\n",
       "   'F259-1',\n",
       "   'F28-1',\n",
       "   'F29-1',\n",
       "   'F30-1',\n",
       "   'F300-1',\n",
       "   'F301-1',\n",
       "   'F302-1',\n",
       "   'F308-1',\n",
       "   'F309-1',\n",
       "   'F31-1',\n",
       "   'F310-1',\n",
       "   'F311-1',\n",
       "   'F312-1',\n",
       "   'F313-1',\n",
       "   'F314-1',\n",
       "   'F315-1',\n",
       "   'F316-1',\n",
       "   'F317-1',\n",
       "   'F318-1',\n",
       "   'F319-1',\n",
       "   'F32-1',\n",
       "   'F320-1',\n",
       "   'F321-1',\n",
       "   'F322-1',\n",
       "   'F323-1',\n",
       "   'F328-1',\n",
       "   'F329-1',\n",
       "   'F33-1',\n",
       "   'F330-1',\n",
       "   'F331-1',\n",
       "   'F332-1',\n",
       "   'F333-1',\n",
       "   'F334-1',\n",
       "   'F338-1',\n",
       "   'F339-1',\n",
       "   'F34-1',\n",
       "   'F340-1',\n",
       "   'F341-1',\n",
       "   'F348-1',\n",
       "   'F349-1',\n",
       "   'F38-1',\n",
       "   'F380-1',\n",
       "   'F381-1',\n",
       "   'F388-1',\n",
       "   'F39-1',\n",
       "   'F40-1',\n",
       "   'F400-1',\n",
       "   'F401-1',\n",
       "   'F402-1',\n",
       "   'F408-1',\n",
       "   'F409-1',\n",
       "   'F41-1',\n",
       "   'F410-1',\n",
       "   'F411-1',\n",
       "   'F412-1',\n",
       "   'F413-1',\n",
       "   'F418-1',\n",
       "   'F419-1',\n",
       "   'F42-1',\n",
       "   'F420-1',\n",
       "   'F421-1',\n",
       "   'F422-1',\n",
       "   'F428-1',\n",
       "   'F429-1',\n",
       "   'F43-1',\n",
       "   'F430-1',\n",
       "   'F431-1',\n",
       "   'F432-1',\n",
       "   'F438-1',\n",
       "   'F439-1',\n",
       "   'F44-1',\n",
       "   'F440-1',\n",
       "   'F441-1',\n",
       "   'F442-1',\n",
       "   'F443-1',\n",
       "   'F444-1',\n",
       "   'F445-1',\n",
       "   'F446-1',\n",
       "   'F447-1',\n",
       "   'F448-1',\n",
       "   'F449-1',\n",
       "   'F45-1',\n",
       "   'F450-1',\n",
       "   'F451-1',\n",
       "   'F452-1',\n",
       "   'F453-1',\n",
       "   'F454-1',\n",
       "   'F458-1',\n",
       "   'F459-1',\n",
       "   'F48-1',\n",
       "   'F480-1',\n",
       "   'F481-1',\n",
       "   'F488-1',\n",
       "   'F489-1',\n",
       "   'F50-1',\n",
       "   'F500-1',\n",
       "   'F501-1',\n",
       "   'F502-1',\n",
       "   'F503-1',\n",
       "   'F504-1',\n",
       "   'F505-1',\n",
       "   'F508-1',\n",
       "   'F509-1',\n",
       "   'F51-1',\n",
       "   'F510-1',\n",
       "   'F511-1',\n",
       "   'F512-1',\n",
       "   'F513-1',\n",
       "   'F514-1',\n",
       "   'F515-1',\n",
       "   'F518-1',\n",
       "   'F519-1',\n",
       "   'F52-1',\n",
       "   'F520-1',\n",
       "   'F521-1',\n",
       "   'F522-1',\n",
       "   'F523-1',\n",
       "   'F524-1',\n",
       "   'F525-1',\n",
       "   'F526-1',\n",
       "   'F527-1',\n",
       "   'F528-1',\n",
       "   'F529-1',\n",
       "   'F53-1',\n",
       "   'F530-1',\n",
       "   'F531-1',\n",
       "   'F538-1',\n",
       "   'F539-1',\n",
       "   'F54-1',\n",
       "   'F55-1',\n",
       "   'F59-1',\n",
       "   'F60-1',\n",
       "   'F600-1',\n",
       "   'F601-1',\n",
       "   'F602-1',\n",
       "   'F603-1',\n",
       "   'F604-1',\n",
       "   'F605-1',\n",
       "   'F606-1',\n",
       "   'F607-1',\n",
       "   'F608-1',\n",
       "   'F609-1',\n",
       "   'F61-1',\n",
       "   'F62-1',\n",
       "   'F620-1',\n",
       "   'F621-1',\n",
       "   'F628-1',\n",
       "   'F629-1',\n",
       "   'F63-1',\n",
       "   'F630-1',\n",
       "   'F631-1',\n",
       "   'F632-1',\n",
       "   'F633-1',\n",
       "   'F638-1',\n",
       "   'F639-1',\n",
       "   'F64-1',\n",
       "   'F640-1',\n",
       "   'F641-1',\n",
       "   'F642-1',\n",
       "   'F648-1',\n",
       "   'F649-1',\n",
       "   'F65-1',\n",
       "   'F650-1',\n",
       "   'F651-1',\n",
       "   'F652-1',\n",
       "   'F653-1',\n",
       "   'F654-1',\n",
       "   'F655-1',\n",
       "   'F656-1',\n",
       "   'F658-1',\n",
       "   'F659-1',\n",
       "   'F66-1',\n",
       "   'F660-1',\n",
       "   'F661-1',\n",
       "   'F662-1',\n",
       "   'F668-1',\n",
       "   'F669-1',\n",
       "   'F68-1',\n",
       "   'F680-1',\n",
       "   'F681-1',\n",
       "   'F688-1',\n",
       "   'F69-1',\n",
       "   'F70-1',\n",
       "   'F700-1',\n",
       "   'F701-1',\n",
       "   'F708-1',\n",
       "   'F709-1',\n",
       "   'F71-1',\n",
       "   'F710-1',\n",
       "   'F711-1',\n",
       "   'F718-1',\n",
       "   'F719-1',\n",
       "   'F72-1',\n",
       "   'F720-1',\n",
       "   'F721-1',\n",
       "   'F728-1',\n",
       "   'F729-1',\n",
       "   'F73-1',\n",
       "   'F730-1',\n",
       "   'F731-1',\n",
       "   'F738-1',\n",
       "   'F739-1',\n",
       "   'F78-1',\n",
       "   'F780-1',\n",
       "   'F781-1',\n",
       "   'F788-1',\n",
       "   'F789-1',\n",
       "   'F79-1',\n",
       "   'F790-1',\n",
       "   'F791-1',\n",
       "   'F798-1',\n",
       "   'F799-1',\n",
       "   'F80-1',\n",
       "   'F800-1',\n",
       "   'F801-1',\n",
       "   'F802-1',\n",
       "   'F803-1',\n",
       "   'F808-1',\n",
       "   'F809-1',\n",
       "   'F81-1',\n",
       "   'F810-1',\n",
       "   'F811-1',\n",
       "   'F812-1',\n",
       "   'F813-1',\n",
       "   'F818-1',\n",
       "   'F819-1',\n",
       "   'F82-1',\n",
       "   'F83-1',\n",
       "   'F84-1',\n",
       "   'F840-1',\n",
       "   'F841-1',\n",
       "   'F842-1',\n",
       "   'F843-1',\n",
       "   'F844-1',\n",
       "   'F845-1',\n",
       "   'F848-1',\n",
       "   'F849-1',\n",
       "   'F88-1',\n",
       "   'F89-1',\n",
       "   'F90-1',\n",
       "   'F900-1',\n",
       "   'F901-1',\n",
       "   'F908-1',\n",
       "   'F909-1',\n",
       "   'F91-1',\n",
       "   'F910-1',\n",
       "   'F911-1',\n",
       "   'F912-1',\n",
       "   'F913-1',\n",
       "   'F918-1',\n",
       "   'F919-1',\n",
       "   'F92-1',\n",
       "   'F920-1',\n",
       "   'F928-1',\n",
       "   'F929-1',\n",
       "   'F93-1',\n",
       "   'F930-1',\n",
       "   'F931-1',\n",
       "   'F932-1',\n",
       "   'F933-1',\n",
       "   'F938-1',\n",
       "   'F939-1',\n",
       "   'F94-1',\n",
       "   'F940-1',\n",
       "   'F941-1',\n",
       "   'F942-1',\n",
       "   'F948-1',\n",
       "   'F949-1',\n",
       "   'F95-1',\n",
       "   'F950-1',\n",
       "   'F951-1',\n",
       "   'F952-1',\n",
       "   'F958-1',\n",
       "   'F959-1',\n",
       "   'F98-1',\n",
       "   'F980-1',\n",
       "   'F981-1',\n",
       "   'F982-1',\n",
       "   'F983-1',\n",
       "   'F984-1',\n",
       "   'F985-1',\n",
       "   'F986-1',\n",
       "   'F988-1',\n",
       "   'F989-1',\n",
       "   'F99-1'],\n",
       "  'linha': 'CID',\n",
       "  'coluna': 'NU_COMPETENCIA',\n",
       "  'ug': 'regiao',\n",
       "  'uf': '43',\n",
       "  'tpProducao': '4',\n",
       "  'headless_browser': False,\n",
       "  'joinby_col': 'CIAP/CID',\n",
       "  'perform_join': True,\n",
       "  'extract_code': True,\n",
       "  'group_and_sum': True,\n",
       "  'convert_labels': True,\n",
       "  'perform_grand_total': True,\n",
       "  'save_to_excel': True,\n",
       "  'download_dir': 'D:\\\\CodeStuff\\\\Stats\\\\colab_linear_regression\\\\linear_regression\\\\sisab_scrap_download\\\\sul\\\\cid_f',\n",
       "  'regioes': ['SUL']},\n",
       " 'norte': {'ano_range': [2016, 2024],\n",
       "  'cid_list': ['F00-1',\n",
       "   'F000-1',\n",
       "   'F001-1',\n",
       "   'F002-1',\n",
       "   'F009-1',\n",
       "   'F01-1',\n",
       "   'F010-1',\n",
       "   'F011-1',\n",
       "   'F012-1',\n",
       "   'F013-1',\n",
       "   'F018-1',\n",
       "   'F019-1',\n",
       "   'F02-1',\n",
       "   'F020-1',\n",
       "   'F021-1',\n",
       "   'F022-1',\n",
       "   'F023-1',\n",
       "   'F024-1',\n",
       "   'F028-1',\n",
       "   'F03-1',\n",
       "   'F04-1',\n",
       "   'F05-1',\n",
       "   'F050-1',\n",
       "   'F051-1',\n",
       "   'F058-1',\n",
       "   'F059-1',\n",
       "   'F06-1',\n",
       "   'F060-1',\n",
       "   'F061-1',\n",
       "   'F062-1',\n",
       "   'F063-1',\n",
       "   'F064-1',\n",
       "   'F065-1',\n",
       "   'F066-1',\n",
       "   'F067-1',\n",
       "   'F068-1',\n",
       "   'F069-1',\n",
       "   'F07-1',\n",
       "   'F070-1',\n",
       "   'F071-1',\n",
       "   'F072-1',\n",
       "   'F078-1',\n",
       "   'F079-1',\n",
       "   'F09-1',\n",
       "   'F20-1',\n",
       "   'F200-1',\n",
       "   'F201-1',\n",
       "   'F202-1',\n",
       "   'F203-1',\n",
       "   'F204-1',\n",
       "   'F205-1',\n",
       "   'F206-1',\n",
       "   'F208-1',\n",
       "   'F209-1',\n",
       "   'F21-1',\n",
       "   'F22-1',\n",
       "   'F220-1',\n",
       "   'F228-1',\n",
       "   'F229-1',\n",
       "   'F23-1',\n",
       "   'F230-1',\n",
       "   'F231-1',\n",
       "   'F232-1',\n",
       "   'F233-1',\n",
       "   'F238-1',\n",
       "   'F239-1',\n",
       "   'F24-1',\n",
       "   'F25-1',\n",
       "   'F250-1',\n",
       "   'F251-1',\n",
       "   'F252-1',\n",
       "   'F258-1',\n",
       "   'F259-1',\n",
       "   'F28-1',\n",
       "   'F29-1',\n",
       "   'F30-1',\n",
       "   'F300-1',\n",
       "   'F301-1',\n",
       "   'F302-1',\n",
       "   'F308-1',\n",
       "   'F309-1',\n",
       "   'F31-1',\n",
       "   'F310-1',\n",
       "   'F311-1',\n",
       "   'F312-1',\n",
       "   'F313-1',\n",
       "   'F314-1',\n",
       "   'F315-1',\n",
       "   'F316-1',\n",
       "   'F317-1',\n",
       "   'F318-1',\n",
       "   'F319-1',\n",
       "   'F32-1',\n",
       "   'F320-1',\n",
       "   'F321-1',\n",
       "   'F322-1',\n",
       "   'F323-1',\n",
       "   'F328-1',\n",
       "   'F329-1',\n",
       "   'F33-1',\n",
       "   'F330-1',\n",
       "   'F331-1',\n",
       "   'F332-1',\n",
       "   'F333-1',\n",
       "   'F334-1',\n",
       "   'F338-1',\n",
       "   'F339-1',\n",
       "   'F34-1',\n",
       "   'F340-1',\n",
       "   'F341-1',\n",
       "   'F348-1',\n",
       "   'F349-1',\n",
       "   'F38-1',\n",
       "   'F380-1',\n",
       "   'F381-1',\n",
       "   'F388-1',\n",
       "   'F39-1',\n",
       "   'F40-1',\n",
       "   'F400-1',\n",
       "   'F401-1',\n",
       "   'F402-1',\n",
       "   'F408-1',\n",
       "   'F409-1',\n",
       "   'F41-1',\n",
       "   'F410-1',\n",
       "   'F411-1',\n",
       "   'F412-1',\n",
       "   'F413-1',\n",
       "   'F418-1',\n",
       "   'F419-1',\n",
       "   'F42-1',\n",
       "   'F420-1',\n",
       "   'F421-1',\n",
       "   'F422-1',\n",
       "   'F428-1',\n",
       "   'F429-1',\n",
       "   'F43-1',\n",
       "   'F430-1',\n",
       "   'F431-1',\n",
       "   'F432-1',\n",
       "   'F438-1',\n",
       "   'F439-1',\n",
       "   'F44-1',\n",
       "   'F440-1',\n",
       "   'F441-1',\n",
       "   'F442-1',\n",
       "   'F443-1',\n",
       "   'F444-1',\n",
       "   'F445-1',\n",
       "   'F446-1',\n",
       "   'F447-1',\n",
       "   'F448-1',\n",
       "   'F449-1',\n",
       "   'F45-1',\n",
       "   'F450-1',\n",
       "   'F451-1',\n",
       "   'F452-1',\n",
       "   'F453-1',\n",
       "   'F454-1',\n",
       "   'F458-1',\n",
       "   'F459-1',\n",
       "   'F48-1',\n",
       "   'F480-1',\n",
       "   'F481-1',\n",
       "   'F488-1',\n",
       "   'F489-1',\n",
       "   'F50-1',\n",
       "   'F500-1',\n",
       "   'F501-1',\n",
       "   'F502-1',\n",
       "   'F503-1',\n",
       "   'F504-1',\n",
       "   'F505-1',\n",
       "   'F508-1',\n",
       "   'F509-1',\n",
       "   'F51-1',\n",
       "   'F510-1',\n",
       "   'F511-1',\n",
       "   'F512-1',\n",
       "   'F513-1',\n",
       "   'F514-1',\n",
       "   'F515-1',\n",
       "   'F518-1',\n",
       "   'F519-1',\n",
       "   'F52-1',\n",
       "   'F520-1',\n",
       "   'F521-1',\n",
       "   'F522-1',\n",
       "   'F523-1',\n",
       "   'F524-1',\n",
       "   'F525-1',\n",
       "   'F526-1',\n",
       "   'F527-1',\n",
       "   'F528-1',\n",
       "   'F529-1',\n",
       "   'F53-1',\n",
       "   'F530-1',\n",
       "   'F531-1',\n",
       "   'F538-1',\n",
       "   'F539-1',\n",
       "   'F54-1',\n",
       "   'F55-1',\n",
       "   'F59-1',\n",
       "   'F60-1',\n",
       "   'F600-1',\n",
       "   'F601-1',\n",
       "   'F602-1',\n",
       "   'F603-1',\n",
       "   'F604-1',\n",
       "   'F605-1',\n",
       "   'F606-1',\n",
       "   'F607-1',\n",
       "   'F608-1',\n",
       "   'F609-1',\n",
       "   'F61-1',\n",
       "   'F62-1',\n",
       "   'F620-1',\n",
       "   'F621-1',\n",
       "   'F628-1',\n",
       "   'F629-1',\n",
       "   'F63-1',\n",
       "   'F630-1',\n",
       "   'F631-1',\n",
       "   'F632-1',\n",
       "   'F633-1',\n",
       "   'F638-1',\n",
       "   'F639-1',\n",
       "   'F64-1',\n",
       "   'F640-1',\n",
       "   'F641-1',\n",
       "   'F642-1',\n",
       "   'F648-1',\n",
       "   'F649-1',\n",
       "   'F65-1',\n",
       "   'F650-1',\n",
       "   'F651-1',\n",
       "   'F652-1',\n",
       "   'F653-1',\n",
       "   'F654-1',\n",
       "   'F655-1',\n",
       "   'F656-1',\n",
       "   'F658-1',\n",
       "   'F659-1',\n",
       "   'F66-1',\n",
       "   'F660-1',\n",
       "   'F661-1',\n",
       "   'F662-1',\n",
       "   'F668-1',\n",
       "   'F669-1',\n",
       "   'F68-1',\n",
       "   'F680-1',\n",
       "   'F681-1',\n",
       "   'F688-1',\n",
       "   'F69-1',\n",
       "   'F70-1',\n",
       "   'F700-1',\n",
       "   'F701-1',\n",
       "   'F708-1',\n",
       "   'F709-1',\n",
       "   'F71-1',\n",
       "   'F710-1',\n",
       "   'F711-1',\n",
       "   'F718-1',\n",
       "   'F719-1',\n",
       "   'F72-1',\n",
       "   'F720-1',\n",
       "   'F721-1',\n",
       "   'F728-1',\n",
       "   'F729-1',\n",
       "   'F73-1',\n",
       "   'F730-1',\n",
       "   'F731-1',\n",
       "   'F738-1',\n",
       "   'F739-1',\n",
       "   'F78-1',\n",
       "   'F780-1',\n",
       "   'F781-1',\n",
       "   'F788-1',\n",
       "   'F789-1',\n",
       "   'F79-1',\n",
       "   'F790-1',\n",
       "   'F791-1',\n",
       "   'F798-1',\n",
       "   'F799-1',\n",
       "   'F80-1',\n",
       "   'F800-1',\n",
       "   'F801-1',\n",
       "   'F802-1',\n",
       "   'F803-1',\n",
       "   'F808-1',\n",
       "   'F809-1',\n",
       "   'F81-1',\n",
       "   'F810-1',\n",
       "   'F811-1',\n",
       "   'F812-1',\n",
       "   'F813-1',\n",
       "   'F818-1',\n",
       "   'F819-1',\n",
       "   'F82-1',\n",
       "   'F83-1',\n",
       "   'F84-1',\n",
       "   'F840-1',\n",
       "   'F841-1',\n",
       "   'F842-1',\n",
       "   'F843-1',\n",
       "   'F844-1',\n",
       "   'F845-1',\n",
       "   'F848-1',\n",
       "   'F849-1',\n",
       "   'F88-1',\n",
       "   'F89-1',\n",
       "   'F90-1',\n",
       "   'F900-1',\n",
       "   'F901-1',\n",
       "   'F908-1',\n",
       "   'F909-1',\n",
       "   'F91-1',\n",
       "   'F910-1',\n",
       "   'F911-1',\n",
       "   'F912-1',\n",
       "   'F913-1',\n",
       "   'F918-1',\n",
       "   'F919-1',\n",
       "   'F92-1',\n",
       "   'F920-1',\n",
       "   'F928-1',\n",
       "   'F929-1',\n",
       "   'F93-1',\n",
       "   'F930-1',\n",
       "   'F931-1',\n",
       "   'F932-1',\n",
       "   'F933-1',\n",
       "   'F938-1',\n",
       "   'F939-1',\n",
       "   'F94-1',\n",
       "   'F940-1',\n",
       "   'F941-1',\n",
       "   'F942-1',\n",
       "   'F948-1',\n",
       "   'F949-1',\n",
       "   'F95-1',\n",
       "   'F950-1',\n",
       "   'F951-1',\n",
       "   'F952-1',\n",
       "   'F958-1',\n",
       "   'F959-1',\n",
       "   'F98-1',\n",
       "   'F980-1',\n",
       "   'F981-1',\n",
       "   'F982-1',\n",
       "   'F983-1',\n",
       "   'F984-1',\n",
       "   'F985-1',\n",
       "   'F986-1',\n",
       "   'F988-1',\n",
       "   'F989-1',\n",
       "   'F99-1'],\n",
       "  'linha': 'CID',\n",
       "  'coluna': 'NU_COMPETENCIA',\n",
       "  'ug': 'regiao',\n",
       "  'uf': '43',\n",
       "  'tpProducao': '4',\n",
       "  'headless_browser': False,\n",
       "  'joinby_col': 'CIAP/CID',\n",
       "  'perform_join': True,\n",
       "  'extract_code': True,\n",
       "  'group_and_sum': True,\n",
       "  'convert_labels': True,\n",
       "  'perform_grand_total': True,\n",
       "  'save_to_excel': True,\n",
       "  'download_dir': 'D:\\\\CodeStuff\\\\Stats\\\\colab_linear_regression\\\\linear_regression\\\\sisab_scrap_download\\\\norte\\\\cid_f',\n",
       "  'regioes': ['NORTE']},\n",
       " 'centro_oeste': {'ano_range': [2016, 2024],\n",
       "  'cid_list': ['F00-1',\n",
       "   'F000-1',\n",
       "   'F001-1',\n",
       "   'F002-1',\n",
       "   'F009-1',\n",
       "   'F01-1',\n",
       "   'F010-1',\n",
       "   'F011-1',\n",
       "   'F012-1',\n",
       "   'F013-1',\n",
       "   'F018-1',\n",
       "   'F019-1',\n",
       "   'F02-1',\n",
       "   'F020-1',\n",
       "   'F021-1',\n",
       "   'F022-1',\n",
       "   'F023-1',\n",
       "   'F024-1',\n",
       "   'F028-1',\n",
       "   'F03-1',\n",
       "   'F04-1',\n",
       "   'F05-1',\n",
       "   'F050-1',\n",
       "   'F051-1',\n",
       "   'F058-1',\n",
       "   'F059-1',\n",
       "   'F06-1',\n",
       "   'F060-1',\n",
       "   'F061-1',\n",
       "   'F062-1',\n",
       "   'F063-1',\n",
       "   'F064-1',\n",
       "   'F065-1',\n",
       "   'F066-1',\n",
       "   'F067-1',\n",
       "   'F068-1',\n",
       "   'F069-1',\n",
       "   'F07-1',\n",
       "   'F070-1',\n",
       "   'F071-1',\n",
       "   'F072-1',\n",
       "   'F078-1',\n",
       "   'F079-1',\n",
       "   'F09-1',\n",
       "   'F20-1',\n",
       "   'F200-1',\n",
       "   'F201-1',\n",
       "   'F202-1',\n",
       "   'F203-1',\n",
       "   'F204-1',\n",
       "   'F205-1',\n",
       "   'F206-1',\n",
       "   'F208-1',\n",
       "   'F209-1',\n",
       "   'F21-1',\n",
       "   'F22-1',\n",
       "   'F220-1',\n",
       "   'F228-1',\n",
       "   'F229-1',\n",
       "   'F23-1',\n",
       "   'F230-1',\n",
       "   'F231-1',\n",
       "   'F232-1',\n",
       "   'F233-1',\n",
       "   'F238-1',\n",
       "   'F239-1',\n",
       "   'F24-1',\n",
       "   'F25-1',\n",
       "   'F250-1',\n",
       "   'F251-1',\n",
       "   'F252-1',\n",
       "   'F258-1',\n",
       "   'F259-1',\n",
       "   'F28-1',\n",
       "   'F29-1',\n",
       "   'F30-1',\n",
       "   'F300-1',\n",
       "   'F301-1',\n",
       "   'F302-1',\n",
       "   'F308-1',\n",
       "   'F309-1',\n",
       "   'F31-1',\n",
       "   'F310-1',\n",
       "   'F311-1',\n",
       "   'F312-1',\n",
       "   'F313-1',\n",
       "   'F314-1',\n",
       "   'F315-1',\n",
       "   'F316-1',\n",
       "   'F317-1',\n",
       "   'F318-1',\n",
       "   'F319-1',\n",
       "   'F32-1',\n",
       "   'F320-1',\n",
       "   'F321-1',\n",
       "   'F322-1',\n",
       "   'F323-1',\n",
       "   'F328-1',\n",
       "   'F329-1',\n",
       "   'F33-1',\n",
       "   'F330-1',\n",
       "   'F331-1',\n",
       "   'F332-1',\n",
       "   'F333-1',\n",
       "   'F334-1',\n",
       "   'F338-1',\n",
       "   'F339-1',\n",
       "   'F34-1',\n",
       "   'F340-1',\n",
       "   'F341-1',\n",
       "   'F348-1',\n",
       "   'F349-1',\n",
       "   'F38-1',\n",
       "   'F380-1',\n",
       "   'F381-1',\n",
       "   'F388-1',\n",
       "   'F39-1',\n",
       "   'F40-1',\n",
       "   'F400-1',\n",
       "   'F401-1',\n",
       "   'F402-1',\n",
       "   'F408-1',\n",
       "   'F409-1',\n",
       "   'F41-1',\n",
       "   'F410-1',\n",
       "   'F411-1',\n",
       "   'F412-1',\n",
       "   'F413-1',\n",
       "   'F418-1',\n",
       "   'F419-1',\n",
       "   'F42-1',\n",
       "   'F420-1',\n",
       "   'F421-1',\n",
       "   'F422-1',\n",
       "   'F428-1',\n",
       "   'F429-1',\n",
       "   'F43-1',\n",
       "   'F430-1',\n",
       "   'F431-1',\n",
       "   'F432-1',\n",
       "   'F438-1',\n",
       "   'F439-1',\n",
       "   'F44-1',\n",
       "   'F440-1',\n",
       "   'F441-1',\n",
       "   'F442-1',\n",
       "   'F443-1',\n",
       "   'F444-1',\n",
       "   'F445-1',\n",
       "   'F446-1',\n",
       "   'F447-1',\n",
       "   'F448-1',\n",
       "   'F449-1',\n",
       "   'F45-1',\n",
       "   'F450-1',\n",
       "   'F451-1',\n",
       "   'F452-1',\n",
       "   'F453-1',\n",
       "   'F454-1',\n",
       "   'F458-1',\n",
       "   'F459-1',\n",
       "   'F48-1',\n",
       "   'F480-1',\n",
       "   'F481-1',\n",
       "   'F488-1',\n",
       "   'F489-1',\n",
       "   'F50-1',\n",
       "   'F500-1',\n",
       "   'F501-1',\n",
       "   'F502-1',\n",
       "   'F503-1',\n",
       "   'F504-1',\n",
       "   'F505-1',\n",
       "   'F508-1',\n",
       "   'F509-1',\n",
       "   'F51-1',\n",
       "   'F510-1',\n",
       "   'F511-1',\n",
       "   'F512-1',\n",
       "   'F513-1',\n",
       "   'F514-1',\n",
       "   'F515-1',\n",
       "   'F518-1',\n",
       "   'F519-1',\n",
       "   'F52-1',\n",
       "   'F520-1',\n",
       "   'F521-1',\n",
       "   'F522-1',\n",
       "   'F523-1',\n",
       "   'F524-1',\n",
       "   'F525-1',\n",
       "   'F526-1',\n",
       "   'F527-1',\n",
       "   'F528-1',\n",
       "   'F529-1',\n",
       "   'F53-1',\n",
       "   'F530-1',\n",
       "   'F531-1',\n",
       "   'F538-1',\n",
       "   'F539-1',\n",
       "   'F54-1',\n",
       "   'F55-1',\n",
       "   'F59-1',\n",
       "   'F60-1',\n",
       "   'F600-1',\n",
       "   'F601-1',\n",
       "   'F602-1',\n",
       "   'F603-1',\n",
       "   'F604-1',\n",
       "   'F605-1',\n",
       "   'F606-1',\n",
       "   'F607-1',\n",
       "   'F608-1',\n",
       "   'F609-1',\n",
       "   'F61-1',\n",
       "   'F62-1',\n",
       "   'F620-1',\n",
       "   'F621-1',\n",
       "   'F628-1',\n",
       "   'F629-1',\n",
       "   'F63-1',\n",
       "   'F630-1',\n",
       "   'F631-1',\n",
       "   'F632-1',\n",
       "   'F633-1',\n",
       "   'F638-1',\n",
       "   'F639-1',\n",
       "   'F64-1',\n",
       "   'F640-1',\n",
       "   'F641-1',\n",
       "   'F642-1',\n",
       "   'F648-1',\n",
       "   'F649-1',\n",
       "   'F65-1',\n",
       "   'F650-1',\n",
       "   'F651-1',\n",
       "   'F652-1',\n",
       "   'F653-1',\n",
       "   'F654-1',\n",
       "   'F655-1',\n",
       "   'F656-1',\n",
       "   'F658-1',\n",
       "   'F659-1',\n",
       "   'F66-1',\n",
       "   'F660-1',\n",
       "   'F661-1',\n",
       "   'F662-1',\n",
       "   'F668-1',\n",
       "   'F669-1',\n",
       "   'F68-1',\n",
       "   'F680-1',\n",
       "   'F681-1',\n",
       "   'F688-1',\n",
       "   'F69-1',\n",
       "   'F70-1',\n",
       "   'F700-1',\n",
       "   'F701-1',\n",
       "   'F708-1',\n",
       "   'F709-1',\n",
       "   'F71-1',\n",
       "   'F710-1',\n",
       "   'F711-1',\n",
       "   'F718-1',\n",
       "   'F719-1',\n",
       "   'F72-1',\n",
       "   'F720-1',\n",
       "   'F721-1',\n",
       "   'F728-1',\n",
       "   'F729-1',\n",
       "   'F73-1',\n",
       "   'F730-1',\n",
       "   'F731-1',\n",
       "   'F738-1',\n",
       "   'F739-1',\n",
       "   'F78-1',\n",
       "   'F780-1',\n",
       "   'F781-1',\n",
       "   'F788-1',\n",
       "   'F789-1',\n",
       "   'F79-1',\n",
       "   'F790-1',\n",
       "   'F791-1',\n",
       "   'F798-1',\n",
       "   'F799-1',\n",
       "   'F80-1',\n",
       "   'F800-1',\n",
       "   'F801-1',\n",
       "   'F802-1',\n",
       "   'F803-1',\n",
       "   'F808-1',\n",
       "   'F809-1',\n",
       "   'F81-1',\n",
       "   'F810-1',\n",
       "   'F811-1',\n",
       "   'F812-1',\n",
       "   'F813-1',\n",
       "   'F818-1',\n",
       "   'F819-1',\n",
       "   'F82-1',\n",
       "   'F83-1',\n",
       "   'F84-1',\n",
       "   'F840-1',\n",
       "   'F841-1',\n",
       "   'F842-1',\n",
       "   'F843-1',\n",
       "   'F844-1',\n",
       "   'F845-1',\n",
       "   'F848-1',\n",
       "   'F849-1',\n",
       "   'F88-1',\n",
       "   'F89-1',\n",
       "   'F90-1',\n",
       "   'F900-1',\n",
       "   'F901-1',\n",
       "   'F908-1',\n",
       "   'F909-1',\n",
       "   'F91-1',\n",
       "   'F910-1',\n",
       "   'F911-1',\n",
       "   'F912-1',\n",
       "   'F913-1',\n",
       "   'F918-1',\n",
       "   'F919-1',\n",
       "   'F92-1',\n",
       "   'F920-1',\n",
       "   'F928-1',\n",
       "   'F929-1',\n",
       "   'F93-1',\n",
       "   'F930-1',\n",
       "   'F931-1',\n",
       "   'F932-1',\n",
       "   'F933-1',\n",
       "   'F938-1',\n",
       "   'F939-1',\n",
       "   'F94-1',\n",
       "   'F940-1',\n",
       "   'F941-1',\n",
       "   'F942-1',\n",
       "   'F948-1',\n",
       "   'F949-1',\n",
       "   'F95-1',\n",
       "   'F950-1',\n",
       "   'F951-1',\n",
       "   'F952-1',\n",
       "   'F958-1',\n",
       "   'F959-1',\n",
       "   'F98-1',\n",
       "   'F980-1',\n",
       "   'F981-1',\n",
       "   'F982-1',\n",
       "   'F983-1',\n",
       "   'F984-1',\n",
       "   'F985-1',\n",
       "   'F986-1',\n",
       "   'F988-1',\n",
       "   'F989-1',\n",
       "   'F99-1'],\n",
       "  'linha': 'CID',\n",
       "  'coluna': 'NU_COMPETENCIA',\n",
       "  'ug': 'regiao',\n",
       "  'uf': '43',\n",
       "  'tpProducao': '4',\n",
       "  'headless_browser': False,\n",
       "  'joinby_col': 'CIAP/CID',\n",
       "  'perform_join': True,\n",
       "  'extract_code': True,\n",
       "  'group_and_sum': True,\n",
       "  'convert_labels': True,\n",
       "  'perform_grand_total': True,\n",
       "  'save_to_excel': True,\n",
       "  'download_dir': 'D:\\\\CodeStuff\\\\Stats\\\\colab_linear_regression\\\\linear_regression\\\\sisab_scrap_download\\\\centro_oeste\\\\cid_f',\n",
       "  'regioes': ['CENTRO-OESTE']},\n",
       " 'sudeste': {'ano_range': [2016, 2024],\n",
       "  'cid_list': ['F00-1',\n",
       "   'F000-1',\n",
       "   'F001-1',\n",
       "   'F002-1',\n",
       "   'F009-1',\n",
       "   'F01-1',\n",
       "   'F010-1',\n",
       "   'F011-1',\n",
       "   'F012-1',\n",
       "   'F013-1',\n",
       "   'F018-1',\n",
       "   'F019-1',\n",
       "   'F02-1',\n",
       "   'F020-1',\n",
       "   'F021-1',\n",
       "   'F022-1',\n",
       "   'F023-1',\n",
       "   'F024-1',\n",
       "   'F028-1',\n",
       "   'F03-1',\n",
       "   'F04-1',\n",
       "   'F05-1',\n",
       "   'F050-1',\n",
       "   'F051-1',\n",
       "   'F058-1',\n",
       "   'F059-1',\n",
       "   'F06-1',\n",
       "   'F060-1',\n",
       "   'F061-1',\n",
       "   'F062-1',\n",
       "   'F063-1',\n",
       "   'F064-1',\n",
       "   'F065-1',\n",
       "   'F066-1',\n",
       "   'F067-1',\n",
       "   'F068-1',\n",
       "   'F069-1',\n",
       "   'F07-1',\n",
       "   'F070-1',\n",
       "   'F071-1',\n",
       "   'F072-1',\n",
       "   'F078-1',\n",
       "   'F079-1',\n",
       "   'F09-1',\n",
       "   'F20-1',\n",
       "   'F200-1',\n",
       "   'F201-1',\n",
       "   'F202-1',\n",
       "   'F203-1',\n",
       "   'F204-1',\n",
       "   'F205-1',\n",
       "   'F206-1',\n",
       "   'F208-1',\n",
       "   'F209-1',\n",
       "   'F21-1',\n",
       "   'F22-1',\n",
       "   'F220-1',\n",
       "   'F228-1',\n",
       "   'F229-1',\n",
       "   'F23-1',\n",
       "   'F230-1',\n",
       "   'F231-1',\n",
       "   'F232-1',\n",
       "   'F233-1',\n",
       "   'F238-1',\n",
       "   'F239-1',\n",
       "   'F24-1',\n",
       "   'F25-1',\n",
       "   'F250-1',\n",
       "   'F251-1',\n",
       "   'F252-1',\n",
       "   'F258-1',\n",
       "   'F259-1',\n",
       "   'F28-1',\n",
       "   'F29-1',\n",
       "   'F30-1',\n",
       "   'F300-1',\n",
       "   'F301-1',\n",
       "   'F302-1',\n",
       "   'F308-1',\n",
       "   'F309-1',\n",
       "   'F31-1',\n",
       "   'F310-1',\n",
       "   'F311-1',\n",
       "   'F312-1',\n",
       "   'F313-1',\n",
       "   'F314-1',\n",
       "   'F315-1',\n",
       "   'F316-1',\n",
       "   'F317-1',\n",
       "   'F318-1',\n",
       "   'F319-1',\n",
       "   'F32-1',\n",
       "   'F320-1',\n",
       "   'F321-1',\n",
       "   'F322-1',\n",
       "   'F323-1',\n",
       "   'F328-1',\n",
       "   'F329-1',\n",
       "   'F33-1',\n",
       "   'F330-1',\n",
       "   'F331-1',\n",
       "   'F332-1',\n",
       "   'F333-1',\n",
       "   'F334-1',\n",
       "   'F338-1',\n",
       "   'F339-1',\n",
       "   'F34-1',\n",
       "   'F340-1',\n",
       "   'F341-1',\n",
       "   'F348-1',\n",
       "   'F349-1',\n",
       "   'F38-1',\n",
       "   'F380-1',\n",
       "   'F381-1',\n",
       "   'F388-1',\n",
       "   'F39-1',\n",
       "   'F40-1',\n",
       "   'F400-1',\n",
       "   'F401-1',\n",
       "   'F402-1',\n",
       "   'F408-1',\n",
       "   'F409-1',\n",
       "   'F41-1',\n",
       "   'F410-1',\n",
       "   'F411-1',\n",
       "   'F412-1',\n",
       "   'F413-1',\n",
       "   'F418-1',\n",
       "   'F419-1',\n",
       "   'F42-1',\n",
       "   'F420-1',\n",
       "   'F421-1',\n",
       "   'F422-1',\n",
       "   'F428-1',\n",
       "   'F429-1',\n",
       "   'F43-1',\n",
       "   'F430-1',\n",
       "   'F431-1',\n",
       "   'F432-1',\n",
       "   'F438-1',\n",
       "   'F439-1',\n",
       "   'F44-1',\n",
       "   'F440-1',\n",
       "   'F441-1',\n",
       "   'F442-1',\n",
       "   'F443-1',\n",
       "   'F444-1',\n",
       "   'F445-1',\n",
       "   'F446-1',\n",
       "   'F447-1',\n",
       "   'F448-1',\n",
       "   'F449-1',\n",
       "   'F45-1',\n",
       "   'F450-1',\n",
       "   'F451-1',\n",
       "   'F452-1',\n",
       "   'F453-1',\n",
       "   'F454-1',\n",
       "   'F458-1',\n",
       "   'F459-1',\n",
       "   'F48-1',\n",
       "   'F480-1',\n",
       "   'F481-1',\n",
       "   'F488-1',\n",
       "   'F489-1',\n",
       "   'F50-1',\n",
       "   'F500-1',\n",
       "   'F501-1',\n",
       "   'F502-1',\n",
       "   'F503-1',\n",
       "   'F504-1',\n",
       "   'F505-1',\n",
       "   'F508-1',\n",
       "   'F509-1',\n",
       "   'F51-1',\n",
       "   'F510-1',\n",
       "   'F511-1',\n",
       "   'F512-1',\n",
       "   'F513-1',\n",
       "   'F514-1',\n",
       "   'F515-1',\n",
       "   'F518-1',\n",
       "   'F519-1',\n",
       "   'F52-1',\n",
       "   'F520-1',\n",
       "   'F521-1',\n",
       "   'F522-1',\n",
       "   'F523-1',\n",
       "   'F524-1',\n",
       "   'F525-1',\n",
       "   'F526-1',\n",
       "   'F527-1',\n",
       "   'F528-1',\n",
       "   'F529-1',\n",
       "   'F53-1',\n",
       "   'F530-1',\n",
       "   'F531-1',\n",
       "   'F538-1',\n",
       "   'F539-1',\n",
       "   'F54-1',\n",
       "   'F55-1',\n",
       "   'F59-1',\n",
       "   'F60-1',\n",
       "   'F600-1',\n",
       "   'F601-1',\n",
       "   'F602-1',\n",
       "   'F603-1',\n",
       "   'F604-1',\n",
       "   'F605-1',\n",
       "   'F606-1',\n",
       "   'F607-1',\n",
       "   'F608-1',\n",
       "   'F609-1',\n",
       "   'F61-1',\n",
       "   'F62-1',\n",
       "   'F620-1',\n",
       "   'F621-1',\n",
       "   'F628-1',\n",
       "   'F629-1',\n",
       "   'F63-1',\n",
       "   'F630-1',\n",
       "   'F631-1',\n",
       "   'F632-1',\n",
       "   'F633-1',\n",
       "   'F638-1',\n",
       "   'F639-1',\n",
       "   'F64-1',\n",
       "   'F640-1',\n",
       "   'F641-1',\n",
       "   'F642-1',\n",
       "   'F648-1',\n",
       "   'F649-1',\n",
       "   'F65-1',\n",
       "   'F650-1',\n",
       "   'F651-1',\n",
       "   'F652-1',\n",
       "   'F653-1',\n",
       "   'F654-1',\n",
       "   'F655-1',\n",
       "   'F656-1',\n",
       "   'F658-1',\n",
       "   'F659-1',\n",
       "   'F66-1',\n",
       "   'F660-1',\n",
       "   'F661-1',\n",
       "   'F662-1',\n",
       "   'F668-1',\n",
       "   'F669-1',\n",
       "   'F68-1',\n",
       "   'F680-1',\n",
       "   'F681-1',\n",
       "   'F688-1',\n",
       "   'F69-1',\n",
       "   'F70-1',\n",
       "   'F700-1',\n",
       "   'F701-1',\n",
       "   'F708-1',\n",
       "   'F709-1',\n",
       "   'F71-1',\n",
       "   'F710-1',\n",
       "   'F711-1',\n",
       "   'F718-1',\n",
       "   'F719-1',\n",
       "   'F72-1',\n",
       "   'F720-1',\n",
       "   'F721-1',\n",
       "   'F728-1',\n",
       "   'F729-1',\n",
       "   'F73-1',\n",
       "   'F730-1',\n",
       "   'F731-1',\n",
       "   'F738-1',\n",
       "   'F739-1',\n",
       "   'F78-1',\n",
       "   'F780-1',\n",
       "   'F781-1',\n",
       "   'F788-1',\n",
       "   'F789-1',\n",
       "   'F79-1',\n",
       "   'F790-1',\n",
       "   'F791-1',\n",
       "   'F798-1',\n",
       "   'F799-1',\n",
       "   'F80-1',\n",
       "   'F800-1',\n",
       "   'F801-1',\n",
       "   'F802-1',\n",
       "   'F803-1',\n",
       "   'F808-1',\n",
       "   'F809-1',\n",
       "   'F81-1',\n",
       "   'F810-1',\n",
       "   'F811-1',\n",
       "   'F812-1',\n",
       "   'F813-1',\n",
       "   'F818-1',\n",
       "   'F819-1',\n",
       "   'F82-1',\n",
       "   'F83-1',\n",
       "   'F84-1',\n",
       "   'F840-1',\n",
       "   'F841-1',\n",
       "   'F842-1',\n",
       "   'F843-1',\n",
       "   'F844-1',\n",
       "   'F845-1',\n",
       "   'F848-1',\n",
       "   'F849-1',\n",
       "   'F88-1',\n",
       "   'F89-1',\n",
       "   'F90-1',\n",
       "   'F900-1',\n",
       "   'F901-1',\n",
       "   'F908-1',\n",
       "   'F909-1',\n",
       "   'F91-1',\n",
       "   'F910-1',\n",
       "   'F911-1',\n",
       "   'F912-1',\n",
       "   'F913-1',\n",
       "   'F918-1',\n",
       "   'F919-1',\n",
       "   'F92-1',\n",
       "   'F920-1',\n",
       "   'F928-1',\n",
       "   'F929-1',\n",
       "   'F93-1',\n",
       "   'F930-1',\n",
       "   'F931-1',\n",
       "   'F932-1',\n",
       "   'F933-1',\n",
       "   'F938-1',\n",
       "   'F939-1',\n",
       "   'F94-1',\n",
       "   'F940-1',\n",
       "   'F941-1',\n",
       "   'F942-1',\n",
       "   'F948-1',\n",
       "   'F949-1',\n",
       "   'F95-1',\n",
       "   'F950-1',\n",
       "   'F951-1',\n",
       "   'F952-1',\n",
       "   'F958-1',\n",
       "   'F959-1',\n",
       "   'F98-1',\n",
       "   'F980-1',\n",
       "   'F981-1',\n",
       "   'F982-1',\n",
       "   'F983-1',\n",
       "   'F984-1',\n",
       "   'F985-1',\n",
       "   'F986-1',\n",
       "   'F988-1',\n",
       "   'F989-1',\n",
       "   'F99-1'],\n",
       "  'linha': 'CID',\n",
       "  'coluna': 'NU_COMPETENCIA',\n",
       "  'ug': 'regiao',\n",
       "  'uf': '43',\n",
       "  'tpProducao': '4',\n",
       "  'headless_browser': False,\n",
       "  'joinby_col': 'CIAP/CID',\n",
       "  'perform_join': True,\n",
       "  'extract_code': True,\n",
       "  'group_and_sum': True,\n",
       "  'convert_labels': True,\n",
       "  'perform_grand_total': True,\n",
       "  'save_to_excel': True,\n",
       "  'download_dir': 'D:\\\\CodeStuff\\\\Stats\\\\colab_linear_regression\\\\linear_regression\\\\sisab_scrap_download\\\\sudeste\\\\cid_f',\n",
       "  'regioes': ['SUDESTE']},\n",
       " 'nordeste': {'ano_range': [2016, 2024],\n",
       "  'cid_list': ['F00-1',\n",
       "   'F000-1',\n",
       "   'F001-1',\n",
       "   'F002-1',\n",
       "   'F009-1',\n",
       "   'F01-1',\n",
       "   'F010-1',\n",
       "   'F011-1',\n",
       "   'F012-1',\n",
       "   'F013-1',\n",
       "   'F018-1',\n",
       "   'F019-1',\n",
       "   'F02-1',\n",
       "   'F020-1',\n",
       "   'F021-1',\n",
       "   'F022-1',\n",
       "   'F023-1',\n",
       "   'F024-1',\n",
       "   'F028-1',\n",
       "   'F03-1',\n",
       "   'F04-1',\n",
       "   'F05-1',\n",
       "   'F050-1',\n",
       "   'F051-1',\n",
       "   'F058-1',\n",
       "   'F059-1',\n",
       "   'F06-1',\n",
       "   'F060-1',\n",
       "   'F061-1',\n",
       "   'F062-1',\n",
       "   'F063-1',\n",
       "   'F064-1',\n",
       "   'F065-1',\n",
       "   'F066-1',\n",
       "   'F067-1',\n",
       "   'F068-1',\n",
       "   'F069-1',\n",
       "   'F07-1',\n",
       "   'F070-1',\n",
       "   'F071-1',\n",
       "   'F072-1',\n",
       "   'F078-1',\n",
       "   'F079-1',\n",
       "   'F09-1',\n",
       "   'F20-1',\n",
       "   'F200-1',\n",
       "   'F201-1',\n",
       "   'F202-1',\n",
       "   'F203-1',\n",
       "   'F204-1',\n",
       "   'F205-1',\n",
       "   'F206-1',\n",
       "   'F208-1',\n",
       "   'F209-1',\n",
       "   'F21-1',\n",
       "   'F22-1',\n",
       "   'F220-1',\n",
       "   'F228-1',\n",
       "   'F229-1',\n",
       "   'F23-1',\n",
       "   'F230-1',\n",
       "   'F231-1',\n",
       "   'F232-1',\n",
       "   'F233-1',\n",
       "   'F238-1',\n",
       "   'F239-1',\n",
       "   'F24-1',\n",
       "   'F25-1',\n",
       "   'F250-1',\n",
       "   'F251-1',\n",
       "   'F252-1',\n",
       "   'F258-1',\n",
       "   'F259-1',\n",
       "   'F28-1',\n",
       "   'F29-1',\n",
       "   'F30-1',\n",
       "   'F300-1',\n",
       "   'F301-1',\n",
       "   'F302-1',\n",
       "   'F308-1',\n",
       "   'F309-1',\n",
       "   'F31-1',\n",
       "   'F310-1',\n",
       "   'F311-1',\n",
       "   'F312-1',\n",
       "   'F313-1',\n",
       "   'F314-1',\n",
       "   'F315-1',\n",
       "   'F316-1',\n",
       "   'F317-1',\n",
       "   'F318-1',\n",
       "   'F319-1',\n",
       "   'F32-1',\n",
       "   'F320-1',\n",
       "   'F321-1',\n",
       "   'F322-1',\n",
       "   'F323-1',\n",
       "   'F328-1',\n",
       "   'F329-1',\n",
       "   'F33-1',\n",
       "   'F330-1',\n",
       "   'F331-1',\n",
       "   'F332-1',\n",
       "   'F333-1',\n",
       "   'F334-1',\n",
       "   'F338-1',\n",
       "   'F339-1',\n",
       "   'F34-1',\n",
       "   'F340-1',\n",
       "   'F341-1',\n",
       "   'F348-1',\n",
       "   'F349-1',\n",
       "   'F38-1',\n",
       "   'F380-1',\n",
       "   'F381-1',\n",
       "   'F388-1',\n",
       "   'F39-1',\n",
       "   'F40-1',\n",
       "   'F400-1',\n",
       "   'F401-1',\n",
       "   'F402-1',\n",
       "   'F408-1',\n",
       "   'F409-1',\n",
       "   'F41-1',\n",
       "   'F410-1',\n",
       "   'F411-1',\n",
       "   'F412-1',\n",
       "   'F413-1',\n",
       "   'F418-1',\n",
       "   'F419-1',\n",
       "   'F42-1',\n",
       "   'F420-1',\n",
       "   'F421-1',\n",
       "   'F422-1',\n",
       "   'F428-1',\n",
       "   'F429-1',\n",
       "   'F43-1',\n",
       "   'F430-1',\n",
       "   'F431-1',\n",
       "   'F432-1',\n",
       "   'F438-1',\n",
       "   'F439-1',\n",
       "   'F44-1',\n",
       "   'F440-1',\n",
       "   'F441-1',\n",
       "   'F442-1',\n",
       "   'F443-1',\n",
       "   'F444-1',\n",
       "   'F445-1',\n",
       "   'F446-1',\n",
       "   'F447-1',\n",
       "   'F448-1',\n",
       "   'F449-1',\n",
       "   'F45-1',\n",
       "   'F450-1',\n",
       "   'F451-1',\n",
       "   'F452-1',\n",
       "   'F453-1',\n",
       "   'F454-1',\n",
       "   'F458-1',\n",
       "   'F459-1',\n",
       "   'F48-1',\n",
       "   'F480-1',\n",
       "   'F481-1',\n",
       "   'F488-1',\n",
       "   'F489-1',\n",
       "   'F50-1',\n",
       "   'F500-1',\n",
       "   'F501-1',\n",
       "   'F502-1',\n",
       "   'F503-1',\n",
       "   'F504-1',\n",
       "   'F505-1',\n",
       "   'F508-1',\n",
       "   'F509-1',\n",
       "   'F51-1',\n",
       "   'F510-1',\n",
       "   'F511-1',\n",
       "   'F512-1',\n",
       "   'F513-1',\n",
       "   'F514-1',\n",
       "   'F515-1',\n",
       "   'F518-1',\n",
       "   'F519-1',\n",
       "   'F52-1',\n",
       "   'F520-1',\n",
       "   'F521-1',\n",
       "   'F522-1',\n",
       "   'F523-1',\n",
       "   'F524-1',\n",
       "   'F525-1',\n",
       "   'F526-1',\n",
       "   'F527-1',\n",
       "   'F528-1',\n",
       "   'F529-1',\n",
       "   'F53-1',\n",
       "   'F530-1',\n",
       "   'F531-1',\n",
       "   'F538-1',\n",
       "   'F539-1',\n",
       "   'F54-1',\n",
       "   'F55-1',\n",
       "   'F59-1',\n",
       "   'F60-1',\n",
       "   'F600-1',\n",
       "   'F601-1',\n",
       "   'F602-1',\n",
       "   'F603-1',\n",
       "   'F604-1',\n",
       "   'F605-1',\n",
       "   'F606-1',\n",
       "   'F607-1',\n",
       "   'F608-1',\n",
       "   'F609-1',\n",
       "   'F61-1',\n",
       "   'F62-1',\n",
       "   'F620-1',\n",
       "   'F621-1',\n",
       "   'F628-1',\n",
       "   'F629-1',\n",
       "   'F63-1',\n",
       "   'F630-1',\n",
       "   'F631-1',\n",
       "   'F632-1',\n",
       "   'F633-1',\n",
       "   'F638-1',\n",
       "   'F639-1',\n",
       "   'F64-1',\n",
       "   'F640-1',\n",
       "   'F641-1',\n",
       "   'F642-1',\n",
       "   'F648-1',\n",
       "   'F649-1',\n",
       "   'F65-1',\n",
       "   'F650-1',\n",
       "   'F651-1',\n",
       "   'F652-1',\n",
       "   'F653-1',\n",
       "   'F654-1',\n",
       "   'F655-1',\n",
       "   'F656-1',\n",
       "   'F658-1',\n",
       "   'F659-1',\n",
       "   'F66-1',\n",
       "   'F660-1',\n",
       "   'F661-1',\n",
       "   'F662-1',\n",
       "   'F668-1',\n",
       "   'F669-1',\n",
       "   'F68-1',\n",
       "   'F680-1',\n",
       "   'F681-1',\n",
       "   'F688-1',\n",
       "   'F69-1',\n",
       "   'F70-1',\n",
       "   'F700-1',\n",
       "   'F701-1',\n",
       "   'F708-1',\n",
       "   'F709-1',\n",
       "   'F71-1',\n",
       "   'F710-1',\n",
       "   'F711-1',\n",
       "   'F718-1',\n",
       "   'F719-1',\n",
       "   'F72-1',\n",
       "   'F720-1',\n",
       "   'F721-1',\n",
       "   'F728-1',\n",
       "   'F729-1',\n",
       "   'F73-1',\n",
       "   'F730-1',\n",
       "   'F731-1',\n",
       "   'F738-1',\n",
       "   'F739-1',\n",
       "   'F78-1',\n",
       "   'F780-1',\n",
       "   'F781-1',\n",
       "   'F788-1',\n",
       "   'F789-1',\n",
       "   'F79-1',\n",
       "   'F790-1',\n",
       "   'F791-1',\n",
       "   'F798-1',\n",
       "   'F799-1',\n",
       "   'F80-1',\n",
       "   'F800-1',\n",
       "   'F801-1',\n",
       "   'F802-1',\n",
       "   'F803-1',\n",
       "   'F808-1',\n",
       "   'F809-1',\n",
       "   'F81-1',\n",
       "   'F810-1',\n",
       "   'F811-1',\n",
       "   'F812-1',\n",
       "   'F813-1',\n",
       "   'F818-1',\n",
       "   'F819-1',\n",
       "   'F82-1',\n",
       "   'F83-1',\n",
       "   'F84-1',\n",
       "   'F840-1',\n",
       "   'F841-1',\n",
       "   'F842-1',\n",
       "   'F843-1',\n",
       "   'F844-1',\n",
       "   'F845-1',\n",
       "   'F848-1',\n",
       "   'F849-1',\n",
       "   'F88-1',\n",
       "   'F89-1',\n",
       "   'F90-1',\n",
       "   'F900-1',\n",
       "   'F901-1',\n",
       "   'F908-1',\n",
       "   'F909-1',\n",
       "   'F91-1',\n",
       "   'F910-1',\n",
       "   'F911-1',\n",
       "   'F912-1',\n",
       "   'F913-1',\n",
       "   'F918-1',\n",
       "   'F919-1',\n",
       "   'F92-1',\n",
       "   'F920-1',\n",
       "   'F928-1',\n",
       "   'F929-1',\n",
       "   'F93-1',\n",
       "   'F930-1',\n",
       "   'F931-1',\n",
       "   'F932-1',\n",
       "   'F933-1',\n",
       "   'F938-1',\n",
       "   'F939-1',\n",
       "   'F94-1',\n",
       "   'F940-1',\n",
       "   'F941-1',\n",
       "   'F942-1',\n",
       "   'F948-1',\n",
       "   'F949-1',\n",
       "   'F95-1',\n",
       "   'F950-1',\n",
       "   'F951-1',\n",
       "   'F952-1',\n",
       "   'F958-1',\n",
       "   'F959-1',\n",
       "   'F98-1',\n",
       "   'F980-1',\n",
       "   'F981-1',\n",
       "   'F982-1',\n",
       "   'F983-1',\n",
       "   'F984-1',\n",
       "   'F985-1',\n",
       "   'F986-1',\n",
       "   'F988-1',\n",
       "   'F989-1',\n",
       "   'F99-1'],\n",
       "  'linha': 'CID',\n",
       "  'coluna': 'NU_COMPETENCIA',\n",
       "  'ug': 'regiao',\n",
       "  'uf': '43',\n",
       "  'tpProducao': '4',\n",
       "  'headless_browser': False,\n",
       "  'joinby_col': 'CIAP/CID',\n",
       "  'perform_join': True,\n",
       "  'extract_code': True,\n",
       "  'group_and_sum': True,\n",
       "  'convert_labels': True,\n",
       "  'perform_grand_total': True,\n",
       "  'save_to_excel': True,\n",
       "  'download_dir': 'D:\\\\CodeStuff\\\\Stats\\\\colab_linear_regression\\\\linear_regression\\\\sisab_scrap_download\\\\nordeste\\\\cid_f',\n",
       "  'regioes': ['NORDESTE']},\n",
       " 'sul_total': {'ano_range': [2016, 2024],\n",
       "  'cid_list': [],\n",
       "  'linha': 'BRASIL',\n",
       "  'coluna': 'NU_COMPETENCIA',\n",
       "  'ug': 'regiao',\n",
       "  'uf': '43',\n",
       "  'tpProducao': '4',\n",
       "  'headless_browser': False,\n",
       "  'joinby_col': 'Brasil',\n",
       "  'perform_join': True,\n",
       "  'extract_code': False,\n",
       "  'group_and_sum': False,\n",
       "  'convert_labels': True,\n",
       "  'perform_grand_total': False,\n",
       "  'save_to_excel': True,\n",
       "  'download_dir': 'D:\\\\CodeStuff\\\\Stats\\\\colab_linear_regression\\\\linear_regression\\\\sisab_scrap_download\\\\sul\\\\total_atendimentos',\n",
       "  'regioes': ['SUL']},\n",
       " 'norte_total': {'ano_range': [2016, 2024],\n",
       "  'cid_list': [],\n",
       "  'linha': 'BRASIL',\n",
       "  'coluna': 'NU_COMPETENCIA',\n",
       "  'ug': 'regiao',\n",
       "  'uf': '43',\n",
       "  'tpProducao': '4',\n",
       "  'headless_browser': False,\n",
       "  'joinby_col': 'Brasil',\n",
       "  'perform_join': True,\n",
       "  'extract_code': False,\n",
       "  'group_and_sum': False,\n",
       "  'convert_labels': True,\n",
       "  'perform_grand_total': False,\n",
       "  'save_to_excel': True,\n",
       "  'download_dir': 'D:\\\\CodeStuff\\\\Stats\\\\colab_linear_regression\\\\linear_regression\\\\sisab_scrap_download\\\\norte\\\\total_atendimentos',\n",
       "  'regioes': ['NORTE']},\n",
       " 'centro_oeste_total': {'ano_range': [2016, 2024],\n",
       "  'cid_list': [],\n",
       "  'linha': 'BRASIL',\n",
       "  'coluna': 'NU_COMPETENCIA',\n",
       "  'ug': 'regiao',\n",
       "  'uf': '43',\n",
       "  'tpProducao': '4',\n",
       "  'headless_browser': False,\n",
       "  'joinby_col': 'Brasil',\n",
       "  'perform_join': True,\n",
       "  'extract_code': False,\n",
       "  'group_and_sum': False,\n",
       "  'convert_labels': True,\n",
       "  'perform_grand_total': False,\n",
       "  'save_to_excel': True,\n",
       "  'download_dir': 'D:\\\\CodeStuff\\\\Stats\\\\colab_linear_regression\\\\linear_regression\\\\sisab_scrap_download\\\\centro_oeste\\\\total_atendimentos',\n",
       "  'regioes': ['CENTRO-OESTE']},\n",
       " 'sudeste_total': {'ano_range': [2016, 2024],\n",
       "  'cid_list': [],\n",
       "  'linha': 'BRASIL',\n",
       "  'coluna': 'NU_COMPETENCIA',\n",
       "  'ug': 'regiao',\n",
       "  'uf': '43',\n",
       "  'tpProducao': '4',\n",
       "  'headless_browser': False,\n",
       "  'joinby_col': 'Brasil',\n",
       "  'perform_join': True,\n",
       "  'extract_code': False,\n",
       "  'group_and_sum': False,\n",
       "  'convert_labels': True,\n",
       "  'perform_grand_total': False,\n",
       "  'save_to_excel': True,\n",
       "  'download_dir': 'D:\\\\CodeStuff\\\\Stats\\\\colab_linear_regression\\\\linear_regression\\\\sisab_scrap_download\\\\sudeste\\\\total_atendimentos',\n",
       "  'regioes': ['SUDESTE']},\n",
       " 'nordeste_total': {'ano_range': [2016, 2024],\n",
       "  'cid_list': [],\n",
       "  'linha': 'BRASIL',\n",
       "  'coluna': 'NU_COMPETENCIA',\n",
       "  'ug': 'regiao',\n",
       "  'uf': '43',\n",
       "  'tpProducao': '4',\n",
       "  'headless_browser': False,\n",
       "  'joinby_col': 'Brasil',\n",
       "  'perform_join': True,\n",
       "  'extract_code': False,\n",
       "  'group_and_sum': False,\n",
       "  'convert_labels': True,\n",
       "  'perform_grand_total': False,\n",
       "  'save_to_excel': True,\n",
       "  'download_dir': 'D:\\\\CodeStuff\\\\Stats\\\\colab_linear_regression\\\\linear_regression\\\\sisab_scrap_download\\\\nordeste\\\\total_atendimentos',\n",
       "  'regioes': ['NORDESTE']}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VARIABLES_TABLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb42c61",
   "metadata": {},
   "source": [
    "# Scrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5ec4c307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(args, url, download_dir):\n",
    "    DROPDOWN_BUTTON_PATH = \"/html/body/div[2]/div/section[2]/span/form/div[3]/div[1]/div/div[2]/div[2]/div[5]/div/div/div[2]/button\"\n",
    "    CSV_BUTTON_PATH = \"/html/body/div[2]/div/section[2]/span/form/div[3]/div[1]/div/div[2]/div[2]/div[5]/div/div/div[2]/ul/li[2]/a\"\n",
    "    \n",
    "    CID_CIAP_ADD = '//*[@id=\"btnAddCid\"]'\n",
    "    CID_CIAP_SELECTION = \"/html/body/div[2]/div/section[2]/span/form/div[1]/div/div/div[2]/div/div/div/div[2]/div/table/tbody\"\n",
    "    CONCLUIR_CID_CIAP_SELECTION = '/html/body/div[2]/div/section[2]/span/form/div[1]/div/div/div[2]/button[2]'\n",
    "    \n",
    "    COMPETENCIA_DROPDOWN_BUTTON_PATH = '/html/body/div[2]/div/section[2]/span/form/div[3]/div[1]/div/div[2]/div[2]/div[2]/div/div[2]/span/div/button'\n",
    "    COMPETENCIA_CHECKBOX_WRAPPER = \"/html/body/div[2]/div/section[2]/span/form/div[3]/div[1]/div/div[2]/div[2]/div[2]/div/div[2]/span/div/ul\"\n",
    "    \n",
    "    TIPO_PRODUCAO = '//*[@id=\"tpProducao\"]'\n",
    "    LINHA_SELECT = '//*[@id=\"selectLinha\"]'\n",
    "    COLUNA_SELECT = '//*[@id=\"selectcoluna\"]'\n",
    "    UG_SELECT = '//*[@id=\"unidGeo\"]'\n",
    "    UF_SELECT = '//*[@id=\"estadoMunicipio\"]'\n",
    "    \n",
    "    UG_DROPDOWN_BUTTON_PATH = '/html/body/div[2]/div/section[2]/span/form/div[3]/div[1]/div/div[2]/div[2]/div[1]/div/div[2]/span/div/button'\n",
    "    UG_CHECKBOX_WRAPPER = '/html/body/div[2]/div/section[2]/span/form/div[3]/div[1]/div/div[2]/div[2]/div[1]/div/div[2]/span/div/ul'\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # Cria o diretório de download se ele não existir\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "    \n",
    "\n",
    "    preffix = PREFFIX\n",
    "    file_name = f\"{preffix}{args['ano']}.csv\"  # Nome do arquivo desejado\n",
    "    #temp_file_path = os.path.join(download_dir, file_name + \".crdownload\")\n",
    "    final_file_path = os.path.join(download_dir, file_name)\n",
    "\n",
    "    if os.path.exists(final_file_path):\n",
    "        print(f\"------------ SKIPPING {args['ano']}: The file '{final_file_path}' already exists.\")\n",
    "        return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Configurações do Chrome headless (sem abrir janela)\n",
    "    options = Options()\n",
    "    if args['headless_browser']:\n",
    "        options.add_argument(\"--headless=new\")\n",
    "    \n",
    "    # Configura o caminho de download\n",
    "    prefs = {\n",
    "    \"download.default_directory\": download_dir,\n",
    "    \"download.prompt_for_download\": False,  # Não perguntar onde salvar\n",
    "    \"download.directory_upgrade\": True,    # Atualizar o diretório automaticamente\n",
    "    \"safebrowsing.enabled\": True           # Desativar verificações de segurança para downloads\n",
    "    }\n",
    "    options.add_experimental_option(\"prefs\", prefs)\n",
    "    \n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    print(driver)\n",
    "    try:\n",
    "        # Acessa a página\n",
    "        \n",
    "        driver.get(url)\n",
    "\n",
    "        # Espera o JS carregar\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        wait.until(EC.presence_of_element_located((By.NAME, \"javax.faces.ViewState\")))\n",
    "\n",
    "        ##################################################################################\n",
    "\n",
    "        select_option_by_value(driver, LINHA_SELECT, args['linha'])\n",
    "        time.sleep(1)\n",
    "        select_option_by_value(driver, COLUNA_SELECT, args['coluna'])\n",
    "        time.sleep(1)\n",
    "        select_option_by_value(driver, UG_SELECT, args['ug'])\n",
    "        time.sleep(1)\n",
    "        select_option_by_value(driver, TIPO_PRODUCAO, args['tpProducao'])\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "        AtivarListaDeCheckBoxes(driver, criar_lista_competencias_por_ano(args['ano']), COMPETENCIA_DROPDOWN_BUTTON_PATH, COMPETENCIA_CHECKBOX_WRAPPER)\n",
    "        time.sleep(1)\n",
    "\n",
    "        ug_dropdown_select_arg = args['municipios'] if args['ug'] == \"municipio\" else args['estados'] if args['ug'] == 'estado' else args['regioes'] if args['ug'] == \"regiao\" else None\n",
    "\n",
    "        if ug_dropdown_select_arg:\n",
    "            if args['ug'] == \"municipio\":\n",
    "                select_option_by_value(driver, UF_SELECT, args['uf'])\n",
    "                time.sleep(1)\n",
    "            AtivarListaDeCheckBoxes(driver,ug_dropdown_select_arg, UG_DROPDOWN_BUTTON_PATH, UG_CHECKBOX_WRAPPER)\n",
    "            time.sleep(1)\n",
    "\n",
    "\n",
    "\n",
    "        ### CID CIAP\n",
    "        cid_Ciap = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, CID_CIAP_ADD))\n",
    "        )\n",
    "        cid_Ciap.click()\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH, CID_CIAP_SELECTION)))\n",
    "        ExecuteCIDfilters(driver, args['cid_list'])\n",
    "        cid_Ciap_conclude = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, CONCLUIR_CID_CIAP_SELECTION))\n",
    "        )\n",
    "        cid_Ciap_conclude.click()\n",
    "        time.sleep(1)\n",
    "\n",
    "        ###################################################################\n",
    "        # Clica no botão de download (exportar)\n",
    "        dropdown_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, DROPDOWN_BUTTON_PATH))\n",
    "        )\n",
    "\n",
    "        # Usa ActionChains para garantir o clique\n",
    "        actions = ActionChains(driver)\n",
    "        actions.move_to_element(dropdown_button).click().perform()\n",
    "        print(\"Dropdown do botão de download clicado.\")\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Espera o botão de download CSV aparecer e clica nele\n",
    "        try:\n",
    "            csv_opt_button = WebDriverWait(driver, 30).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, CSV_BUTTON_PATH))\n",
    "            )\n",
    "            actions.move_to_element(csv_opt_button).click().perform()\n",
    "            print(\"Botão de download clicado via ActionChains.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao localizar ou clicar no botão de download: {e}\")\n",
    "\n",
    "        ###################################################################\n",
    "\n",
    "\n",
    "        \n",
    "        start_time = time.time()\n",
    "        timeout = 360\n",
    "\n",
    "        def check_file_exists(download_dir, preffix):\n",
    "            \"\"\"Checks if a temporary or final file exists in the download directory.\"\"\"\n",
    "            for file in os.listdir(download_dir):\n",
    "                if file.endswith(\".tmp\") or file.endswith('.crdownload') or (file.endswith(\".csv\") and not file.startswith(preffix)):\n",
    "                    return file  # Return the name of the file found\n",
    "            return None\n",
    "        def check_file_exists(download_dir, preffix):\n",
    "            \"\"\"\n",
    "            Verifica se existe um arquivo temporário (.tmp) ou o arquivo CSV original (sem prefixo) na pasta.\n",
    "            \"\"\"\n",
    "            for file in os.listdir(download_dir):\n",
    "                if file.endswith(\".tmp\") or file.endswith('.crdownload') or (file.endswith(\".csv\") and not file.startswith(preffix)):\n",
    "                    return file  # Retorna o nome do arquivo encontrado\n",
    "            return None\n",
    "\n",
    "        while True:\n",
    "            # Check if the timeout has been exceeded\n",
    "            elapsed_time = time.time() - start_time\n",
    "            if elapsed_time > timeout:\n",
    "                print(\"Timeout reached. Download not completed.\")\n",
    "                driver.quit()\n",
    "                return False\n",
    "\n",
    "            # Check if a file exists\n",
    "            file_found = check_file_exists(download_dir, preffix)\n",
    "\n",
    "            if file_found:\n",
    "                # If it's a temporary file, wait for it to finish\n",
    "                if file_found.endswith(\".tmp\") or file_found.endswith('.crdownload'):\n",
    "                    print(\"Temporary file found. Waiting for download to complete...\")\n",
    "                    time.sleep(10)\n",
    "                    continue\n",
    "                # If it's the final CSV file, rename and exit\n",
    "                elif file_found.endswith(\".csv\") and not file_found.startswith(preffix):\n",
    "                    final_file_path = os.path.join(download_dir, file_name)\n",
    "                    source_file_path = os.path.join(download_dir, file_found)\n",
    "\n",
    "                    # Check if the destination file already exists\n",
    "                    if os.path.exists(final_file_path):\n",
    "                        print(f\"File {final_file_path} already exists. Overwriting...\")\n",
    "                        os.remove(final_file_path)  # Remove the existing file\n",
    "\n",
    "                    os.rename(source_file_path, final_file_path)\n",
    "                    print(f\"File saved as: {final_file_path}\")\n",
    "                    driver.quit()\n",
    "                    return True\n",
    "            else:\n",
    "                # If no file is found, wait and continue checking\n",
    "                print(\"No file found. Waiting...\")\n",
    "                time.sleep(10)\n",
    "    finally:\n",
    "        # Fecha o driver\n",
    "        driver.quit()\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed36387",
   "metadata": {},
   "source": [
    "# Joiner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "07ff5369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_files(download_dir, prefix, column_name):\n",
    "    \"\"\"\n",
    "    Processes CSV files in a directory, extracts metadata, and returns a summary of unique values.\n",
    "\n",
    "    Args:\n",
    "        download_dir (str): Directory containing the CSV files.\n",
    "        prefix (str): Prefix to filter the files.\n",
    "        column_name (str): Column name to extract unique values.\n",
    "        cid_grup_ref_path (str, optional): Path to the CID group reference file. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing unique values from the specified column.\n",
    "    \"\"\"\n",
    "    df_list = []\n",
    "    #maindf_info = {'variaveis': []}\n",
    "\n",
    "    def get_start_and_end_lines_skip_metadata(file_path):\n",
    "        with open(file_path, 'r', encoding='latin1') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        start_line = None\n",
    "        end_line = None\n",
    "        n_brnco_lines = 0\n",
    "        for i, line in enumerate(lines):\n",
    "            if line == '\\n':\n",
    "                n_brnco_lines += 1\n",
    "            if n_brnco_lines == 2:\n",
    "                start_line = i + 1\n",
    "                break\n",
    "\n",
    "        n_brnco_lines = 0\n",
    "        for i, line in enumerate(lines[start_line:], start=start_line):\n",
    "            if line == '\\n':\n",
    "                n_brnco_lines += 1\n",
    "            if n_brnco_lines == 2:\n",
    "                end_line = i - 2\n",
    "                break\n",
    "        return start_line, end_line\n",
    "\n",
    "    for file in os.listdir(download_dir):\n",
    "        if file.endswith(\".csv\") and file.startswith(prefix):\n",
    "            file_path = os.path.join(download_dir, file)\n",
    "            \n",
    "            start_line, end_line = get_start_and_end_lines_skip_metadata(file_path)\n",
    "            print(f\"Processing file: {file}, Start line: {start_line}, End line: {end_line}\")\n",
    "            \n",
    "            # Read the file as a DataFrame\n",
    "            df = pd.read_csv(\n",
    "                file_path, \n",
    "                sep=';', \n",
    "                encoding='latin1', \n",
    "                skiprows=start_line, \n",
    "                nrows=end_line - start_line, \n",
    "                dtype=str  # Read all columns as strings\n",
    "            )\n",
    "            \n",
    "            # Clean and convert columns\n",
    "            for col in df.columns:\n",
    "                df[col] = df[col].str.replace('.', '', regex=False).str.replace(',', '.', regex=False)\n",
    "                try:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            \n",
    "            df_list.append(df)\n",
    "\n",
    "    # Extract unique values from the specified column\n",
    "    #for i, df in enumerate(df_list):\n",
    "    #    if column_name in df.columns:  # Ensure the column exists\n",
    "    #        maindf_info['variaveis'].extend(df[column_name].unique())  # Add unique values from the column\n",
    "#\n",
    "    ## Remove duplicates from the final list\n",
    "    #maindf_info['variaveis'] = list(set(maindf_info['variaveis']))\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "66529956",
   "metadata": {},
   "outputs": [],
   "source": [
    "### joiner helper functions\n",
    "def join_dfs(df_list, joinby_col):\n",
    "    # Step 1: Extract unique CIAP/CID values\n",
    "    maindf_info = {\n",
    "        'variaveis': []\n",
    "    }\n",
    "\n",
    "    for i, df in enumerate(df_list):\n",
    "        if joinby_col in df.columns:  # Ensure the column exists\n",
    "            maindf_info['variaveis'].extend(df[joinby_col].unique())  # Add unique values from the column\n",
    "\n",
    "    # Remove duplicates from the final list\n",
    "    unique_ciap_cid = list(set(maindf_info['variaveis']))\n",
    "\n",
    "    # Step 2: Create a new DataFrame with CIAP/CID as the index\n",
    "    result_df = pd.DataFrame(index=unique_ciap_cid)\n",
    "\n",
    "    # Step 3: Iterate through all DataFrames and append their columns\n",
    "    for i, df in enumerate(df_list):\n",
    "        if joinby_col in df.columns:  # Ensure the column exists\n",
    "            df = df.set_index(joinby_col)  # Set CIAP/CID as the index\n",
    "            for col in df.columns:  # Iterate through the columns\n",
    "                if col not in result_df.columns:\n",
    "                    result_df[col] = 0  # Initialize the column with 0\n",
    "                result_df[col] = result_df[col].add(df[col], fill_value=0)  # Add values, filling missing with 0\n",
    "\n",
    "    # Step 4: Reset the index if you want CIAP/CID as a column instead of the index\n",
    "    result_df.reset_index(inplace=True)\n",
    "    result_df.rename(columns={'index': joinby_col}, inplace=True)\n",
    "\n",
    "    # Fill any remaining NaN values with 0\n",
    "    result_df.fillna(0, inplace=True)\n",
    "\n",
    "    for col in result_df.columns[1:]:  # Skip the COLUMN_NAME column\n",
    "        result_df[col] = result_df[col].astype(int)\n",
    "\n",
    "\n",
    "    # Print the resulting DataFrame\n",
    "    return result_df\n",
    "\n",
    "def extract_code_from_parentheses(df, joinby_col, new_column_name='Code'):\n",
    "    \"\"\"\n",
    "    Extracts the code between parentheses from the specified column and adds it as a new column.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame to process.\n",
    "    - column_name (str): The name of the column to extract the code from.\n",
    "    - new_column_name (str): The name of the new column to store the extracted codes.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The updated DataFrame with the new column.\n",
    "    \"\"\"\n",
    "    if not joinby_col:\n",
    "        raise ValueError(\"No column name provided for extraction.\")\n",
    "    if joinby_col in df.columns:\n",
    "        # Use regex to extract the content between parentheses\n",
    "        df[new_column_name] = df[joinby_col].str.extract(r'\\((.*?)\\)')\n",
    "    else:\n",
    "        print(f\"Column '{joinby_col}' not found in the DataFrame.\")\n",
    "    return df\n",
    "\n",
    "def group_and_sum_by_cid_group(df, cid_grup_ref_df, cid_column='CID_code', value_columns=None):\n",
    "    \"\"\"\n",
    "    Groups the DataFrame by CID groups and sums the values.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame containing the CID codes and values.\n",
    "    - cid_grup_ref_df (pd.DataFrame): The reference DataFrame mapping CID codes to groups.\n",
    "    - cid_column (str): The column in `df` containing the CID codes.\n",
    "    - value_columns (list): The columns in `df` to sum. If None, all numeric columns will be summed.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A new DataFrame grouped by CID groups with summed values.\n",
    "    \"\"\"\n",
    "    # Merge the main DataFrame with the reference DataFrame to map CID codes to groups\n",
    "    merged_df = df.merge(cid_grup_ref_df, left_on=cid_column, right_on='cid_ind_cod', how='left')\n",
    "    #return merged_df\n",
    "    # Rename the column to avoid overwriting\n",
    "    #merged_df.rename(columns={cid_column: 'Original_CID_code'}, inplace=True)\n",
    "\n",
    "    # Replace CID codes with their group names\n",
    "    #merged_df['CID_group'] = merged_df['cid_grup_nome']\n",
    "\n",
    "    # Drop unnecessary columns from the merge\n",
    "    #merged_df.drop(['cid_grup_cod', 'cid_grup_nome', 'cid_ind_cod', 'Original_CID_code'], axis=1, inplace=True)\n",
    "\n",
    "    # Group by the CID group names and sum the values\n",
    "\n",
    "    if value_columns is None:\n",
    "        value_columns = merged_df.select_dtypes(include='number').columns  # Default to numeric columns\n",
    "    grouped_df = merged_df.groupby('cid_grup_nome')[value_columns].sum().reset_index()\n",
    "    grouped_df.drop(columns=['CID_code'], inplace=True, errors='ignore')\n",
    "\n",
    "    return grouped_df, merged_df\n",
    "\n",
    "\n",
    "def convert_column_labels_to_datetime(df):\n",
    "    \"\"\"\n",
    "    Converts column labels with Portuguese month names into datetime objects.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame with column labels to convert.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The DataFrame with updated column labels.\n",
    "    \"\"\"\n",
    "    # Define a mapping for Portuguese month abbreviations to numbers\n",
    "    month_mapping = {\n",
    "        \"JAN\": \"01\", \"FEV\": \"02\", \"MAR\": \"03\", \"ABR\": \"04\", \"MAI\": \"05\", \"JUN\": \"06\",\n",
    "        \"JUL\": \"07\", \"AGO\": \"08\", \"SET\": \"09\", \"OUT\": \"10\", \"NOV\": \"11\", \"DEZ\": \"12\"\n",
    "    }\n",
    "\n",
    "    # Create a new dictionary for updated column labels\n",
    "    new_columns = {}\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            # Replace Portuguese month abbreviations with numbers\n",
    "            updated_col = col.upper()\n",
    "            for month, num in month_mapping.items():\n",
    "                updated_col = updated_col.replace(month, num)\n",
    "            # Convert to datetime if possible\n",
    "            new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n",
    "        except Exception:\n",
    "            # If conversion fails, keep the original column name\n",
    "            new_columns[col] = col\n",
    "    df.rename(columns=new_columns, inplace=True)\n",
    "    return df\n",
    "def add_grand_total_row(df, joinby_col):\n",
    "    \"\"\"\n",
    "    Adiciona uma linha ao DataFrame com a soma de todas as colunas numéricas.\n",
    "    A linha será chamada 'Grand Total'.\n",
    "    \"\"\"\n",
    "    # Calcula a soma de todas as colunas numéricas\n",
    "    first_col = df.columns[0]  # Preserva a primeira coluna (joinby_col)\n",
    "    total_row = df.select_dtypes(include='number').sum()\n",
    "    \n",
    "    # Adiciona a linha ao DataFrame\n",
    "    total_row[first_col] = \"Grand Total\"  # Define o nome da linha\n",
    "    df = pd.concat([df, pd.DataFrame([total_row])], ignore_index=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "47d8ef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dfs(\n",
    "    df_list,\n",
    "    cid_grup_ref_df,\n",
    "    joinby_col,\n",
    "    output_file_path=None,\n",
    "    perform_join=True,\n",
    "    extract_code=False,\n",
    "    group_and_sum=False,\n",
    "    convert_labels=True,\n",
    "    perform_grand_total=False,\n",
    "    save_to_excel=True,\n",
    "):\n",
    "    results = {}\n",
    "\n",
    "    # Step 1: Join DataFrames\n",
    "    if perform_join:\n",
    "        joined_df = join_dfs(df_list, joinby_col=joinby_col)\n",
    "        results['joined_df'] = joined_df\n",
    "    else:\n",
    "        joined_df = df_list  # assume it's already joined or passed as a single df\n",
    "\n",
    "    # Step 2: Extract code from parentheses\n",
    "    if extract_code:\n",
    "        coded_df = extract_code_from_parentheses(joined_df, joinby_col=joinby_col, new_column_name='CID_code')\n",
    "        results['coded_df'] = coded_df\n",
    "    else:\n",
    "        coded_df = joined_df\n",
    "\n",
    "    # Step 3: Group and sum by CID group\n",
    "    if group_and_sum:\n",
    "        grouped_df, test_grouped_df = group_and_sum_by_cid_group(\n",
    "            coded_df,\n",
    "            cid_grup_ref_df,\n",
    "            cid_column='CID_code',\n",
    "            value_columns=None  # or coded_df.columns[1:] if needed\n",
    "        )\n",
    "        results['grouped_df'] = grouped_df\n",
    "        results['test_grouped_df'] = test_grouped_df\n",
    "    else:\n",
    "        grouped_df = coded_df\n",
    "\n",
    "    # Step 4: Convert column labels to datetime\n",
    "    if convert_labels:\n",
    "        dated_df = convert_column_labels_to_datetime(grouped_df)\n",
    "        results['dated_df'] = dated_df\n",
    "    else:\n",
    "        dated_df = grouped_df\n",
    "\n",
    "    # Step 5: Check for \"UNNAMED\" columns\n",
    "    unnamed_columns = [col for col in dated_df.columns if \"UNNAMED\" in str(col).upper()]\n",
    "    if unnamed_columns:\n",
    "        for col in unnamed_columns:\n",
    "            if dated_df[col].sum() == 0:  # Check if all values are zero\n",
    "                print(f\"=========WARNING=========: Column '{col}' is unnamed and contains only zeros. It will be removed.\")\n",
    "            else:\n",
    "                print(f\"=========WARNING=========: Column '{col}' is unnamed but contains non-zero values. It will still be removed.\")\n",
    "        dated_df.drop(columns=unnamed_columns, inplace=True)\n",
    "\n",
    "\t# Step 6: Add a grand total row if required\n",
    "    if perform_grand_total:\n",
    "        dated_df = add_grand_total_row(dated_df, joinby_col=joinby_col)\n",
    "        results['dated_df_with_grand_total'] = dated_df\n",
    "    else:    \n",
    "        dated_df = dated_df\n",
    "\n",
    "    # Step 7: Save to Excel\n",
    "    if save_to_excel and output_file_path is not None:\n",
    "        output_dir = os.path.dirname(output_file_path)\n",
    "            \n",
    "        # Create the directory if it doesn't exist\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            \n",
    "        dated_df.to_excel(output_file_path, index=False, engine='openpyxl')\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1718ce75",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "904a9d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(var_table, url):\n",
    "    \"\"\"\n",
    "    Função para baixar dados de uma série de anos.\n",
    "    \"\"\"\n",
    "    cid_grup_ref_df = pd.read_excel(\"D:\\\\CodeStuff\\\\Stats\\\\colab_linear_regression\\\\linear_regression\\\\sisab_scrap_download\\\\_input\\\\cid_f_grupos_referencia.xlsx\")\n",
    "\n",
    "    for variable, var_args in var_table.items():\n",
    "        download_dir = var_args['download_dir']\n",
    "        args = copy.deepcopy(var_args)\n",
    "        for ano in range(args['ano_range'][0], args['ano_range'][1] + 1):\n",
    "            args['ano'] = ano\n",
    "            max_retries = MAX_TRIES_DOWNLOAD  # Set the maximum number of retries\n",
    "            retries = 0\n",
    "\n",
    "            while retries < max_retries:\n",
    "                data_dowloaded = download_data(args, url, download_dir)\n",
    "                if data_dowloaded:\n",
    "                    print(f\"Download successful for year {ano}.\")\n",
    "                    break  # Exit the retry loop if download is successful\n",
    "                else:\n",
    "                    retries += 1\n",
    "                    print(f\"Retrying download for year {ano}... Attempt {retries}/{max_retries}\")\n",
    "                    time.sleep(5)  # Wait before retrying\n",
    "\n",
    "            if not data_dowloaded:\n",
    "                print(f\"Failed to download data for year {ano} after {max_retries} attempts.\")\n",
    "        # Optionally, handle the failure (e.g., log it, skip to the next year, etc.)\n",
    "\n",
    "        csv_list = process_csv_files(download_dir, PREFFIX, args['joinby_col'])\n",
    "        results = process_dfs(\n",
    "            df_list=csv_list,\n",
    "            cid_grup_ref_df=cid_grup_ref_df,\n",
    "            joinby_col=args['joinby_col'],\n",
    "            output_file_path=args['download_dir']+f\"\\\\joined_files\\\\joined_{variable}.xlsx\",\n",
    "            perform_join=args['perform_join'],\n",
    "            extract_code=args['extract_code'],\n",
    "            group_and_sum=args['group_and_sum'],\n",
    "            convert_labels=args['convert_labels'],\n",
    "            perform_grand_total=args['perform_grand_total'],\n",
    "            save_to_excel=args['save_to_excel'],\n",
    "        )\n",
    "    print(\"Download e processamento concluídos.\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e9d87",
   "metadata": {},
   "source": [
    "# Defs Incidence Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fbd2ac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def CreateIncidenceRatioDF(cid_f_args, total_atd_args, scaling_factor = 1000):\n",
    "    \"\"\"\n",
    "    Creates a new DataFrame with the incidence ratio between the numerator and denominator.\n",
    "    \n",
    "    Args:\n",
    "        extra_args (dict): A dictionary where each key has a value that is another dictionary containing a `download_dir` key.\n",
    "        total_atd_name (str): A constant representing the folder name for the denominator data.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary where each key corresponds to the key in `extra_args`, and the value is the resulting incidence ratio DataFrame.\n",
    "    \"\"\"\n",
    "    incidence_ratio_results = {}\n",
    "\n",
    "    for key, value in cid_f_args.items():\n",
    "        # Paths for numerator and denominator\n",
    "        total_atd_arg = total_atd_args[key+'_total']\n",
    "\n",
    "        numerator_dir = os.path.join(value['download_dir'], \"joined_files\")\n",
    "        denominator_dir = os.path.join(total_atd_arg['download_dir'], \"joined_files\")\n",
    "\n",
    "        \n",
    "        # Find the numerator and denominator CSV files\n",
    "        numerator_file = next((f for f in os.listdir(numerator_dir) if f.startswith(\"joined_\") and f.endswith(\".xlsx\")), None)\n",
    "        denominator_file = next((f for f in os.listdir(denominator_dir) if f.startswith(\"joined_\") and f.endswith(\".xlsx\")), None)\n",
    "        print(denominator_file)\n",
    "        if not numerator_file or not denominator_file:\n",
    "            print(f\"Missing files for key '{key}'. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Load the numerator and denominator DataFrames\n",
    "        numerator_df = pd.read_excel(os.path.join(numerator_dir, numerator_file), index_col=0)\n",
    "        denominator_df = pd.read_excel(os.path.join(denominator_dir, denominator_file), index_col=0)\n",
    "        \n",
    "        # Ensure the denominator has only one row\n",
    "        if len(denominator_df) != 1:\n",
    "            raise ValueError(f\"Denominator file for key '{key}' must have exactly one row.\")\n",
    "        \n",
    "        # Extract the single row from the denominator\n",
    "        denominator_row = denominator_df.iloc[0]\n",
    "        \n",
    "        # Create the incidence ratio DataFrame\n",
    "        incidence_ratio_df = numerator_df.copy()\n",
    "        for col in numerator_df.columns:\n",
    "            if col in denominator_row.index:\n",
    "                incidence_ratio_df[col] = numerator_df[col] / denominator_row[col] * scaling_factor  # Multiply by 1000 for incidence rate\n",
    "            else:\n",
    "                print(f\"Column '{col}' not found in denominator for key '{key}'. Setting values to NaN.\")\n",
    "                incidence_ratio_df[col] = float('nan')\n",
    "        \n",
    "        # Store the result in the dictionary\n",
    "        incidence_ratio_results[key] = incidence_ratio_df\n",
    "    \n",
    "    return incidence_ratio_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6bcfa7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExportIRdf(ir_df, download_dir, file_name):\n",
    "    \"\"\"\n",
    "    Exports the incidence ratio DataFrame to an Excel file.\n",
    "    \n",
    "    Args:\n",
    "        ir_df (pd.DataFrame): The incidence ratio DataFrame to export.\n",
    "        download_dir (str): The directory where the file will be saved.\n",
    "        file_name (str): The name of the output file.\n",
    "    \"\"\"\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "    \n",
    "    # Save the DataFrame to an Excel file\n",
    "    output_file_path = os.path.join(download_dir, file_name)\n",
    "    ir_df.to_excel(output_file_path, index=True, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "04d08f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractAndSaveGrandTotals(incidence_ratio_results, output_dir, output_file_name):\n",
    "    \"\"\"\n",
    "    Extracts the Grand Total row from all incidence ratio DataFrames and consolidates them into a new file.\n",
    "    \n",
    "    Args:\n",
    "        incidence_ratio_results (dict): A dictionary where keys are region names and values are DataFrames.\n",
    "        output_dir (str): The directory where the consolidated file will be saved.\n",
    "        output_file_name (str): The name of the output file.\n",
    "    \"\"\"\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Initialize an empty DataFrame to store the Grand Totals\n",
    "    grand_totals_df = pd.DataFrame()\n",
    "\n",
    "    for region, df in incidence_ratio_results.items():\n",
    "        # Extract the Grand Total row\n",
    "        grand_total_row = df[df.index == \"Grand Total\"]\n",
    "        \n",
    "        if not grand_total_row.empty:\n",
    "            # Include the index as a column before transposing\n",
    "            grand_total_row = grand_total_row.reset_index()\n",
    "            grand_total_row = grand_total_row.iloc[:, 1:].T  # Transpose to make columns into rows\n",
    "            grand_total_row.columns = [region]  # Rename the column to the region name\n",
    "            \n",
    "            # Add the Grand Total row to the consolidated DataFrame\n",
    "            grand_totals_df = pd.concat([grand_totals_df, grand_total_row], axis=1)\n",
    "        else:\n",
    "            print(f\"Grand Total row not found for region '{region}'. Skipping...\")\n",
    "\n",
    "    # Save the consolidated DataFrame to an Excel file\n",
    "    output_file_path = os.path.join(output_dir, output_file_name)\n",
    "    grand_totals_df = grand_totals_df.T  # Transpose to make it more readable\n",
    "    grand_totals_df.to_excel(output_file_path, index=True, engine=\"openpyxl\")\n",
    "    print(f\"Grand Totals saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2d7c97",
   "metadata": {},
   "source": [
    "# EXEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e7131bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ SKIPPING 2016: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sul\\cid_f\\scrapped_relatorio_2016.csv' already exists.\n",
      "Download successful for year 2016.\n",
      "------------ SKIPPING 2017: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sul\\cid_f\\scrapped_relatorio_2017.csv' already exists.\n",
      "Download successful for year 2017.\n",
      "------------ SKIPPING 2018: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sul\\cid_f\\scrapped_relatorio_2018.csv' already exists.\n",
      "Download successful for year 2018.\n",
      "------------ SKIPPING 2019: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sul\\cid_f\\scrapped_relatorio_2019.csv' already exists.\n",
      "Download successful for year 2019.\n",
      "------------ SKIPPING 2020: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sul\\cid_f\\scrapped_relatorio_2020.csv' already exists.\n",
      "Download successful for year 2020.\n",
      "------------ SKIPPING 2021: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sul\\cid_f\\scrapped_relatorio_2021.csv' already exists.\n",
      "Download successful for year 2021.\n",
      "------------ SKIPPING 2022: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sul\\cid_f\\scrapped_relatorio_2022.csv' already exists.\n",
      "Download successful for year 2022.\n",
      "------------ SKIPPING 2023: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sul\\cid_f\\scrapped_relatorio_2023.csv' already exists.\n",
      "Download successful for year 2023.\n",
      "------------ SKIPPING 2024: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sul\\cid_f\\scrapped_relatorio_2024.csv' already exists.\n",
      "Download successful for year 2024.\n",
      "Processing file: scrapped_relatorio_2016.csv, Start line: 9, End line: 362\n",
      "Processing file: scrapped_relatorio_2017.csv, Start line: 9, End line: 362\n",
      "Processing file: scrapped_relatorio_2018.csv, Start line: 9, End line: 360\n",
      "Processing file: scrapped_relatorio_2019.csv, Start line: 9, End line: 364\n",
      "Processing file: scrapped_relatorio_2020.csv, Start line: 9, End line: 363\n",
      "Processing file: scrapped_relatorio_2021.csv, Start line: 9, End line: 361\n",
      "Processing file: scrapped_relatorio_2022.csv, Start line: 9, End line: 364\n",
      "Processing file: scrapped_relatorio_2023.csv, Start line: 9, End line: 363\n",
      "Processing file: scrapped_relatorio_2024.csv, Start line: 9, End line: 365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df.reset_index(inplace=True)\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:57: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_column_name] = df[joinby_col].str.extract(r'\\((.*?)\\)')\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:122: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:122: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:122: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========WARNING=========: Column 'UNNAMED: 13' is unnamed and contains only zeros. It will be removed.\n",
      "------------ SKIPPING 2016: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\norte\\cid_f\\scrapped_relatorio_2016.csv' already exists.\n",
      "Download successful for year 2016.\n",
      "------------ SKIPPING 2017: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\norte\\cid_f\\scrapped_relatorio_2017.csv' already exists.\n",
      "Download successful for year 2017.\n",
      "------------ SKIPPING 2018: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\norte\\cid_f\\scrapped_relatorio_2018.csv' already exists.\n",
      "Download successful for year 2018.\n",
      "------------ SKIPPING 2019: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\norte\\cid_f\\scrapped_relatorio_2019.csv' already exists.\n",
      "Download successful for year 2019.\n",
      "------------ SKIPPING 2020: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\norte\\cid_f\\scrapped_relatorio_2020.csv' already exists.\n",
      "Download successful for year 2020.\n",
      "------------ SKIPPING 2021: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\norte\\cid_f\\scrapped_relatorio_2021.csv' already exists.\n",
      "Download successful for year 2021.\n",
      "------------ SKIPPING 2022: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\norte\\cid_f\\scrapped_relatorio_2022.csv' already exists.\n",
      "Download successful for year 2022.\n",
      "------------ SKIPPING 2023: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\norte\\cid_f\\scrapped_relatorio_2023.csv' already exists.\n",
      "Download successful for year 2023.\n",
      "------------ SKIPPING 2024: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\norte\\cid_f\\scrapped_relatorio_2024.csv' already exists.\n",
      "Download successful for year 2024.\n",
      "Processing file: scrapped_relatorio_2016.csv, Start line: 9, End line: 344\n",
      "Processing file: scrapped_relatorio_2017.csv, Start line: 9, End line: 354\n",
      "Processing file: scrapped_relatorio_2018.csv, Start line: 9, End line: 356\n",
      "Processing file: scrapped_relatorio_2019.csv, Start line: 9, End line: 356\n",
      "Processing file: scrapped_relatorio_2020.csv, Start line: 9, End line: 361\n",
      "Processing file: scrapped_relatorio_2021.csv, Start line: 9, End line: 355\n",
      "Processing file: scrapped_relatorio_2022.csv, Start line: 9, End line: 361\n",
      "Processing file: scrapped_relatorio_2023.csv, Start line: 9, End line: 357\n",
      "Processing file: scrapped_relatorio_2024.csv, Start line: 9, End line: 361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df.reset_index(inplace=True)\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:57: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_column_name] = df[joinby_col].str.extract(r'\\((.*?)\\)')\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:122: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:122: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:122: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========WARNING=========: Column 'UNNAMED: 13' is unnamed and contains only zeros. It will be removed.\n",
      "------------ SKIPPING 2016: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\centro_oeste\\cid_f\\scrapped_relatorio_2016.csv' already exists.\n",
      "Download successful for year 2016.\n",
      "------------ SKIPPING 2017: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\centro_oeste\\cid_f\\scrapped_relatorio_2017.csv' already exists.\n",
      "Download successful for year 2017.\n",
      "------------ SKIPPING 2018: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\centro_oeste\\cid_f\\scrapped_relatorio_2018.csv' already exists.\n",
      "Download successful for year 2018.\n",
      "------------ SKIPPING 2019: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\centro_oeste\\cid_f\\scrapped_relatorio_2019.csv' already exists.\n",
      "Download successful for year 2019.\n",
      "------------ SKIPPING 2020: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\centro_oeste\\cid_f\\scrapped_relatorio_2020.csv' already exists.\n",
      "Download successful for year 2020.\n",
      "------------ SKIPPING 2021: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\centro_oeste\\cid_f\\scrapped_relatorio_2021.csv' already exists.\n",
      "Download successful for year 2021.\n",
      "------------ SKIPPING 2022: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\centro_oeste\\cid_f\\scrapped_relatorio_2022.csv' already exists.\n",
      "Download successful for year 2022.\n",
      "------------ SKIPPING 2023: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\centro_oeste\\cid_f\\scrapped_relatorio_2023.csv' already exists.\n",
      "Download successful for year 2023.\n",
      "------------ SKIPPING 2024: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\centro_oeste\\cid_f\\scrapped_relatorio_2024.csv' already exists.\n",
      "Download successful for year 2024.\n",
      "Processing file: scrapped_relatorio_2016.csv, Start line: 9, End line: 356\n",
      "Processing file: scrapped_relatorio_2017.csv, Start line: 9, End line: 353\n",
      "Processing file: scrapped_relatorio_2018.csv, Start line: 9, End line: 358\n",
      "Processing file: scrapped_relatorio_2019.csv, Start line: 9, End line: 356\n",
      "Processing file: scrapped_relatorio_2020.csv, Start line: 9, End line: 356\n",
      "Processing file: scrapped_relatorio_2021.csv, Start line: 9, End line: 360\n",
      "Processing file: scrapped_relatorio_2022.csv, Start line: 9, End line: 362\n",
      "Processing file: scrapped_relatorio_2023.csv, Start line: 9, End line: 359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df.reset_index(inplace=True)\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:57: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_column_name] = df[joinby_col].str.extract(r'\\((.*?)\\)')\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:122: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:122: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:122: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: scrapped_relatorio_2024.csv, Start line: 9, End line: 363\n",
      "=========WARNING=========: Column 'UNNAMED: 13' is unnamed and contains only zeros. It will be removed.\n",
      "------------ SKIPPING 2016: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sudeste\\cid_f\\scrapped_relatorio_2016.csv' already exists.\n",
      "Download successful for year 2016.\n",
      "------------ SKIPPING 2017: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sudeste\\cid_f\\scrapped_relatorio_2017.csv' already exists.\n",
      "Download successful for year 2017.\n",
      "------------ SKIPPING 2018: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sudeste\\cid_f\\scrapped_relatorio_2018.csv' already exists.\n",
      "Download successful for year 2018.\n",
      "------------ SKIPPING 2019: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sudeste\\cid_f\\scrapped_relatorio_2019.csv' already exists.\n",
      "Download successful for year 2019.\n",
      "------------ SKIPPING 2020: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sudeste\\cid_f\\scrapped_relatorio_2020.csv' already exists.\n",
      "Download successful for year 2020.\n",
      "------------ SKIPPING 2021: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sudeste\\cid_f\\scrapped_relatorio_2021.csv' already exists.\n",
      "Download successful for year 2021.\n",
      "------------ SKIPPING 2022: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sudeste\\cid_f\\scrapped_relatorio_2022.csv' already exists.\n",
      "Download successful for year 2022.\n",
      "------------ SKIPPING 2023: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sudeste\\cid_f\\scrapped_relatorio_2023.csv' already exists.\n",
      "Download successful for year 2023.\n",
      "------------ SKIPPING 2024: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sudeste\\cid_f\\scrapped_relatorio_2024.csv' already exists.\n",
      "Download successful for year 2024.\n",
      "Processing file: scrapped_relatorio_2016.csv, Start line: 9, End line: 362\n",
      "Processing file: scrapped_relatorio_2017.csv, Start line: 9, End line: 362\n",
      "Processing file: scrapped_relatorio_2018.csv, Start line: 9, End line: 365\n",
      "Processing file: scrapped_relatorio_2019.csv, Start line: 9, End line: 365\n",
      "Processing file: scrapped_relatorio_2020.csv, Start line: 9, End line: 364\n",
      "Processing file: scrapped_relatorio_2021.csv, Start line: 9, End line: 364\n",
      "Processing file: scrapped_relatorio_2022.csv, Start line: 9, End line: 366\n",
      "Processing file: scrapped_relatorio_2023.csv, Start line: 9, End line: 366\n",
      "Processing file: scrapped_relatorio_2024.csv, Start line: 9, End line: 365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df.reset_index(inplace=True)\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:57: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_column_name] = df[joinby_col].str.extract(r'\\((.*?)\\)')\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:122: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:122: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:122: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========WARNING=========: Column 'UNNAMED: 13' is unnamed and contains only zeros. It will be removed.\n",
      "------------ SKIPPING 2016: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\nordeste\\cid_f\\scrapped_relatorio_2016.csv' already exists.\n",
      "Download successful for year 2016.\n",
      "------------ SKIPPING 2017: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\nordeste\\cid_f\\scrapped_relatorio_2017.csv' already exists.\n",
      "Download successful for year 2017.\n",
      "------------ SKIPPING 2018: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\nordeste\\cid_f\\scrapped_relatorio_2018.csv' already exists.\n",
      "Download successful for year 2018.\n",
      "------------ SKIPPING 2019: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\nordeste\\cid_f\\scrapped_relatorio_2019.csv' already exists.\n",
      "Download successful for year 2019.\n",
      "------------ SKIPPING 2020: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\nordeste\\cid_f\\scrapped_relatorio_2020.csv' already exists.\n",
      "Download successful for year 2020.\n",
      "------------ SKIPPING 2021: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\nordeste\\cid_f\\scrapped_relatorio_2021.csv' already exists.\n",
      "Download successful for year 2021.\n",
      "------------ SKIPPING 2022: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\nordeste\\cid_f\\scrapped_relatorio_2022.csv' already exists.\n",
      "Download successful for year 2022.\n",
      "------------ SKIPPING 2023: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\nordeste\\cid_f\\scrapped_relatorio_2023.csv' already exists.\n",
      "Download successful for year 2023.\n",
      "------------ SKIPPING 2024: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\nordeste\\cid_f\\scrapped_relatorio_2024.csv' already exists.\n",
      "Download successful for year 2024.\n",
      "Processing file: scrapped_relatorio_2016.csv, Start line: 9, End line: 358\n",
      "Processing file: scrapped_relatorio_2017.csv, Start line: 9, End line: 362\n",
      "Processing file: scrapped_relatorio_2018.csv, Start line: 9, End line: 364\n",
      "Processing file: scrapped_relatorio_2019.csv, Start line: 9, End line: 362\n",
      "Processing file: scrapped_relatorio_2020.csv, Start line: 9, End line: 360\n",
      "Processing file: scrapped_relatorio_2021.csv, Start line: 9, End line: 364\n",
      "Processing file: scrapped_relatorio_2022.csv, Start line: 9, End line: 360\n",
      "Processing file: scrapped_relatorio_2023.csv, Start line: 9, End line: 365\n",
      "Processing file: scrapped_relatorio_2024.csv, Start line: 9, End line: 363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df.reset_index(inplace=True)\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:57: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[new_column_name] = df[joinby_col].str.extract(r'\\((.*?)\\)')\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:122: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:122: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:122: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========WARNING=========: Column 'UNNAMED: 13' is unnamed and contains only zeros. It will be removed.\n",
      "------------ SKIPPING 2016: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sul\\total_atendimentos\\scrapped_relatorio_2016.csv' already exists.\n",
      "Download successful for year 2016.\n",
      "------------ SKIPPING 2017: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sul\\total_atendimentos\\scrapped_relatorio_2017.csv' already exists.\n",
      "Download successful for year 2017.\n",
      "------------ SKIPPING 2018: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sul\\total_atendimentos\\scrapped_relatorio_2018.csv' already exists.\n",
      "Download successful for year 2018.\n",
      "------------ SKIPPING 2019: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sul\\total_atendimentos\\scrapped_relatorio_2019.csv' already exists.\n",
      "Download successful for year 2019.\n",
      "------------ SKIPPING 2020: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sul\\total_atendimentos\\scrapped_relatorio_2020.csv' already exists.\n",
      "Download successful for year 2020.\n",
      "------------ SKIPPING 2021: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sul\\total_atendimentos\\scrapped_relatorio_2021.csv' already exists.\n",
      "Download successful for year 2021.\n",
      "------------ SKIPPING 2022: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sul\\total_atendimentos\\scrapped_relatorio_2022.csv' already exists.\n",
      "Download successful for year 2022.\n",
      "------------ SKIPPING 2023: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sul\\total_atendimentos\\scrapped_relatorio_2023.csv' already exists.\n",
      "Download successful for year 2023.\n",
      "------------ SKIPPING 2024: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sul\\total_atendimentos\\scrapped_relatorio_2024.csv' already exists.\n",
      "Download successful for year 2024.\n",
      "Processing file: scrapped_relatorio_2016.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2017.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2018.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2019.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2020.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2021.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2022.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2023.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2024.csv, Start line: 8, End line: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df.reset_index(inplace=True)\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:122: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:122: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:122: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========WARNING=========: Column 'UNNAMED: 13' is unnamed and contains only zeros. It will be removed.\n",
      "------------ SKIPPING 2016: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\norte\\total_atendimentos\\scrapped_relatorio_2016.csv' already exists.\n",
      "Download successful for year 2016.\n",
      "------------ SKIPPING 2017: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\norte\\total_atendimentos\\scrapped_relatorio_2017.csv' already exists.\n",
      "Download successful for year 2017.\n",
      "------------ SKIPPING 2018: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\norte\\total_atendimentos\\scrapped_relatorio_2018.csv' already exists.\n",
      "Download successful for year 2018.\n",
      "------------ SKIPPING 2019: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\norte\\total_atendimentos\\scrapped_relatorio_2019.csv' already exists.\n",
      "Download successful for year 2019.\n",
      "------------ SKIPPING 2020: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\norte\\total_atendimentos\\scrapped_relatorio_2020.csv' already exists.\n",
      "Download successful for year 2020.\n",
      "------------ SKIPPING 2021: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\norte\\total_atendimentos\\scrapped_relatorio_2021.csv' already exists.\n",
      "Download successful for year 2021.\n",
      "------------ SKIPPING 2022: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\norte\\total_atendimentos\\scrapped_relatorio_2022.csv' already exists.\n",
      "Download successful for year 2022.\n",
      "------------ SKIPPING 2023: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\norte\\total_atendimentos\\scrapped_relatorio_2023.csv' already exists.\n",
      "Download successful for year 2023.\n",
      "------------ SKIPPING 2024: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\norte\\total_atendimentos\\scrapped_relatorio_2024.csv' already exists.\n",
      "Download successful for year 2024.\n",
      "Processing file: scrapped_relatorio_2016.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2017.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2018.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2019.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2020.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2021.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2022.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2023.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2024.csv, Start line: 8, End line: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df.reset_index(inplace=True)\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:122: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:122: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:122: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df.reset_index(inplace=True)\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:122: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:122: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:122: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========WARNING=========: Column 'UNNAMED: 13' is unnamed and contains only zeros. It will be removed.\n",
      "------------ SKIPPING 2016: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\centro_oeste\\total_atendimentos\\scrapped_relatorio_2016.csv' already exists.\n",
      "Download successful for year 2016.\n",
      "------------ SKIPPING 2017: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\centro_oeste\\total_atendimentos\\scrapped_relatorio_2017.csv' already exists.\n",
      "Download successful for year 2017.\n",
      "------------ SKIPPING 2018: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\centro_oeste\\total_atendimentos\\scrapped_relatorio_2018.csv' already exists.\n",
      "Download successful for year 2018.\n",
      "------------ SKIPPING 2019: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\centro_oeste\\total_atendimentos\\scrapped_relatorio_2019.csv' already exists.\n",
      "Download successful for year 2019.\n",
      "------------ SKIPPING 2020: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\centro_oeste\\total_atendimentos\\scrapped_relatorio_2020.csv' already exists.\n",
      "Download successful for year 2020.\n",
      "------------ SKIPPING 2021: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\centro_oeste\\total_atendimentos\\scrapped_relatorio_2021.csv' already exists.\n",
      "Download successful for year 2021.\n",
      "------------ SKIPPING 2022: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\centro_oeste\\total_atendimentos\\scrapped_relatorio_2022.csv' already exists.\n",
      "Download successful for year 2022.\n",
      "------------ SKIPPING 2023: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\centro_oeste\\total_atendimentos\\scrapped_relatorio_2023.csv' already exists.\n",
      "Download successful for year 2023.\n",
      "------------ SKIPPING 2024: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\centro_oeste\\total_atendimentos\\scrapped_relatorio_2024.csv' already exists.\n",
      "Download successful for year 2024.\n",
      "Processing file: scrapped_relatorio_2016.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2017.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2018.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2019.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2020.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2021.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2022.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2023.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2024.csv, Start line: 8, End line: 9\n",
      "=========WARNING=========: Column 'UNNAMED: 13' is unnamed and contains only zeros. It will be removed.\n",
      "------------ SKIPPING 2016: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sudeste\\total_atendimentos\\scrapped_relatorio_2016.csv' already exists.\n",
      "Download successful for year 2016.\n",
      "------------ SKIPPING 2017: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sudeste\\total_atendimentos\\scrapped_relatorio_2017.csv' already exists.\n",
      "Download successful for year 2017.\n",
      "------------ SKIPPING 2018: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sudeste\\total_atendimentos\\scrapped_relatorio_2018.csv' already exists.\n",
      "Download successful for year 2018.\n",
      "------------ SKIPPING 2019: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sudeste\\total_atendimentos\\scrapped_relatorio_2019.csv' already exists.\n",
      "Download successful for year 2019.\n",
      "------------ SKIPPING 2020: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sudeste\\total_atendimentos\\scrapped_relatorio_2020.csv' already exists.\n",
      "Download successful for year 2020.\n",
      "------------ SKIPPING 2021: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sudeste\\total_atendimentos\\scrapped_relatorio_2021.csv' already exists.\n",
      "Download successful for year 2021.\n",
      "------------ SKIPPING 2022: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sudeste\\total_atendimentos\\scrapped_relatorio_2022.csv' already exists.\n",
      "Download successful for year 2022.\n",
      "------------ SKIPPING 2023: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sudeste\\total_atendimentos\\scrapped_relatorio_2023.csv' already exists.\n",
      "Download successful for year 2023.\n",
      "------------ SKIPPING 2024: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sudeste\\total_atendimentos\\scrapped_relatorio_2024.csv' already exists.\n",
      "Download successful for year 2024.\n",
      "Processing file: scrapped_relatorio_2016.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2017.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2018.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2019.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2020.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2021.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2022.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2023.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2024.csv, Start line: 8, End line: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df.reset_index(inplace=True)\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:122: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:122: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:122: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========WARNING=========: Column 'UNNAMED: 13' is unnamed and contains only zeros. It will be removed.\n",
      "------------ SKIPPING 2016: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\nordeste\\total_atendimentos\\scrapped_relatorio_2016.csv' already exists.\n",
      "Download successful for year 2016.\n",
      "------------ SKIPPING 2017: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\nordeste\\total_atendimentos\\scrapped_relatorio_2017.csv' already exists.\n",
      "Download successful for year 2017.\n",
      "------------ SKIPPING 2018: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\nordeste\\total_atendimentos\\scrapped_relatorio_2018.csv' already exists.\n",
      "Download successful for year 2018.\n",
      "------------ SKIPPING 2019: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\nordeste\\total_atendimentos\\scrapped_relatorio_2019.csv' already exists.\n",
      "Download successful for year 2019.\n",
      "------------ SKIPPING 2020: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\nordeste\\total_atendimentos\\scrapped_relatorio_2020.csv' already exists.\n",
      "Download successful for year 2020.\n",
      "------------ SKIPPING 2021: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\nordeste\\total_atendimentos\\scrapped_relatorio_2021.csv' already exists.\n",
      "Download successful for year 2021.\n",
      "------------ SKIPPING 2022: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\nordeste\\total_atendimentos\\scrapped_relatorio_2022.csv' already exists.\n",
      "Download successful for year 2022.\n",
      "------------ SKIPPING 2023: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\nordeste\\total_atendimentos\\scrapped_relatorio_2023.csv' already exists.\n",
      "Download successful for year 2023.\n",
      "------------ SKIPPING 2024: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\nordeste\\total_atendimentos\\scrapped_relatorio_2024.csv' already exists.\n",
      "Download successful for year 2024.\n",
      "Processing file: scrapped_relatorio_2016.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2017.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2018.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2019.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2020.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2021.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2022.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2023.csv, Start line: 8, End line: 9\n",
      "Processing file: scrapped_relatorio_2024.csv, Start line: 8, End line: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3755173102.py:61: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df[col] = 0  # Initialize the column with 0\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df.reset_index(inplace=True)\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:122: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:122: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_14632\\3056129719.py:122: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========WARNING=========: Column 'UNNAMED: 13' is unnamed and contains only zeros. It will be removed.\n",
      "Download e processamento concluídos.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://sisab.saude.gov.br/paginas/acessoRestrito/relatorio/federal/saude/RelSauProducao.xhtml\"\n",
    "main(VARIABLES_TABLE, url)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c372609f",
   "metadata": {},
   "source": [
    "# EXEC IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "590f16b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joined_sul_total.xlsx\n",
      "joined_norte_total.xlsx\n",
      "joined_centro_oeste_total.xlsx\n",
      "joined_sudeste_total.xlsx\n",
      "joined_nordeste_total.xlsx\n",
      "Grand Totals saved to D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\_regions_joined\\regions_joined.xlsx\n"
     ]
    }
   ],
   "source": [
    "incidence_ratio = CreateIncidenceRatioDF(CID_F_ARGS,TOTAL_ATD_ARGS)\n",
    "for key, df in incidence_ratio.items():\n",
    "    # Export the DataFrame to an Excel file\n",
    "    download_dir = CID_F_ARGS[key]['download_dir'] + \"\\\\incidence_ratio\"\n",
    "    file_name = f\"incidence_ratio_{key}.xlsx\"\n",
    "    ExportIRdf(df, download_dir, file_name)\n",
    "    \n",
    "ExtractAndSaveGrandTotals(incidence_ratio, \"D:\\\\CodeStuff\\\\Stats\\\\colab_linear_regression\\\\linear_regression\\\\sisab_scrap_download\\\\_regions_joined\", \"regions_joined.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
