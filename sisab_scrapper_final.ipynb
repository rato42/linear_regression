{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ea4f7af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CID_CODES = [\n",
    "'F000',\n",
    "'F001',\n",
    "'F011',\n",
    "'F012',\n",
    "'F02',\n",
    "'F020',\n",
    "'F028',\n",
    "'F03',\n",
    "'F058',\n",
    "'F059',\n",
    "'F063',\n",
    "'F064',\n",
    "'F068',\n",
    "'F069',\n",
    "'F078',\n",
    "'F079',\n",
    "'F100',\n",
    "'F106',\n",
    "'F110',\n",
    "'F116',\n",
    "'F121',\n",
    "'F126',\n",
    "'F131',\n",
    "'F136',\n",
    "'F141',\n",
    "'F146',\n",
    "'F151',\n",
    "'F156',\n",
    "'F161',\n",
    "'F166',\n",
    "'F181',\n",
    "'F186',\n",
    "'F191',\n",
    "'F196',\n",
    "'F202',\n",
    "'F203',\n",
    "'F208',\n",
    "'F209',\n",
    "'F229',\n",
    "'F23',\n",
    "'F238',\n",
    "'F239',\n",
    "'F252',\n",
    "'F258',\n",
    "'F300',\n",
    "'F301',\n",
    "'F310',\n",
    "'F311',\n",
    "'F316',\n",
    "'F317',\n",
    "'F321',\n",
    "'F322',\n",
    "'F33',\n",
    "'F330',\n",
    "'F338',\n",
    "'F339',\n",
    "'F349',\n",
    "'F38',\n",
    "'F40',\n",
    "'F400',\n",
    "'F41',\n",
    "'F410',\n",
    "'F419',\n",
    "'F42',\n",
    "'F429',\n",
    "'F43',\n",
    "'F439',\n",
    "'F44',\n",
    "'F444',\n",
    "'F445',\n",
    "'F449',\n",
    "'F45',\n",
    "'F454',\n",
    "'F458',\n",
    "'F488',\n",
    "'F489',\n",
    "'F503',\n",
    "'F504',\n",
    "'F51',\n",
    "'F510',\n",
    "'F515',\n",
    "'F518',\n",
    "'F522',\n",
    "'F523',\n",
    "'F527',\n",
    "'F528',\n",
    "'F538',\n",
    "'F539',\n",
    "'F600',\n",
    "'F601',\n",
    "'F606',\n",
    "'F607',\n",
    "'F620',\n",
    "'F621',\n",
    "'F631',\n",
    "'F632',\n",
    "'F64',\n",
    "'F640',\n",
    "'F65',\n",
    "'F650',\n",
    "'F655',\n",
    "'F656',\n",
    "'F661',\n",
    "'F662',\n",
    "'F681',\n",
    "'F688',\n",
    "'F708',\n",
    "'F709',\n",
    "'F719',\n",
    "'F72',\n",
    "'F73',\n",
    "'F730',\n",
    "'F780',\n",
    "'F781',\n",
    "'F791',\n",
    "'F798',\n",
    "'F802',\n",
    "'F803',\n",
    "'F811',\n",
    "'F812',\n",
    "'F82',\n",
    "'F83',\n",
    "'F843',\n",
    "'F844',\n",
    "'F88',\n",
    "'F89',\n",
    "'F909',\n",
    "'F91',\n",
    "'F918',\n",
    "'F919',\n",
    "'F93',\n",
    "'F930',\n",
    "'F939',\n",
    "'F94',\n",
    "'F949',\n",
    "'F95',\n",
    "'F959',\n",
    "'F98',\n",
    "'F984',\n",
    "'F985',\n",
    "'F99',\n",
    "'F00',\n",
    "'F010',\n",
    "'F019',\n",
    "'F024',\n",
    "'F051',\n",
    "'F062',\n",
    "'F067',\n",
    "'F072',\n",
    "'F10',\n",
    "'F105',\n",
    "'F11',\n",
    "'F115',\n",
    "'F120',\n",
    "'F125',\n",
    "'F130',\n",
    "'F135',\n",
    "'F140',\n",
    "'F145',\n",
    "'F150',\n",
    "'F155',\n",
    "'F160',\n",
    "'F165',\n",
    "'F180',\n",
    "'F185',\n",
    "'F190',\n",
    "'F195',\n",
    "'F201',\n",
    "'F206',\n",
    "'F228',\n",
    "'F233',\n",
    "'F251',\n",
    "'F30',\n",
    "'F31',\n",
    "'F315',\n",
    "'F320',\n",
    "'F329',\n",
    "'F334',\n",
    "'F348',\n",
    "'F39',\n",
    "'F409',\n",
    "'F418',\n",
    "'F428',\n",
    "'F438',\n",
    "'F443',\n",
    "'F448',\n",
    "'F453',\n",
    "'F481',\n",
    "'F502',\n",
    "'F509',\n",
    "'F514',\n",
    "'F521',\n",
    "'F526',\n",
    "'F531',\n",
    "'F60',\n",
    "'F605',\n",
    "'F62',\n",
    "'F630',\n",
    "'F639',\n",
    "'F649',\n",
    "'F654',\n",
    "'F660',\n",
    "'F680',\n",
    "'F701',\n",
    "'F718',\n",
    "'F729',\n",
    "'F78',\n",
    "'F790',\n",
    "'F801',\n",
    "'F810',\n",
    "'F819',\n",
    "'F842',\n",
    "'F849',\n",
    "'F908',\n",
    "'F913',\n",
    "'F929',\n",
    "'F938',\n",
    "'F948',\n",
    "'F958',\n",
    "'F983',\n",
    "'F989',\n",
    "'F111',\n",
    "'F117',\n",
    "'F122',\n",
    "'F127',\n",
    "'F132',\n",
    "'F137',\n",
    "'F142',\n",
    "'F147',\n",
    "'F152',\n",
    "'F157',\n",
    "'F162',\n",
    "'F167',\n",
    "'F182',\n",
    "'F187',\n",
    "'F192',\n",
    "'F197',\n",
    "'F101',\n",
    "'F107',\n",
    "'F002',\n",
    "'F013',\n",
    "'F021',\n",
    "'F04',\n",
    "'F06',\n",
    "'F065',\n",
    "'F07',\n",
    "'F09',\n",
    "'F204',\n",
    "'F21',\n",
    "'F230',\n",
    "'F24',\n",
    "'F259',\n",
    "'F302',\n",
    "'F312',\n",
    "'F318',\n",
    "'F323',\n",
    "'F331',\n",
    "'F34',\n",
    "'F380',\n",
    "'F401',\n",
    "'F411',\n",
    "'F420',\n",
    "'F430',\n",
    "'F440',\n",
    "'F446',\n",
    "'F450',\n",
    "'F459',\n",
    "'F50',\n",
    "'F505',\n",
    "'F511',\n",
    "'F519',\n",
    "'F524',\n",
    "'F529',\n",
    "'F54',\n",
    "'F602',\n",
    "'F608',\n",
    "'F628',\n",
    "'F633',\n",
    "'F641',\n",
    "'F651',\n",
    "'F658',\n",
    "'F668',\n",
    "'F69',\n",
    "'F71',\n",
    "'F720',\n",
    "'F731',\n",
    "'F788',\n",
    "'F799',\n",
    "'F808',\n",
    "'F813',\n",
    "'F84',\n",
    "'F845',\n",
    "'F90',\n",
    "'F910',\n",
    "'F92',\n",
    "'F931',\n",
    "'F940',\n",
    "'F950',\n",
    "'F980',\n",
    "'F986',\n",
    "'F112',\n",
    "'F118',\n",
    "'F123',\n",
    "'F128',\n",
    "'F133',\n",
    "'F138',\n",
    "'F143',\n",
    "'F148',\n",
    "'F153',\n",
    "'F158',\n",
    "'F163',\n",
    "'F168',\n",
    "'F183',\n",
    "'F188',\n",
    "'F193',\n",
    "'F198',\n",
    "'F102',\n",
    "'F108',\n",
    "'F009',\n",
    "'F018',\n",
    "'F022',\n",
    "'F05',\n",
    "'F060',\n",
    "'F066',\n",
    "'F070',\n",
    "'F20',\n",
    "'F205',\n",
    "'F22',\n",
    "'F231',\n",
    "'F25',\n",
    "'F28',\n",
    "'F308',\n",
    "'F313',\n",
    "'F319',\n",
    "'F328',\n",
    "'F332',\n",
    "'F340',\n",
    "'F381',\n",
    "'F402',\n",
    "'F412',\n",
    "'F421',\n",
    "'F431',\n",
    "'F441',\n",
    "'F447',\n",
    "'F451',\n",
    "'F48',\n",
    "'F500',\n",
    "'F508',\n",
    "'F512',\n",
    "'F52',\n",
    "'F525',\n",
    "'F53',\n",
    "'F55',\n",
    "'F603',\n",
    "'F609',\n",
    "'F629',\n",
    "'F638',\n",
    "'F642',\n",
    "'F652',\n",
    "'F659',\n",
    "'F669',\n",
    "'F70',\n",
    "'F710',\n",
    "'F721',\n",
    "'F738',\n",
    "'F789',\n",
    "'F80',\n",
    "'F809',\n",
    "'F818',\n",
    "'F840',\n",
    "'F848',\n",
    "'F900',\n",
    "'F911',\n",
    "'F920',\n",
    "'F932',\n",
    "'F941',\n",
    "'F951',\n",
    "'F981',\n",
    "'F988',\n",
    "'F113',\n",
    "'F119',\n",
    "'F124',\n",
    "'F129',\n",
    "'F134',\n",
    "'F139',\n",
    "'F144',\n",
    "'F149',\n",
    "'F154',\n",
    "'F159',\n",
    "'F164',\n",
    "'F169',\n",
    "'F184',\n",
    "'F189',\n",
    "'F194',\n",
    "'F199',\n",
    "'F103',\n",
    "'F109',\n",
    "'F01',\n",
    "'F023',\n",
    "'F050',\n",
    "'F061',\n",
    "'F071',\n",
    "'F200',\n",
    "'F220',\n",
    "'F232',\n",
    "'F250',\n",
    "'F29',\n",
    "'F309',\n",
    "'F314',\n",
    "'F32',\n",
    "'F333',\n",
    "'F341',\n",
    "'F388',\n",
    "'F408',\n",
    "'F413',\n",
    "'F422',\n",
    "'F432',\n",
    "'F442',\n",
    "'F452',\n",
    "'F480',\n",
    "'F501',\n",
    "'F513',\n",
    "'F520',\n",
    "'F530',\n",
    "'F59',\n",
    "'F604',\n",
    "'F61',\n",
    "'F63',\n",
    "'F648',\n",
    "'F653',\n",
    "'F66',\n",
    "'F68',\n",
    "'F700',\n",
    "'F711',\n",
    "'F728',\n",
    "'F739',\n",
    "'F79',\n",
    "'F800',\n",
    "'F81',\n",
    "'F841',\n",
    "'F901',\n",
    "'F912',\n",
    "'F928',\n",
    "'F933',\n",
    "'F942',\n",
    "'F952',\n",
    "'F982',\n",
    "'F114',\n",
    "'F12',\n",
    "'F13',\n",
    "'F14',\n",
    "'F15',\n",
    "'F16',\n",
    "'F18',\n",
    "'F19',\n",
    "'F104',\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24f0865",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bc3070d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import pandas as pd\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03885df6",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e0f29c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cid_string(cid, suffixo = \"-1\"):\n",
    "    \"\"\"\n",
    "    Cria uma string CID formatada para ser usada no payload do POST.\n",
    "    \"\"\"\n",
    "    cid_string = cid + suffixo\n",
    "    return cid_string\n",
    "\n",
    "def populate_cid_list(codes):\n",
    "    \"\"\"\n",
    "    Preenche a lista CID_LIST com os c√≥digos CID formatados.\n",
    "    \"\"\"\n",
    "    cid_list = []\n",
    "    for code in codes:\n",
    "        cid_list.append(create_cid_string(code))\n",
    "    return cid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bf6b086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sigtap_string(sig, prefixo = None):  #\"sigtap-\"\n",
    "    \"\"\"\n",
    "    Cria uma string CID formatada para ser usada no payload do POST.\n",
    "    \"\"\"\n",
    "    if prefixo:\n",
    "        sig_string = prefixo +sig \n",
    "    else:\n",
    "        return sig\n",
    "    return sig_string\n",
    "\n",
    "def populate_sigtap_list(codes):\n",
    "    \"\"\"\n",
    "    Preenche a lista CID_LIST com os c√≥digos CID formatados.\n",
    "    \"\"\"\n",
    "    cid_list = []\n",
    "    for code in codes:\n",
    "        cid_list.append(create_sigtap_string(code))\n",
    "    return cid_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "48d8c475",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Helper Functions\n",
    "\n",
    "def select_option_by_value(driver, select_xpath, value, timeout=50):\n",
    "    try:\n",
    "        select_element = WebDriverWait(driver, timeout).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, select_xpath))\n",
    "        )\n",
    "        select = Select(select_element)\n",
    "        select.select_by_value(value)\n",
    "        #print(f\"Selecionado valor '{value}' em '{select_xpath}'\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao selecionar valor '{value}': {e}\")\n",
    "        return False\n",
    "    \n",
    "def criar_lista_competencias_por_ano(ano, range_meses= [1,12]):\n",
    "    competencias = []\n",
    "    for mes in range(range_meses[0], range_meses[1]+1):\n",
    "        competencia = f\"{ano}{mes:02}\"\n",
    "        competencias.append(competencia)\n",
    "    return competencias\n",
    "\n",
    "def create_script_string(script, arg=None, arg_as_string = True):\n",
    "    \"\"\"\n",
    "    Cria uma string de script JavaScript para execu√ß√£o.\n",
    "    \"\"\"\n",
    "    script_str = None\n",
    "    if arg:\n",
    "        if arg_as_string:\n",
    "            script_str = f\"{script}('{arg}')\"\n",
    "        else:\n",
    "            script_str = f\"{script}({arg})\"\n",
    "    else:\n",
    "        script_str = f\"{script}()\"\n",
    "    #print(\"script_str = \", script_str)\n",
    "    return script_str\n",
    "\n",
    "def exec_script(driver, script, arg=None, arg_as_string = True):\n",
    "    \"\"\"\n",
    "    Executa um script JavaScript no contexto da p√°gina atual.\n",
    "    \"\"\"\n",
    "    script_string = create_script_string(script, arg, arg_as_string)\n",
    "        \n",
    "    return driver.execute_script(script_string)\n",
    "\n",
    "def ExecuteCIDfilters(driver, cid_list):\n",
    "    for cid in cid_list:\n",
    "        exec_script(driver, 'addCid', cid)\n",
    "        #time.sleep(0.01)\n",
    "        #print(f\"Adicionando CID: {cid}\")\n",
    "    return True\n",
    "\n",
    "def ExecuteScriptfilters(driver, code_list, script):\n",
    "    for code in code_list:\n",
    "        exec_script(driver, script, code)\n",
    "    return True\n",
    "\n",
    "\n",
    "def ativar_checkbox_por_valor(driver, container_xpath, valor):\n",
    "    \"\"\"\n",
    "    Ativa o checkbox com o valor especificado dentro de um cont√™iner identificado por XPath.\n",
    "\n",
    "    Par√¢metros:\n",
    "    - driver: inst√¢ncia do WebDriver\n",
    "    - container_xpath: XPath do cont√™iner que cont√©m os checkboxes\n",
    "    - valor: valor do atributo 'value' do checkbox a ser ativado\n",
    "    \"\"\"\n",
    "    try:\n",
    "        container = driver.find_element(By.XPATH, container_xpath)\n",
    "        checkboxes = container.find_elements(By.XPATH, \".//input[@type='checkbox']\")\n",
    "\n",
    "        for checkbox in checkboxes:\n",
    "            if checkbox.get_attribute(\"value\") == valor:\n",
    "                if not checkbox.is_selected():\n",
    "                    checkbox.click()\n",
    "                    #print(f\"Checkbox '{valor}' ativado.\")\n",
    "                #else:\n",
    "                    #print(f\"Checkbox '{valor}' j√° estava ativado.\")\n",
    "                return True\n",
    "\n",
    "        print(f\"Checkbox com valor '{valor}' n√£o encontrado no container {container_xpath}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ativar checkbox '{valor}': {e}\")\n",
    "        return False\n",
    "      \n",
    "def AtivarListaDeCheckBoxes(driver, lista_checkboxes, dropdown_xpath, container_xpath):\n",
    "    drop_button = WebDriverWait(driver, 50).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, dropdown_xpath))\n",
    "    )\n",
    "    actions = ActionChains(driver)\n",
    "    actions.move_to_element(drop_button).click().perform()\n",
    "    WebDriverWait(driver, 50).until(\n",
    "        EC.visibility_of_element_located((By.XPATH, container_xpath))\n",
    "    )\n",
    "    for checkbox in lista_checkboxes:\n",
    "        ativar_checkbox_por_valor(driver, container_xpath, checkbox)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f1b8a44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ibge_included = [\n",
    "            '431240',\n",
    "            '430640',\n",
    "            '430760',\n",
    "            '431990',\n",
    "            '430820',\n",
    "            '430180',\n",
    "            '431130',\n",
    "            '430470',\n",
    "            '431410',\n",
    "            '431477',\n",
    "            '430300',\n",
    "            '431406',\n",
    "            '430610',\n",
    "            '431000',\n",
    "            '431530',\n",
    "            '431640',\n",
    "            '430693',\n",
    "            '431750',\n",
    "            '432252',\n",
    "            '430530',\n",
    "            '430807']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d83003",
   "metadata": {},
   "source": [
    "# Args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f4de2d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_download_dir = \"D:\\\\CodeStuff\\\\Stats\\\\colab_linear_regression\\\\linear_regression\\\\sisab_scrap_download\"\n",
    "url_prenatal = \"https://sisab.saude.gov.br/paginas/acessoPublico/relatorio/indicadores/IndicadorPrenatal.xhtml\"\n",
    "url_rel_prod = \"https://sisab.saude.gov.br/paginas/acessoRestrito/relatorio/federal/saude/RelSauProducao.xhtml\"\n",
    "\n",
    "PREFFIX = \"scrapped_relatorio_\"\n",
    "MAX_TRIES_DOWNLOAD = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "15fdf481",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn_dir = base_download_dir + '\\\\prenatal'\n",
    "base_pn_args = {\n",
    "        #'ano_range': [2024, 2024],\n",
    "        'competencias' : criar_lista_competencias_por_ano(2024),\n",
    "        'linha': \"MUN.CO_MUNICIPIO_IBGE\",\n",
    "\n",
    "        'ug': 'estado',#\"brasil\",#\"estado\",\n",
    "        'uf': \"43\",\n",
    "\n",
    "        #'municipios': [\"431490\"],\n",
    "\n",
    "        'estados': [\"RS\"],\n",
    "        'headless_browser': False,\n",
    "\n",
    "        'download_dir': pn_dir,\n",
    "        ### join args\n",
    "        'exec_join': True,\n",
    "        'joinby_col': 'Ibge',\n",
    "\n",
    "        'join_municipio': True,         #Deasus\n",
    "        \n",
    "        \"perform_join\": True,          # Join multiple DataFrames into one\n",
    "        \"extract_code\": False,          # Extract codes (e.g., CID codes) from a specified column\n",
    "        \"group_and_sum\": False,         # Group the data by CID groups and sum the values\n",
    "        \"convert_labels\": False,        # Convert column labels (e.g., month names) into datetime objects\n",
    "        \"perform_grand_total\": False,   # Add a row with the sum of all numeric columns\n",
    "        \"save_to_excel\": True          # Save the final processed DataFrame to an Excel file\n",
    "}\n",
    "\n",
    "\n",
    "PN_ARGS = {\n",
    "    'procedimentos': {\n",
    "        'url': url_rel_prod,\n",
    "        'coluna': \"SIGTAP\",\n",
    "        #'tpProducao' : \"7\",\n",
    "        'sigtap_list' : populate_sigtap_list(['0214010040', '0214010082', '0214010090', '0301010234', '0214010066']),\n",
    "\n",
    "    },\n",
    "    'puericultura' : {\n",
    "        'url': url_rel_prod,\n",
    "        'coluna': 'PCA',\n",
    "         #       'tpProducao' : \"7\",\n",
    "        'probl_condicao': ['ABP004'], # --- Puericultura\n",
    "        'competencias' : criar_lista_competencias_por_ano(2024, [1, 8]),\n",
    "        'tipo_atd' : ['1', '2', '4', '5'],\n",
    "        'fx_etaria': ['0', '7'],\n",
    "        'tipo_fx_etaria': '1',\n",
    "\n",
    "    },\n",
    "    'puerperio' : {\n",
    "         'url': url_rel_prod,\n",
    "        'coluna': 'PCA',\n",
    "        #        'tpProducao' : \"7\",\n",
    "        'competencias' : criar_lista_competencias_por_ano(2024, [1, 8]),\n",
    "        'probl_condicao': ['ABP002'], # ---- Puerperio\n",
    "        'tipo_atd' : ['1', '2', '4', '5'],\n",
    "\n",
    "    },\n",
    "    'vd_RN': {\n",
    "         'url': url_rel_prod,\n",
    "        'coluna': 'MV',\n",
    "\n",
    "        'competencias' : criar_lista_competencias_por_ano(2024, [1, 8]),\n",
    "        'motivo_visita': ['7'],\n",
    "\n",
    "        'fx_etaria': ['0', '7'],\n",
    "        'tipo_fx_etaria': '1',\n",
    "    },\n",
    "    'vd_puerpera': {\n",
    "         'url': url_rel_prod,\n",
    "        'coluna': 'MV',\n",
    "\n",
    "        'competencias' : criar_lista_competencias_por_ano(2024, [1, 8]),\n",
    "        'motivo_visita': ['6'],\n",
    "\n",
    "    },\n",
    "\n",
    "    'gestantes_prim_atd' : {\n",
    "         'url': url_prenatal,\n",
    "         'coluna' : 'ind01',\n",
    "         \n",
    "    },\n",
    "    'gestantes_prim_atd_12' : {\n",
    "        'url': url_prenatal,\n",
    "        'coluna' : 'ind02',\n",
    "    },\n",
    "    'gestantes_exame_avaliados20sem' : {\n",
    "        'url': url_prenatal,\n",
    "        'coluna' : 'ind03',\n",
    "    }\n",
    "\n",
    "\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7b07e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regioes \"CENTRO-OESTE\", \"NORTE\", \"NORDESTE\", \"SUDESTE\", \"SUL\"\n",
    "\n",
    "\n",
    "cid_anos_range = [2016,2023]\n",
    "\n",
    "total_atd_name = \"total_atendimentos\"\n",
    "cidf_name = \"cid_f\"\n",
    "\n",
    "base_cidf_args = {\n",
    "        'url' : url_rel_prod,\n",
    "        'ano_range': cid_anos_range,\n",
    "        #\"ano\": 2024,\n",
    "        \"cid_list\": populate_cid_list(CID_CODES),\n",
    "        'linha': \"CID\",\n",
    "        'coluna': \"NU_COMPETENCIA\",\n",
    "        'ug': 'regiao',#\"brasil\",#\"estado\",\n",
    "        'uf': \"43\",\n",
    "        'tpProducao' : \"4\",\n",
    "        #'municipios': [\"431490\"],\n",
    "\n",
    "        #'estados': [\"RS\"],\n",
    "        'headless_browser': False,\n",
    "\n",
    "\n",
    "        ### join args\n",
    "        'join_municipio': False,         #Deasus\n",
    "        'exec_join' : True,\n",
    "        'joinby_col': 'CIAP/CID',\n",
    "        \"perform_join\": True,          # Join multiple DataFrames into one\n",
    "        \"extract_code\": True,          # Extract codes (e.g., CID codes) from a specified column\n",
    "        \"group_and_sum\": True,         # Group the data by CID groups and sum the values\n",
    "        \"convert_labels\": True,        # Convert column labels (e.g., month names) into datetime objects\n",
    "        \"perform_grand_total\": True,   # Add a row with the sum of all numeric columns\n",
    "        \"save_to_excel\": True          # Save the final processed DataFrame to an Excel file\n",
    "}\n",
    "\n",
    "\n",
    "CID_F_STRATIFIED_NACIONAL_ARGS = {\n",
    "    'nacional_estratificado':{\n",
    "        'download_dir': base_download_dir+\"\\\\nacional_estratificado\"+\"\\\\\"+cidf_name,\n",
    "        'ug': 'brasil'\n",
    "    }\n",
    "}\n",
    "\n",
    "CID_F_ARGS = {\n",
    "    'sul': {\n",
    "        'download_dir': base_download_dir+\"\\\\sul\"+\"\\\\\"+cidf_name,\n",
    "        'regioes': [\"SUL\"],\n",
    "    },\n",
    "        'norte': {\n",
    "        'download_dir': base_download_dir+\"\\\\norte\"+\"\\\\\"+cidf_name,\n",
    "        'regioes': [\"NORTE\"],\n",
    "    },\n",
    "        'centro_oeste': {\n",
    "        'download_dir': base_download_dir+\"\\\\centro_oeste\"+\"\\\\\"+cidf_name,\n",
    "        'regioes': [\"CENTRO-OESTE\"],\n",
    "    },\n",
    "        'sudeste': {\n",
    "        'download_dir': base_download_dir+\"\\\\sudeste\"+\"\\\\\"+cidf_name,\n",
    "        'regioes': [\"SUDESTE\"],\n",
    "    },\n",
    "        'nordeste': {\n",
    "        'download_dir': base_download_dir+\"\\\\nordeste\"+\"\\\\\"+cidf_name,\n",
    "        'regioes': [\"NORDESTE\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "base_total_args =  {\n",
    "        'url' : url_rel_prod,\n",
    "        'ano_range': cid_anos_range,\n",
    "        #\"ano\": 2024,\n",
    "        \"cid_list\": [],#populate_cid_list(CID_CODES),\n",
    "        'linha': \"BRASIL\",\n",
    "        'coluna': \"NU_COMPETENCIA\",\n",
    "        'ug': \"regiao\",\n",
    "        'uf': \"43\",\n",
    "        'tpProducao' : \"4\",\n",
    "        #'municipios': [\"431490\"],\n",
    "        #'estados': [\"RS\"],\n",
    "        'headless_browser': False,\n",
    "        \n",
    "        ### join args\n",
    "        'join_municipio': False,         #Deasus\n",
    "        'joinby_col': \"Brasil\",\n",
    "        \"perform_join\": True,         \n",
    "        \"extract_code\": False,        \n",
    "        \"group_and_sum\": False,       \n",
    "        \"convert_labels\": True,       \n",
    "        \"perform_grand_total\": False, \n",
    "        \"save_to_excel\": True         \n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "TOTAL_ATD_ARGS = {\n",
    "    'sul_total': {\n",
    "        'download_dir': base_download_dir+\"\\\\sul\"+\"\\\\\" +total_atd_name,\n",
    "        'regioes': [\"SUL\"],\n",
    "    },\n",
    "        'norte_total': {\n",
    "        'download_dir': base_download_dir+\"\\\\norte\"+\"\\\\\" +total_atd_name,\n",
    "        'regioes': [\"NORTE\"],\n",
    "    },\n",
    "        'centro_oeste_total': {\n",
    "        'download_dir': base_download_dir+\"\\\\centro_oeste\"+\"\\\\\" +total_atd_name,\n",
    "        'regioes': [\"CENTRO-OESTE\"],\n",
    "    },\n",
    "        'sudeste_total': {\n",
    "        'download_dir': base_download_dir+\"\\\\sudeste\"+ \"\\\\\" +total_atd_name,\n",
    "        'regioes': [\"SUDESTE\"],\n",
    "    },\n",
    "        'nordeste_total': {\n",
    "        'download_dir': base_download_dir+\"\\\\nordeste\"+ \"\\\\\" +total_atd_name,\n",
    "        'regioes': [\"NORDESTE\"],\n",
    "    },\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b73ffa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cid_args_age_groups =  {\n",
    "        'url' : url_rel_prod,\n",
    "        'ano_range': cid_anos_range,\n",
    "        #\"ano\": 2024,\n",
    "        \"cid_list\": populate_cid_list(CID_CODES),\n",
    "        'linha': \"BRASIL\",\n",
    "        'coluna': \"NU_COMPETENCIA\",\n",
    "        #'ug': \"regiao\",\n",
    "        #'uf': \"43\",\n",
    "        'tpProducao' : \"4\",\n",
    "        #'municipios': [\"431490\"],\n",
    "        #'estados': [\"RS\"],\n",
    "        'headless_browser': False,\n",
    "        \n",
    "        ### join args\n",
    "        'join_municipio': False,         #Deasus\n",
    "        'joinby_col': \"Brasil\",\n",
    "        \"perform_join\": True,         \n",
    "        \"extract_code\": False,        \n",
    "        \"group_and_sum\": False,       \n",
    "        \"convert_labels\": True,       \n",
    "        \"perform_grand_total\": False, \n",
    "        \"save_to_excel\": True         \n",
    "\n",
    "    }\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b489a7a9",
   "metadata": {},
   "source": [
    "# Populate Var Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8518f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIABLES_TABLE ={}\n",
    "\n",
    "def add_to_variable_table(args, base_args, var_table):\n",
    "    for key in args.keys():\n",
    "        var_table[key] = copy.deepcopy(base_args)\n",
    "        for key2, value in args[key].items():\n",
    "            var_table[key][key2] = value\n",
    "    return var_table\n",
    "\n",
    "VARIABLES_TABLE = add_to_variable_table(CID_F_ARGS,base_cidf_args,var_table=VARIABLES_TABLE)\n",
    "\n",
    "#for key in CID_F_ARGS.keys():\n",
    "#    VARIABLES_TABLE[key] = copy.deepcopy(base_cidf_args)\n",
    "#    for key2, value in CID_F_ARGS[key].items():\n",
    "#        VARIABLES_TABLE[key][key2] = value\n",
    "#\n",
    "#for key in TOTAL_ATD_ARGS.keys():\n",
    "#    VARIABLES_TABLE[key] = copy.deepcopy(base_total_args)\n",
    "#    for key2, value in TOTAL_ATD_ARGS[key].items():\n",
    "#        VARIABLES_TABLE[key][key2] = value\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45599e9",
   "metadata": {},
   "source": [
    "# XPaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3095cb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Xpaths(url):\n",
    "    if url == url_rel_prod:\n",
    "        return {\n",
    "    \"DROPDOWN_BUTTON_PATH\": \"/html/body/div[2]/div/section[2]/span/form/div[3]/div[1]/div/div[2]/div[2]/div[5]/div/div/div[2]/button\",\n",
    "    #\"DROPDOWN_MENU\" : \"/html/body/div[2]/div/section[2]/span/form/div[3]/div[1]/div/div[2]/div[2]/div[5]/div/div/div[2]/ul\",\n",
    "    \"CSV_BUTTON_PATH\": \"/html/body/div[2]/div/section[2]/span/form/div[3]/div[1]/div/div[2]/div[2]/div[5]/div/div/div[2]/ul/li[2]/a\",\n",
    "\n",
    "    \"CID_CIAP_ADD\": '//*[@id=\"btnAddCid\"]',\n",
    "    \"CID_CIAP_SELECTION\": \"/html/body/div[2]/div/section[2]/span/form/div[1]/div/div/div[2]/div/div/div/div[2]/div/table/tbody\",\n",
    "    \"CONCLUIR_CID_CIAP_SELECTION\": '/html/body/div[2]/div/section[2]/span/form/div[1]/div/div/div[2]/button[2]',\n",
    "\n",
    "    \"SIGTAP_ADD\": '//*[@id=\"btnAddSigtap\"]',\n",
    "    \"SIGTAP_SELECTION\": '/html/body/div[2]/div/section[2]/span/form/div[2]/div/div/div[2]/div/div/div/div[2]/div/div/div[2]',\n",
    "    \"CONCLUIR_SIGTAP_SELECTION\": '/html/body/div[2]/div/section[2]/span/form/div[2]/div/div/div[2]/button[2]',\n",
    "\n",
    "    \"COMPETENCIA_DROPDOWN_BUTTON_PATH\": '/html/body/div[2]/div/section[2]/span/form/div[3]/div[1]/div/div[2]/div[2]/div[2]/div/div[2]/span/div/button',\n",
    "    \"COMPETENCIA_CHECKBOX_WRAPPER\": \"/html/body/div[2]/div/section[2]/span/form/div[3]/div[1]/div/div[2]/div[2]/div[2]/div/div[2]/span/div/ul\",\n",
    "\n",
    "    \"TIPO_PRODUCAO\": '//*[@id=\"tpProducao\"]',\n",
    "    \"LINHA_SELECT\": '//*[@id=\"selectLinha\"]',\n",
    "    \"COLUNA_SELECT\": '//*[@id=\"selectcoluna\"]',\n",
    "    \"UG_SELECT\": '//*[@id=\"unidGeo\"]',\n",
    "    \"UF_SELECT\": '//*[@id=\"estadoMunicipio\"]',\n",
    "\n",
    "    \"UG_DROPDOWN_BUTTON_PATH\": '/html/body/div[2]/div/section[2]/span/form/div[3]/div[1]/div/div[2]/div[2]/div[1]/div/div[2]/span/div/button',\n",
    "    \"UG_CHECKBOX_WRAPPER\": '/html/body/div[2]/div/section[2]/span/form/div[3]/div[1]/div/div[2]/div[2]/div[1]/div/div[2]/span/div/ul',\n",
    "\n",
    "    \"PROBLEMA_CA_DROPDOWN_BUTTON\": '/html/body/div[2]/div/section[2]/span/form/div[3]/div[1]/div/div[2]/div[2]/div[4]/div/div[2]/div[4]/div/div[2]/div/span[1]/div/div/div[3]/div/button',\n",
    "    \"PROBLEMA_CA_CHECKBOX_WRAPPER\": '/html/body/div[2]/div/section[2]/span/form/div[3]/div[1]/div/div[2]/div[2]/div[4]/div/div[2]/div[4]/div/div[2]/div/span[1]/div/div/div[3]/div/ul',\n",
    "\n",
    "    \"IDADE_INICIO\": '//*[@id=\"idadeInicio\"]',\n",
    "    \"IDADE_FIM\": '//*[@id=\"idadeFim\"]',\n",
    "\n",
    "    \"TIPO_ATENDIMENTO_DROPDOWN\": '/html/body/div[2]/div/section[2]/span/form/div[3]/div[1]/div/div[2]/div[2]/div[4]/div/div[2]/div[3]/div/span/span/div/button',\n",
    "    \"TIPO_ATENDIMENTO_WRAPPER\": '/html/body/div[2]/div/section[2]/span/form/div[3]/div[1]/div/div[2]/div[2]/div[4]/div/div[2]/div[3]/div/span/span/div/ul',\n",
    "\n",
    "    \"MOTIVO_VISITA_DROPDOWN\": '/html/body/div[2]/div/section[2]/span/form/div[3]/div[1]/div/div[2]/div[2]/div[4]/div/div[2]/div[4]/div/div[2]/div/span[4]/div/div/div[1]/div/button',\n",
    "    \"MOTIVO_VISITA_WRAPPER\": '/html/body/div[2]/div/section[2]/span/form/div[3]/div[1]/div/div[2]/div[2]/div[4]/div/div[2]/div[4]/div/div[2]/div/span[4]/div/div/div[1]/div/ul'\n",
    "    }\n",
    "    elif url == url_prenatal:\n",
    "        return { \n",
    "            \"DROPDOWN_BUTTON_PATH\": '/html/body/div[2]/div/section[2]/span/form/div/div[1]/div/div[2]/div[2]/div[4]/div/div/div/div/button',\n",
    "            \"CSV_BUTTON_PATH\": '/html/body/div[2]/div/section[2]/span/form/div/div[1]/div/div[2]/div[2]/div[4]/div/div/div/div/ul/li[2]/a',\n",
    "\n",
    "            \"LINHA_SELECT\": '//*[@id=\"selectLinha\"]',\n",
    "            \"COLUNA_SELECT\": '//*[@id=\"selectcoluna\"]',\n",
    "\n",
    "            \"UG_SELECT\": '//*[@id=\"unidGeo\"]',\n",
    "            \"UF_SELECT\": '//*[@id=\"estadoMunicipio\"]',\n",
    "\n",
    "            \"UG_DROPDOWN_BUTTON_PATH\": '/html/body/div[2]/div/section[2]/span/form/div/div[1]/div/div[2]/div[2]/div[2]/div/div[2]/span/div/button',\n",
    "            \"UG_CHECKBOX_WRAPPER\": '/html/body/div[2]/div/section[2]/span/form/div/div[1]/div/div[2]/div[2]/div[2]/div/div[2]/span/div/ul',\n",
    "\n",
    "            'COMPETENCIA_DROPDOWN_BUTTON_PATH' : '/html/body/div[2]/div/section[2]/span/form/div/div[1]/div/div[2]/div[2]/div[3]/div/div[2]/span/div/button',\n",
    "            \"COMPETENCIA_CHECKBOX_WRAPPER\": '/html/body/div[2]/div/section[2]/span/form/div/div[1]/div/div[2]/div[2]/div[3]/div/div[2]/span/div/ul',\n",
    "\n",
    "            \n",
    "        }\n",
    "    #desempenho\n",
    "    #//*[@id=\"coIndicador\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb42c61",
   "metadata": {},
   "source": [
    "# Scrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "69c0f807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df = df.groupby(['Ibge', 'Municipio'], as_index=False)['Exames at√© 20sem'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5ec4c307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(args, url, download_dir, var='no_var_defined'):\n",
    "    XPaths = Get_Xpaths(url)\n",
    "\n",
    "    ### RelPreNatal\n",
    "\n",
    "\n",
    "\n",
    "    # Cria o diret√≥rio de download se ele n√£o existir\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "    \n",
    "\n",
    "    preffix = PREFFIX\n",
    "    ano = args['ano'] if 'ano' in  args else '_'\n",
    "    file_name = f\"{preffix}_{ano}_{var}.csv\"  # Nome do arquivo desejado\n",
    "    #temp_file_path = os.path.join(download_dir, file_name + \".crdownload\")\n",
    "    final_file_path = os.path.join(download_dir, file_name)\n",
    "\n",
    "    if os.path.exists(final_file_path):\n",
    "        print(f\"------------ SKIPPING: The file '{final_file_path}' already exists.\")\n",
    "        return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Configura√ß√µes do Chrome headless (sem abrir janela)\n",
    "    options = Options()\n",
    "    if args['headless_browser']:\n",
    "        options.add_argument(\"--headless=new\")\n",
    "    \n",
    "    # Configura o caminho de download\n",
    "    prefs = {\n",
    "    \"download.default_directory\": download_dir,\n",
    "    \"download.prompt_for_download\": False,  # N√£o perguntar onde salvar\n",
    "    \"download.directory_upgrade\": True,    # Atualizar o diret√≥rio automaticamente\n",
    "    \"safebrowsing.enabled\": True           # Desativar verifica√ß√µes de seguran√ßa para downloads\n",
    "    }\n",
    "    options.add_experimental_option(\"prefs\", prefs)\n",
    "    \n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    print(driver)\n",
    "    try:\n",
    "        # Acessa a p√°gina\n",
    "        \n",
    "        driver.get(url)\n",
    "\n",
    "        # Espera o JS carregar\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        wait.until(EC.presence_of_element_located((By.NAME, \"javax.faces.ViewState\")))\n",
    "\n",
    "        ##################################################################################\n",
    "\n",
    "        select_option_by_value(driver, XPaths['LINHA_SELECT'], args['linha'])\n",
    "        time.sleep(1)\n",
    "        select_option_by_value(driver, XPaths['UG_SELECT'], args['ug'])\n",
    "        time.sleep(1)\n",
    "        if 'tpProducao' in args:\n",
    "            select_option_by_value(driver, XPaths['TIPO_PRODUCAO'], args['tpProducao'])\n",
    "            time.sleep(1)\n",
    "        select_option_by_value(driver, XPaths['COLUNA_SELECT'], args['coluna'])\n",
    "        time.sleep(1)\n",
    "\n",
    "        if 'competencias' in args:\n",
    "            AtivarListaDeCheckBoxes(driver, args['competencias'], XPaths['COMPETENCIA_DROPDOWN_BUTTON_PATH'], XPaths['COMPETENCIA_CHECKBOX_WRAPPER'])\n",
    "\n",
    "        #AtivarListaDeCheckBoxes(driver, criar_lista_competencias_por_ano(args['ano']), COMPETENCIA_DROPDOWN_BUTTON_PATH, COMPETENCIA_CHECKBOX_WRAPPER)\n",
    "        time.sleep(1)\n",
    "\n",
    "        ug_dropdown_select_arg = args['municipios'] if args['ug'] == \"municipio\" else args['estados'] if args['ug'] == 'estado' else args['regioes'] if args['ug'] == \"regiao\" else None\n",
    "\n",
    "        if ug_dropdown_select_arg:\n",
    "            if args['ug'] == \"municipio\":\n",
    "                select_option_by_value(driver, XPaths['UF_SELECT'], args['uf'])\n",
    "                time.sleep(1)\n",
    "            AtivarListaDeCheckBoxes(driver,ug_dropdown_select_arg, XPaths['UG_DROPDOWN_BUTTON_PATH'], XPaths['UG_CHECKBOX_WRAPPER'])\n",
    "            time.sleep(1)\n",
    "\n",
    "        if 'tipo_atd' in args:\n",
    "            AtivarListaDeCheckBoxes(driver, args['tipo_atd'], XPaths['TIPO_ATENDIMENTO_DROPDOWN'], XPaths['TIPO_ATENDIMENTO_WRAPPER'])\n",
    "\n",
    "        if 'probl_condicao' in args:\n",
    "            AtivarListaDeCheckBoxes(driver, args['probl_condicao'], XPaths['PROBLEMA_CA_DROPDOWN_BUTTON'], XPaths['PROBLEMA_CA_CHECKBOX_WRAPPER'])\n",
    "        \n",
    "        if \"motivo_visita\" in args:\n",
    "            AtivarListaDeCheckBoxes(driver, args['motivo_visita'], XPaths['MOTIVO_VISITA_DROPDOWN'], XPaths['MOTIVO_VISITA_WRAPPER'])\n",
    "\n",
    "        if 'fx_etaria' in args:\n",
    "            wait = WebDriverWait(driver, 10)\n",
    "\n",
    "            # Espera e preenche o campo de in√≠cio\n",
    "            fx_etaria_inicio = wait.until(EC.element_to_be_clickable((By.XPATH, XPaths['IDADE_INICIO'])))\n",
    "            fx_etaria_inicio.clear()\n",
    "            fx_etaria_inicio.send_keys(args['fx_etaria'][0])\n",
    "\n",
    "            # Espera e preenche o campo de fim\n",
    "            fx_etaria_fim = wait.until(EC.element_to_be_clickable((By.XPATH, XPaths['IDADE_FIM'])))\n",
    "            fx_etaria_fim.clear()\n",
    "            fx_etaria_fim.send_keys(args['fx_etaria'][1])\n",
    "\n",
    "            xpath_tipo_idade = f'//*[@id=\"tpIdade:{args[\"tipo_fx_etaria\"]}\"]'\n",
    "\n",
    "            # Aguarda o bot√£o de r√°dio estar clic√°vel\n",
    "            tipo_idade = wait.until(EC.element_to_be_clickable((By.XPATH, xpath_tipo_idade)))\n",
    "            tipo_idade.click()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        ### CID CIAP\n",
    "        if 'cid_list' in args:\n",
    "            cid_Ciap = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, XPaths['CID_CIAP_ADD']))\n",
    "            )\n",
    "            cid_Ciap.click()\n",
    "            wait.until(EC.presence_of_element_located((By.XPATH, XPaths['CID_CIAP_SELECTION'])))\n",
    "            ExecuteScriptfilters(driver, args['cid_list'], 'addCid')\n",
    "            cid_Ciap_conclude = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, XPaths['CONCLUIR_CID_CIAP_SELECTION']))\n",
    "            )\n",
    "            cid_Ciap_conclude.click()\n",
    "            time.sleep(1)\n",
    "\n",
    "        ### Sigtap\n",
    "        if 'sigtap_list' in args:\n",
    "            sig = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, XPaths['SIGTAP_ADD']))\n",
    "            )\n",
    "            actions = ActionChains(driver)\n",
    "            actions.move_to_element(sig).click().perform()\n",
    "            #sig.click()\n",
    "            wait.until(EC.presence_of_element_located((By.XPATH, XPaths['SIGTAP_SELECTION'])))\n",
    "            ExecuteScriptfilters(driver, args['sigtap_list'], 'addSigtap')\n",
    "            sig_conclude = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, XPaths['CONCLUIR_SIGTAP_SELECTION']))\n",
    "            )\n",
    "            sig_conclude.click()\n",
    "            time.sleep(1)\n",
    "\n",
    "        ###################################################################\n",
    "        #time.sleep(1000)\n",
    "        # Clica no bot√£o de download (exportar)\n",
    "        dropdown_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, XPaths['DROPDOWN_BUTTON_PATH']))\n",
    "        )\n",
    "        #print(f\"dropdown download button: {dropdown_button}\")\n",
    "        #dropdown_button.click()\n",
    "        #print(\"bot√£o de download dropdown clicado.\")\n",
    "#\n",
    "#\n",
    "        #wait.until(EC.presence_of_element_located((By.XPATH, XPaths['DROPDOWN_MENU'])))\n",
    "        #time.sleep(1)\n",
    "#\n",
    "        #csv_opt_button = WebDriverWait(driver, 50).until(\n",
    "        #    EC.element_to_be_clickable((By.XPATH, XPaths['CSV_BUTTON_PATH']))\n",
    "        #    )\n",
    "        #print(f\"csv download button: {csv_opt_button}\")\n",
    "        #time.sleep(1)\n",
    "        #csv_opt_button.click()\n",
    "        #print(\"bot√£o de download CSV clicado.\")\n",
    "\n",
    "         #Usa ActionChains para garantir o clique\n",
    "        actions = ActionChains(driver)\n",
    "        actions.move_to_element(dropdown_button).click().perform()\n",
    "        print(\"Dropdown do bot√£o de download clicado.\")\n",
    "\n",
    "         #Espera o bot√£o de download CSV aparecer e clica nele\n",
    "        try:\n",
    "            csv_opt_button = WebDriverWait(driver, 50).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, XPaths['CSV_BUTTON_PATH']))\n",
    "            )\n",
    "            #csv_opt_button.click()\n",
    "            actions.move_to_element(csv_opt_button).click().perform()\n",
    "            print(\"Bot√£o de download clicado via ActionChains.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao localizar ou clicar no bot√£o de download: {e}\")\n",
    "            driver.quit()\n",
    "            return False\n",
    "\n",
    "        ###################################################################\n",
    "\n",
    "\n",
    "        time.sleep(10)\n",
    "        start_time = time.time()\n",
    "        timeout = 360\n",
    "\n",
    "        def check_file_exists(download_dir, preffix):\n",
    "            \"\"\"\n",
    "            Verifica se existe um arquivo tempor√°rio (.tmp) ou o arquivo CSV original (sem prefixo) na pasta.\n",
    "            \"\"\"\n",
    "            for file in os.listdir(download_dir):\n",
    "                if file.endswith(\".tmp\") or file.endswith('.crdownload') or (file.endswith(\".csv\") and not file.startswith(preffix)):\n",
    "                    return file  # Retorna o nome do arquivo encontrado\n",
    "            return None\n",
    "\n",
    "        while True:\n",
    "            # Check if the timeout has been exceeded\n",
    "            elapsed_time = time.time() - start_time\n",
    "            if elapsed_time > timeout:\n",
    "                print(\"Timeout reached. Download not completed.\")\n",
    "                driver.quit()\n",
    "                return False\n",
    "\n",
    "            # Check if a file exists\n",
    "            file_found = check_file_exists(download_dir, preffix)\n",
    "\n",
    "            if file_found:\n",
    "                # If it's a temporary file, wait for it to finish\n",
    "                #if file_found.endswith(\".tmp\") or file_found.endswith('.crdownload'):\n",
    "                #    print(\"Temporary file found. Waiting for download to complete...\")\n",
    "                #    time.sleep(10)\n",
    "                #    continue\n",
    "                # If it's the final CSV file, rename and exit\n",
    "                #elif file_found.endswith(\".csv\") and not file_found.startswith(preffix):\n",
    "                    final_file_path = os.path.join(download_dir, file_name)\n",
    "                    source_file_path = os.path.join(download_dir, file_found)\n",
    "\n",
    "                    # Check if the destination file already exists\n",
    "                    if os.path.exists(final_file_path):\n",
    "                        print(f\"File {final_file_path} already exists. Overwriting...\")\n",
    "                        os.remove(final_file_path)  # Remove the existing file\n",
    "\n",
    "                    os.rename(source_file_path, final_file_path)\n",
    "                    print(f\"File saved as: {final_file_path}\")\n",
    "                    driver.quit()\n",
    "                    return True\n",
    "            else:\n",
    "                # If no file is found, wait and continue checking\n",
    "                print(\"No file found. Waiting...\")\n",
    "                time.sleep(10)\n",
    "    finally:\n",
    "        # Fecha o driver\n",
    "        driver.quit()\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed36387",
   "metadata": {},
   "source": [
    "# Joiner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "07ff5369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_files(download_dir, prefix, column_name):\n",
    "    \"\"\"\n",
    "    Processes CSV files in a directory, extracts metadata, and returns a summary of unique values.\n",
    "\n",
    "    Args:\n",
    "        download_dir (str): Directory containing the CSV files.\n",
    "        prefix (str): Prefix to filter the files.\n",
    "        column_name (str): Column name to extract unique values.\n",
    "        cid_grup_ref_path (str, optional): Path to the CID group reference file. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing unique values from the specified column.\n",
    "    \"\"\"\n",
    "    df_list = []\n",
    "    #maindf_info = {'variaveis': []}\n",
    "\n",
    "    def get_start_and_end_lines_skip_metadata(file_path):\n",
    "        with open(file_path, 'r', encoding='latin1') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        start_line = None\n",
    "        end_line = None\n",
    "        n_brnco_lines = 0\n",
    "        for i, line in enumerate(lines):\n",
    "            if line == '\\n':\n",
    "                n_brnco_lines += 1\n",
    "            if n_brnco_lines == 2:\n",
    "                start_line = i + 1\n",
    "                break\n",
    "\n",
    "        n_brnco_lines = 0\n",
    "        for i, line in enumerate(lines[start_line:], start=start_line):\n",
    "            if line == '\\n':\n",
    "                n_brnco_lines += 1\n",
    "            if n_brnco_lines == 2:\n",
    "                end_line = i - 2\n",
    "                break\n",
    "        return start_line, end_line\n",
    "\n",
    "    for file in os.listdir(download_dir):\n",
    "        if file.endswith(\".csv\") and file.startswith(prefix):\n",
    "            file_path = os.path.join(download_dir, file)\n",
    "            \n",
    "            start_line, end_line = get_start_and_end_lines_skip_metadata(file_path)\n",
    "            print(f\"Processing file: {file}, Start line: {start_line}, End line: {end_line}\")\n",
    "            \n",
    "            # Read the file as a DataFrame\n",
    "            df = pd.read_csv(\n",
    "                file_path, \n",
    "                sep=';', \n",
    "                encoding='latin1', \n",
    "                skiprows=start_line, \n",
    "                nrows=end_line - start_line, \n",
    "                dtype=str  # Read all columns as strings\n",
    "            )\n",
    "            \n",
    "            # Clean and convert columns\n",
    "            for col in df.columns:\n",
    "                df[col] = df[col].str.replace('.', '', regex=False).str.replace(',', '.', regex=False)\n",
    "                try:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='ignore')  # Convert to numeric if possible\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            \n",
    "            df_list.append(df)\n",
    "\n",
    "    # Extract unique values from the specified column\n",
    "    #for i, df in enumerate(df_list):\n",
    "    #    if column_name in df.columns:  # Ensure the column exists\n",
    "    #        maindf_info['variaveis'].extend(df[column_name].unique())  # Add unique values from the column\n",
    "#\n",
    "    ## Remove duplicates from the final list\n",
    "    #maindf_info['variaveis'] = list(set(maindf_info['variaveis']))\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "06a21765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def process_csv_files(\n",
    "    download_dir,\n",
    "    prefix,\n",
    "    key_column='Ibge',\n",
    "    drop_columns=None,\n",
    "    fillna_value=0\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes CSV files in a directory, cleans them, and returns a list of DataFrames for merging.\n",
    "\n",
    "    Args:\n",
    "        download_dir (str): Directory containing the CSV files.\n",
    "        prefix (str): Prefix to filter the files.\n",
    "        key_column (str): Column name used to merge DataFrames (e.g., 'Ibge').\n",
    "        column_name (str, optional): If given, will extract and return unique values for this column.\n",
    "        drop_columns (list, optional): List of columns to drop after the first file (e.g., ['Uf', 'Municipio']).\n",
    "        fillna_value (any, optional): Value to fill in for missing data. Default is 0.\n",
    "\n",
    "    Returns:\n",
    "        list: List of cleaned DataFrames ready for merging.\n",
    "    \"\"\"\n",
    "    df_list = []\n",
    "\n",
    "    def get_start_and_end_lines_skip_metadata(file_path):\n",
    "        with open(file_path, 'r', encoding='latin1') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        start_line = None\n",
    "        end_line = None\n",
    "        n_brnco_lines = 0\n",
    "        for i, line in enumerate(lines):\n",
    "            if line.strip() == '':\n",
    "                n_brnco_lines += 1\n",
    "            if n_brnco_lines == 2:\n",
    "                start_line = i + 1\n",
    "                break\n",
    "\n",
    "        n_brnco_lines = 0\n",
    "        for i, line in enumerate(lines[start_line:], start=start_line):\n",
    "            if line.strip() == '':\n",
    "                n_brnco_lines += 1\n",
    "            if n_brnco_lines == 2:\n",
    "                end_line = i - 2\n",
    "                break\n",
    "        return start_line, end_line\n",
    "\n",
    "    for idx, file in enumerate(os.listdir(download_dir)):\n",
    "        if file.endswith(\".csv\") and file.startswith(prefix):\n",
    "            file_path = os.path.join(download_dir, file)\n",
    "            start_line, end_line = get_start_and_end_lines_skip_metadata(file_path)\n",
    "            print(f\"Processing file: {file}, Start line: {start_line}, End line: {end_line}\")\n",
    "\n",
    "            df = pd.read_csv(\n",
    "                file_path,\n",
    "                sep=';',\n",
    "                encoding='latin1',\n",
    "                skiprows=start_line,\n",
    "                nrows=end_line - start_line,\n",
    "                dtype=str\n",
    "            )\n",
    "\n",
    "            df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\n",
    "            df.columns = df.columns.str.strip()\n",
    "\n",
    "            for col in df.columns:\n",
    "                if col != key_column:\n",
    "                    df[col] = df[col].str.replace('.', '', regex=False).str.replace(',', '.', regex=False)\n",
    "                    df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "\n",
    "            # Clean and standardize key column\n",
    "            if key_column not in df.columns:\n",
    "                raise ValueError(f\"'{key_column}' column not found in file: {file}\")\n",
    "            df[key_column] = df[key_column].astype(str).str.strip()\n",
    "\n",
    "            # Drop specified columns from all but first file\n",
    "            if drop_columns and idx > 0:\n",
    "                df = df.drop(columns=drop_columns, errors='ignore')\n",
    "\n",
    "            # Fill NA with specified value\n",
    "            df = df.fillna(fillna_value)\n",
    "\n",
    "            df_list.append(df)\n",
    "\n",
    "    return df_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "66529956",
   "metadata": {},
   "outputs": [],
   "source": [
    "### joiner helper functions\n",
    "def join_dfs(df_list, joinby_col):\n",
    "    # Step 1: Extract unique CIAP/CID values\n",
    "    maindf_info = {\n",
    "        'variaveis': []\n",
    "    }\n",
    "\n",
    "    for i, df in enumerate(df_list):\n",
    "        if joinby_col in df.columns:  # Ensure the column exists\n",
    "            maindf_info['variaveis'].extend(df[joinby_col].unique())  # Add unique values from the column\n",
    "\n",
    "    # Remove duplicates from the final list\n",
    "    unique_ciap_cid = list(set(maindf_info['variaveis']))\n",
    "\n",
    "    # Step 2: Create a new DataFrame with CIAP/CID as the index\n",
    "    result_df = pd.DataFrame(index=unique_ciap_cid)\n",
    "\n",
    "    # Step 3: Iterate through all DataFrames and append their columns\n",
    "    for i, df in enumerate(df_list):\n",
    "        if joinby_col in df.columns:  # Ensure the column exists\n",
    "            df = df.set_index(joinby_col)  # Set CIAP/CID as the index\n",
    "            for col in df.columns:  # Iterate through the columns\n",
    "                if col not in result_df.columns:\n",
    "                    result_df[col] = 0  # Initialize the column with 0\n",
    "                result_df[col] = result_df[col].add(df[col], fill_value=0)  # Add values, filling missing with 0\n",
    "\n",
    "    # Step 4: Reset the index if you want CIAP/CID as a column instead of the index\n",
    "    result_df.reset_index(inplace=True)\n",
    "    result_df.rename(columns={'index': joinby_col}, inplace=True)\n",
    "\n",
    "    # Fill any remaining NaN values with 0\n",
    "    result_df.fillna(0, inplace=True)\n",
    "\n",
    "    for col in result_df.columns[1:]:  # Skip the COLUMN_NAME column\n",
    "        result_df[col] = result_df[col].astype(int)\n",
    "\n",
    "\n",
    "    # Print the resulting DataFrame\n",
    "    return result_df\n",
    "\n",
    "def extract_code_from_parentheses(df, joinby_col, new_column_name='Code'):\n",
    "    \"\"\"\n",
    "    Extracts the code between parentheses from the specified column and adds it as a new column.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame to process.\n",
    "    - column_name (str): The name of the column to extract the code from.\n",
    "    - new_column_name (str): The name of the new column to store the extracted codes.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The updated DataFrame with the new column.\n",
    "    \"\"\"\n",
    "    if not joinby_col:\n",
    "        raise ValueError(\"No column name provided for extraction.\")\n",
    "    if joinby_col in df.columns:\n",
    "        # Use regex to extract the content between parentheses\n",
    "        df[new_column_name] = df[joinby_col].str.extract(r'\\((.*?)\\)')\n",
    "    else:\n",
    "        print(f\"Column '{joinby_col}' not found in the DataFrame.\")\n",
    "    return df\n",
    "\n",
    "def group_and_sum_by_cid_group(df, cid_grup_ref_df, cid_column='CID_code', value_columns=None):\n",
    "    \"\"\"\n",
    "    Groups the DataFrame by CID groups and sums the values.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame containing the CID codes and values.\n",
    "    - cid_grup_ref_df (pd.DataFrame): The reference DataFrame mapping CID codes to groups.\n",
    "    - cid_column (str): The column in `df` containing the CID codes.\n",
    "    - value_columns (list): The columns in `df` to sum. If None, all numeric columns will be summed.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A new DataFrame grouped by CID groups with summed values.\n",
    "    \"\"\"\n",
    "    # Merge the main DataFrame with the reference DataFrame to map CID codes to groups\n",
    "    merged_df = df.merge(cid_grup_ref_df, left_on=cid_column, right_on='cid_ind_cod', how='left')\n",
    "    #return merged_df\n",
    "    # Rename the column to avoid overwriting\n",
    "    #merged_df.rename(columns={cid_column: 'Original_CID_code'}, inplace=True)\n",
    "\n",
    "    # Replace CID codes with their group names\n",
    "    #merged_df['CID_group'] = merged_df['cid_grup_nome']\n",
    "\n",
    "    # Drop unnecessary columns from the merge\n",
    "    #merged_df.drop(['cid_grup_cod', 'cid_grup_nome', 'cid_ind_cod', 'Original_CID_code'], axis=1, inplace=True)\n",
    "\n",
    "    # Group by the CID group names and sum the values\n",
    "\n",
    "    if value_columns is None:\n",
    "        value_columns = merged_df.select_dtypes(include='number').columns  # Default to numeric columns\n",
    "    grouped_df = merged_df.groupby('cid_grup_nome')[value_columns].sum().reset_index()\n",
    "    grouped_df.drop(columns=['CID_code'], inplace=True, errors='ignore')\n",
    "\n",
    "    return grouped_df, merged_df\n",
    "\n",
    "\n",
    "def convert_column_labels_to_datetime(df):\n",
    "    \"\"\"\n",
    "    Converts column labels with Portuguese month names into datetime objects.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame with column labels to convert.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The DataFrame with updated column labels.\n",
    "    \"\"\"\n",
    "    # Define a mapping for Portuguese month abbreviations to numbers\n",
    "    month_mapping = {\n",
    "        \"JAN\": \"01\", \"FEV\": \"02\", \"MAR\": \"03\", \"ABR\": \"04\", \"MAI\": \"05\", \"JUN\": \"06\",\n",
    "        \"JUL\": \"07\", \"AGO\": \"08\", \"SET\": \"09\", \"OUT\": \"10\", \"NOV\": \"11\", \"DEZ\": \"12\"\n",
    "    }\n",
    "\n",
    "    # Create a new dictionary for updated column labels\n",
    "    new_columns = {}\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            # Replace Portuguese month abbreviations with numbers\n",
    "            updated_col = col.upper()\n",
    "            for month, num in month_mapping.items():\n",
    "                updated_col = updated_col.replace(month, num)\n",
    "            # Convert to datetime if possible\n",
    "            new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n",
    "        except Exception:\n",
    "            # If conversion fails, keep the original column name\n",
    "            new_columns[col] = col\n",
    "    df.rename(columns=new_columns, inplace=True)\n",
    "    return df\n",
    "def add_grand_total_row(df, joinby_col):\n",
    "    \"\"\"\n",
    "    Adiciona uma linha ao DataFrame com a soma de todas as colunas num√©ricas.\n",
    "    A linha ser√° chamada 'Grand Total'.\n",
    "    \"\"\"\n",
    "    # Calcula a soma de todas as colunas num√©ricas\n",
    "    first_col = df.columns[0]  # Preserva a primeira coluna (joinby_col)\n",
    "    total_row = df.select_dtypes(include='number').sum()\n",
    "    \n",
    "    # Adiciona a linha ao DataFrame\n",
    "    total_row[first_col] = \"Grand Total\"  # Define o nome da linha\n",
    "    df = pd.concat([df, pd.DataFrame([total_row])], ignore_index=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "47d8ef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dfs(\n",
    "    df_list,\n",
    "    cid_grup_ref_df,\n",
    "    joinby_col,\n",
    "    output_file_path=None,\n",
    "    perform_join=True,\n",
    "    extract_code=False,\n",
    "    group_and_sum=False,\n",
    "    convert_labels=True,\n",
    "    perform_grand_total=False,\n",
    "    save_to_excel=True,\n",
    "):\n",
    "    results = {}\n",
    "\n",
    "    \n",
    "    \n",
    "    # Step 1: Join DataFrames\n",
    "    if perform_join:\n",
    "        joined_df = join_dfs(df_list, joinby_col=joinby_col)\n",
    "        results['joined_df'] = joined_df\n",
    "    else:\n",
    "        joined_df = df_list  # assume it's already joined or passed as a single df\n",
    "\n",
    "    # Step 2: Extract code from parentheses\n",
    "    if extract_code:\n",
    "        coded_df = extract_code_from_parentheses(joined_df, joinby_col=joinby_col, new_column_name='CID_code')\n",
    "        results['coded_df'] = coded_df\n",
    "    else:\n",
    "        coded_df = joined_df\n",
    "\n",
    "    # Step 3: Group and sum by CID group\n",
    "    if group_and_sum:\n",
    "        grouped_df, test_grouped_df = group_and_sum_by_cid_group(\n",
    "            coded_df,\n",
    "            cid_grup_ref_df,\n",
    "            cid_column='CID_code',\n",
    "            value_columns=None  # or coded_df.columns[1:] if needed\n",
    "        )\n",
    "        results['grouped_df'] = grouped_df\n",
    "        results['test_grouped_df'] = test_grouped_df\n",
    "    else:\n",
    "        grouped_df = coded_df\n",
    "\n",
    "    # Step 4: Convert column labels to datetime\n",
    "    if convert_labels:\n",
    "        dated_df = convert_column_labels_to_datetime(grouped_df)\n",
    "        results['dated_df'] = dated_df\n",
    "    else:\n",
    "        dated_df = grouped_df\n",
    "\n",
    "    # Step 5: Check for \"UNNAMED\" columns\n",
    "    unnamed_columns = [col for col in dated_df.columns if \"UNNAMED\" in str(col).upper()]\n",
    "    if unnamed_columns:\n",
    "        for col in unnamed_columns:\n",
    "            if dated_df[col].sum() == 0:  # Check if all values are zero\n",
    "                print(f\"=========WARNING=========: Column '{col}' is unnamed and contains only zeros. It will be removed.\")\n",
    "            else:\n",
    "                print(f\"=========WARNING=========: Column '{col}' is unnamed but contains non-zero values. It will still be removed.\")\n",
    "        dated_df.drop(columns=unnamed_columns, inplace=True)\n",
    "\n",
    "    # Step 6: Add a grand total row if required\n",
    "    if perform_grand_total:\n",
    "        dated_df = add_grand_total_row(dated_df, joinby_col=joinby_col)\n",
    "        results['dated_df_with_grand_total'] = dated_df\n",
    "    else:    \n",
    "        dated_df = dated_df\n",
    "\n",
    "    # Step 7: Save to Excel\n",
    "    if save_to_excel and output_file_path is not None:\n",
    "        output_dir = os.path.dirname(output_file_path)\n",
    "            \n",
    "        # Create the directory if it doesn't exist\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            \n",
    "        dated_df.to_excel(output_file_path, index=False, engine='openpyxl')\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1718ce75",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "deccddfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_download(variable, var_args, download_dir):\n",
    "        \n",
    "        args = copy.deepcopy(var_args)\n",
    "        max_retries = MAX_TRIES_DOWNLOAD  # Set the maximum number of retries\n",
    "        url = args['url']\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            data_dowloaded = download_data(args, url, download_dir, variable)\n",
    "            if data_dowloaded:\n",
    "                print(f\"Download successful for {variable}.\")\n",
    "                break  # Exit the retry loop if download is successful\n",
    "            else:\n",
    "                retries += 1\n",
    "                print(f\"Retrying download for {variable}... Attempt {retries}/{max_retries}\")\n",
    "                time.sleep(5)  # Wait before retrying\n",
    "        if not data_dowloaded:\n",
    "            print(f\"Failed to download data for {variable} after {max_retries} attempts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "904a9d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(var_table):\n",
    "    \"\"\"\n",
    "    Fun√ß√£o para baixar dados de uma s√©rie de anos.\n",
    "    \"\"\"\n",
    "    join_by_col = None\n",
    "\n",
    "    for variable, var_args in var_table.items():\n",
    "        download_dir = var_args['download_dir']\n",
    "        join_by_col = var_args['joinby_col']\n",
    "        join_municipio = var_args['join_municipio']\n",
    "        args = var_args\n",
    "        if 'ano_range' in var_args:\n",
    "            args = copy.deepcopy(var_args)\n",
    "            for ano in range(args['ano_range'][0], args['ano_range'][1] + 1):\n",
    "                args['ano'] = ano\n",
    "                args['competencias'] = criar_lista_competencias_por_ano(ano)\n",
    "                try_download(variable, args, download_dir)\n",
    "        \n",
    "        else:\n",
    "            try_download(variable, var_args, download_dir)\n",
    "\n",
    "\n",
    "        dfs = process_csv_files(download_dir, PREFFIX, join_by_col)\n",
    "\n",
    "        if join_municipio:\n",
    "            dfs_cleaned = [dfs[0]] + [\n",
    "                df.drop(columns=['Uf', 'Municipio'], errors='ignore') for df in dfs[1:]\n",
    "            ]\n",
    "            # Merge on 'Ibge', keeping all rows (outer join)\n",
    "            df_merged = reduce(lambda left, right: pd.merge(left, right, on=join_by_col, how='outer'), dfs_cleaned)\n",
    "            # Se quiser manter colunas como 'Uf' e 'Municipio' do primeiro DataFrame:\n",
    "            df_merged = df_merged[df_merged[join_by_col].isin(ibge_included)]\n",
    "            output_joined_dir = download_dir+\"\\\\joined\"\n",
    "            if not os.path.exists(output_joined_dir):\n",
    "                os.makedirs(output_joined_dir)\n",
    "            df_merged.to_excel(output_joined_dir + \"\\\\joined.xlsx\", index=False, engine='openpyxl')\n",
    "        else:\n",
    "            results = process_dfs(\n",
    "                df_list=dfs,\n",
    "                cid_grup_ref_df=pd.read_excel(\"D:\\\\CodeStuff\\\\Stats\\\\colab_linear_regression\\\\linear_regression\\\\sisab_scrap_download\\\\_input\\\\cid_f_grupos_referencia.xlsx\"),\n",
    "                joinby_col=args['joinby_col'],\n",
    "                output_file_path=args['download_dir']+f\"\\\\joined_files\\\\joined_{variable}.xlsx\",\n",
    "                perform_join=args['perform_join'],\n",
    "                extract_code=args['extract_code'],\n",
    "                group_and_sum=args['group_and_sum'],\n",
    "                convert_labels=args['convert_labels'],\n",
    "                perform_grand_total=args['perform_grand_total'],\n",
    "                save_to_excel=args['save_to_excel'],\n",
    "            )\n",
    "            print(\"Download e processamento conclu√≠dos.\")\n",
    "            return results\n",
    "\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e9d87",
   "metadata": {},
   "source": [
    "# Defs Incidence Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fbd2ac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def CreateIncidenceRatioDF(cid_f_args, total_atd_args, scaling_factor = 1000):\n",
    "    \"\"\"\n",
    "    Creates a new DataFrame with the incidence ratio between the numerator and denominator.\n",
    "    \n",
    "    Args:\n",
    "        extra_args (dict): A dictionary where each key has a value that is another dictionary containing a `download_dir` key.\n",
    "        total_atd_name (str): A constant representing the folder name for the denominator data.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary where each key corresponds to the key in `extra_args`, and the value is the resulting incidence ratio DataFrame.\n",
    "    \"\"\"\n",
    "    incidence_ratio_results = {}\n",
    "\n",
    "    for key, value in cid_f_args.items():\n",
    "        # Paths for numerator and denominator\n",
    "        total_atd_arg = total_atd_args[key+'_total']\n",
    "\n",
    "        numerator_dir = os.path.join(value['download_dir'], \"joined_files\")\n",
    "        denominator_dir = os.path.join(total_atd_arg['download_dir'], \"joined_files\")\n",
    "\n",
    "        \n",
    "        # Find the numerator and denominator CSV files\n",
    "        numerator_file = next((f for f in os.listdir(numerator_dir) if f.startswith(\"joined_\") and f.endswith(\".xlsx\")), None)\n",
    "        denominator_file = next((f for f in os.listdir(denominator_dir) if f.startswith(\"joined_\") and f.endswith(\".xlsx\")), None)\n",
    "        print(denominator_file)\n",
    "        if not numerator_file or not denominator_file:\n",
    "            print(f\"Missing files for key '{key}'. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Load the numerator and denominator DataFrames\n",
    "        numerator_df = pd.read_excel(os.path.join(numerator_dir, numerator_file), index_col=0)\n",
    "        denominator_df = pd.read_excel(os.path.join(denominator_dir, denominator_file), index_col=0)\n",
    "        \n",
    "        # Ensure the denominator has only one row\n",
    "        if len(denominator_df) != 1:\n",
    "            raise ValueError(f\"Denominator file for key '{key}' must have exactly one row.\")\n",
    "        \n",
    "        # Extract the single row from the denominator\n",
    "        denominator_row = denominator_df.iloc[0]\n",
    "        \n",
    "        # Create the incidence ratio DataFrame\n",
    "        incidence_ratio_df = numerator_df.copy()\n",
    "        for col in numerator_df.columns:\n",
    "            if col in denominator_row.index:\n",
    "                incidence_ratio_df[col] = numerator_df[col] / denominator_row[col] * scaling_factor  # Multiply by 1000 for incidence rate\n",
    "            else:\n",
    "                print(f\"Column '{col}' not found in denominator for key '{key}'. Setting values to NaN.\")\n",
    "                incidence_ratio_df[col] = float('nan')\n",
    "        \n",
    "        # Store the result in the dictionary\n",
    "        incidence_ratio_results[key] = incidence_ratio_df\n",
    "    \n",
    "    return incidence_ratio_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6bcfa7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExportIRdf(ir_df, download_dir, file_name):\n",
    "    \"\"\"\n",
    "    Exports the incidence ratio DataFrame to an Excel file.\n",
    "    \n",
    "    Args:\n",
    "        ir_df (pd.DataFrame): The incidence ratio DataFrame to export.\n",
    "        download_dir (str): The directory where the file will be saved.\n",
    "        file_name (str): The name of the output file.\n",
    "    \"\"\"\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "    \n",
    "    # Save the DataFrame to an Excel file\n",
    "    output_file_path = os.path.join(download_dir, file_name)\n",
    "    ir_df.to_excel(output_file_path, index=True, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "04d08f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractAndSaveGrandTotals(incidence_ratio_results, output_dir, output_file_name):\n",
    "    \"\"\"\n",
    "    Extracts the Grand Total row from all incidence ratio DataFrames and consolidates them into a new file.\n",
    "    \n",
    "    Args:\n",
    "        incidence_ratio_results (dict): A dictionary where keys are region names and values are DataFrames.\n",
    "        output_dir (str): The directory where the consolidated file will be saved.\n",
    "        output_file_name (str): The name of the output file.\n",
    "    \"\"\"\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Initialize an empty DataFrame to store the Grand Totals\n",
    "    grand_totals_df = pd.DataFrame()\n",
    "\n",
    "    for region, df in incidence_ratio_results.items():\n",
    "        # Extract the Grand Total row\n",
    "        grand_total_row = df[df.index == \"Grand Total\"]\n",
    "        \n",
    "        if not grand_total_row.empty:\n",
    "            # Include the index as a column before transposing\n",
    "            grand_total_row = grand_total_row.reset_index()\n",
    "            grand_total_row = grand_total_row.iloc[:, 1:].T  # Transpose to make columns into rows\n",
    "            grand_total_row.columns = [region]  # Rename the column to the region name\n",
    "            \n",
    "            # Add the Grand Total row to the consolidated DataFrame\n",
    "            grand_totals_df = pd.concat([grand_totals_df, grand_total_row], axis=1)\n",
    "        else:\n",
    "            print(f\"Grand Total row not found for region '{region}'. Skipping...\")\n",
    "\n",
    "    # Save the consolidated DataFrame to an Excel file\n",
    "    output_file_path = os.path.join(output_dir, output_file_name)\n",
    "    grand_totals_df = grand_totals_df.T  # Transpose to make it more readable\n",
    "    grand_totals_df.to_excel(output_file_path, index=True, engine=\"openpyxl\")\n",
    "    print(f\"Grand Totals saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2d7c97",
   "metadata": {},
   "source": [
    "# EXEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7131bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ SKIPPING: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sul\\cid_f\\scrapped_relatorio__2016_sul.csv' already exists.\n",
      "Download successful for sul.\n",
      "------------ SKIPPING: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sul\\cid_f\\scrapped_relatorio__2017_sul.csv' already exists.\n",
      "Download successful for sul.\n",
      "------------ SKIPPING: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sul\\cid_f\\scrapped_relatorio__2018_sul.csv' already exists.\n",
      "Download successful for sul.\n",
      "------------ SKIPPING: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sul\\cid_f\\scrapped_relatorio__2019_sul.csv' already exists.\n",
      "Download successful for sul.\n",
      "------------ SKIPPING: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sul\\cid_f\\scrapped_relatorio__2020_sul.csv' already exists.\n",
      "Download successful for sul.\n",
      "------------ SKIPPING: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sul\\cid_f\\scrapped_relatorio__2021_sul.csv' already exists.\n",
      "Download successful for sul.\n",
      "------------ SKIPPING: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sul\\cid_f\\scrapped_relatorio__2022_sul.csv' already exists.\n",
      "Download successful for sul.\n",
      "------------ SKIPPING: The file 'D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\sul\\cid_f\\scrapped_relatorio__2023_sul.csv' already exists.\n",
      "Download successful for sul.\n",
      "Processing file: scrapped_relatorio__2016_sul.csv, Start line: 9, End line: 458\n",
      "Processing file: scrapped_relatorio__2017_sul.csv, Start line: 9, End line: 455\n",
      "Processing file: scrapped_relatorio__2018_sul.csv, Start line: 9, End line: 452\n",
      "Processing file: scrapped_relatorio__2019_sul.csv, Start line: 9, End line: 458\n",
      "Processing file: scrapped_relatorio__2020_sul.csv, Start line: 9, End line: 455\n",
      "Processing file: scrapped_relatorio__2021_sul.csv, Start line: 9, End line: 451\n",
      "Processing file: scrapped_relatorio__2022_sul.csv, Start line: 9, End line: 457\n",
      "Processing file: scrapped_relatorio__2023_sul.csv, Start line: 9, End line: 456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_21232\\1403856281.py:71: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_21232\\1403856281.py:71: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_21232\\1403856281.py:71: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_21232\\1403856281.py:71: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_21232\\1403856281.py:71: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_21232\\1403856281.py:71: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_21232\\1403856281.py:71: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_21232\\1403856281.py:71: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download e processamento conclu√≠dos.\n",
      "Grand Total row not found for region 'joined_df'. Skipping...\n",
      "Grand Total row not found for region 'coded_df'. Skipping...\n",
      "Grand Total row not found for region 'grouped_df'. Skipping...\n",
      "Grand Total row not found for region 'test_grouped_df'. Skipping...\n",
      "Grand Total row not found for region 'dated_df'. Skipping...\n",
      "Grand Total row not found for region 'dated_df_with_grand_total'. Skipping...\n",
      "Grand Totals saved to D:\\CodeStuff\\Stats\\colab_linear_regression\\linear_regression\\sisab_scrap_download\\_regions_joined\\regions_joined.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_21232\\3056129719.py:122: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n",
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_21232\\3056129719.py:122: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  new_columns[col] = pd.to_datetime(updated_col, format=\"%m/%Y\", errors=\"ignore\")\n"
     ]
    }
   ],
   "source": [
    "url = \"https://sisab.saude.gov.br/paginas/acessoRestrito/relatorio/federal/saude/RelSauProducao.xhtml\"\n",
    "dfs = main(VARIABLES_TABLE)\n",
    "dfs\n",
    "#ExtractAndSaveGrandTotals(dfs, \"D:\\\\CodeStuff\\\\Stats\\\\colab_linear_regression\\\\linear_regression\\\\sisab_scrap_download\\\\_regions_joined\", \"regions_joined.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c372609f",
   "metadata": {},
   "source": [
    "# EXEC IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "590f16b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#incidence_ratio = CreateIncidenceRatioDF(CID_F_ARGS,TOTAL_ATD_ARGS)\n",
    "#for key, df in incidence_ratio.items():\n",
    "#    # Export the DataFrame to an Excel file\n",
    "#    download_dir = CID_F_ARGS[key]['download_dir'] + \"\\\\incidence_ratio\"\n",
    "#    file_name = f\"incidence_ratio_{key}.xlsx\"\n",
    "#    ExportIRdf(df, download_dir, file_name)\n",
    "#    \n",
    "#ExtractAndSaveGrandTotals(incidence_ratio, \"D:\\\\CodeStuff\\\\Stats\\\\colab_linear_regression\\\\linear_regression\\\\sisab_scrap_download\\\\_regions_joined\", \"regions_joined.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
